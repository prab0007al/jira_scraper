{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21043", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21043"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Refactor the way we check if a transformation is allowed to be committed during upgrades\nDescription: Currently the way we check if a transformation is allowed to be committed during metadata serialization upgrades is to call the {{allowDuringUpgrades}} method, which only returns true for {{Startup}} and {{TriggerSnapshot}} transformations. This means that all other transformations will be disallowed if the metadata serialization version changed, which we allow in minor upgrades. This is overly defensive and we can relax this to only disallow committing completely new transformations since non-upgraded nodes would not be able to deserialize them. Modified transformations must be backwards compatible to be committed, and will be serialized using the oldest serialization version present in the cluster. This patch adds an {{introducedIn}} field on the {{Transformation.Kind}} enum. This allows old transformations to be committed by comparing it with the cluster min version. For more flexibility, a transformation can also override the {{eligibleToCommit}} method to be able to check if the current cluster metadata allows the transformation. Specifically for new schema transformations, a {{compatibleWith}} method has to be implemented. This method should also handle modified schema transformations to make sure non-upgraded nodes can still execute them. See javadoc on the {{SchemaTransformation.compatibleWith}} method for more details.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21042", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21042"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make TopologyException a check exception\nDescription: Make TopologyException a checked exception to ensure they are handled carefully, as they may occur at surprising times Also Fix: - Restore MaxDecidedRX on replay - When catchup_on_start_exit_on_failure == false, should startup on any kind of failure, not only timeout - lazy vtable LIMIT clause regression - DurabilityService.onEpochRetired - Command.validate when uniqueHlc differs - Avoid unsafe publication of AccordExecutor to scheduledFastTasks - AccordCache hitRate metric names - use long for return type of DurationSpec.toNanoseconds - Repair without all replicas should not request all Accord replicas participate Also Improve: - Split Accord startup into local+distributed, ensure we - Add logging to FetchDurableBefore on startup - Add randomised testing of AbstractLazyVirtualTable - Add validation of lazy virtual table key ordering - Don't send requests to faulty replicas - shrinkOrEvict large objects without holding lock - Accord dtest shutdown\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21041", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21041"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: SSTable overlap statistics\nDescription: When trying to evaluate the effect of compaction and its settings, perhaps the most relevant information is the amount of overlap between sstables, i.e. the number of sstables that have to be queried to serve any request for a specific partition. We do not currently present this information for any compaction strategy. For some strategies this can be organized by level, and specifically for UCS, by shard. [This patch in the DataStax repository|https://github.com/datastax/cassandra/commit/c7d6f082e9eef8f9121ef271495e9a2469bde671] implements such a statistic for UCS, which outputs overlap per level as {code} Max overlap map for density.blobs: all: 26, per shard: [11, 11, 17, 17, 14, 14, 20, 20, 17, 17, 23, 23, 20, 20, 26, 26] unrepaired:0: 18, per shard: [6, 10, 14, 18] unrepaired:1: 9, per shard: [6, 9, 5, 8, 4, 7, 3, 6] unrepaired:2: 3, per shard: [3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2] {code}.\nComments: - Presented as a graph, the above statistics look like this: !overlap-graph.png! The three shaded regions are numbers of sstables overlapping in each shard and level, which add up to a little more than the total overlap in each shard shown in the dashed black line. The difference between shards is due to the compactions still ongoing or yet to be started, including the effect of opening sstables early. The difference between the dashed line and the sum of the shaded regions is due to some data in non-intersecting regions of some levels, in this case due to early open resulting in data in a shard being counted both for the lower level (parts still to be compacted) and the higher (parts already compacted and opened early).", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21040", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21040"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optimize ModificationStatement#requiresRead\nDescription: org.apache.cassandra.cql3.statements.ModificationStatement#requiresRead is calculated now every time, we can calculate it once and memorize the result. It costs 1.5% of CPU for a simple 10-row batch insert test. !image-2025-11-22-12-30-59-096.png|width=1000!\nComments: - I have added couple small nits / ideas to your patch.", "output": "Status: Review In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21039", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21039"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Minor Directory lookup performance improvements\nDescription: * isGossipOnlyMember iterates all joined hosts unnecessarily * Location lookup should be cached to avoid several directory lookups\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21038", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21038"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Avoid iterating all prepared statements when getting the PreparedStatementsCacheSize metric\nDescription: The prepared statement cache tracks the current total weight for eviction purposes, we can just return that\nComments: - +1 (same API is mentioned here: https://github.com/ben-manes/caffeine/discussions/822)", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21037", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21037"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Backport fix to nodetool gcstats output\nDescription: Backport the fix to DirectMemory reporting https://issues.apache.org/jira/browse/CASSANDRA-19022\nComments: No comments.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21036", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21036"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Better support for CMS initialization when DOWN nodes have not been upgraded\nDescription: The {{nodetool cms initialize}} command allows the operator to supply a list of nodes to ignore when checking for consensus on the initial state of cluster metadata. If the ignored nodes have been upgraded and have begun startup prior to this point, then the rest of the cluster will view them as upgraded and everything proceeds as expected. However, if an ignored node has been DOWN for the entirety of the upgrade, peers still consider the cluster to be in a mixed state. This has an effect on the metadata serialization version that is selected as well as on which metadata transformations are permitted. It must be possible for the participating nodes to commit both the {{PreInitialize}} and {{Initialize}} transformations in order for the CMS initialization to complete.\nComments: - CI results to follow - +1 - None of the test failures appear to be related to this patch and mostly seem to be failing across all relevant branches.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21035", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21035"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Nodes with LEFT state do not get expired from gossip\nDescription: When a node is removed from the cluster via decommission/removenode/assassinate it is transitioned to a LEFT status in gossip. For historical reasons, this state is intended to persist in gossip for some period (default is 72hrs) before being expunged. Since CEP-21, gossip status is no longer used for anything significant by C* itself, but this behaviour was preserved in case any tooling was relying on it. However, there is a bug in that the purging from gossip after 72 hrs is not occurring correctly. From the perspective of C* itself this is harmless, but it will eventually lead to gossip state being oversized.\nComments: - CI results to follow", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21034", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21034"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: ZstdDictionaryCompressor is not used upon flush\nDescription: When I insert data to a table and do {{nodetool flush}}, an SSTable is created, but it will be compacted with LZ4 (seen via sstablemetadata). Correct compressor should be used upon flush as well so flushed data are compressed by a Zstd dictionary too.\nComments: - This is actually not a bug. I forgot there is \"flush_compression\" in cassandra.yaml which drives this. When I used \"table\" value for that config then it was indeed compressed with Zstd dictionary when flushed. - yes, it is actually by design, the idea was to flush fast with a cheap compression and switch to a better compression in background during a compaction (CASSANDRA-15379)", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21033", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21033"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: DiskUsageBroadcaster does not update usageInfo on node replacement\nDescription: While working on CASSANDRA-21024, I found that on trunk we are not calling the {{onRemove}} method for {{IEndpointStateChangeSubscriber}} implementations when a node is replaced. This leads to the {{DiskUsageBroadcaster}} to not remove the node which was replaced, which makes calls to {{DiskUsageBroadcaster#hasStuffedOrFullNode}} to incorrectly return true if a {{STUFFED}} / {{FULL}} node is replaced with a {{SPACIOUS}} node.\nComments: - The initial PR adds the call to {{GossipHelper.removeFromGossip}} on {{LegacyStateListener#notifyPostCommit}} before we evict the node from gossip. While this gets the tests working, I'm not 100% sure this is the right place to do this. - Thanks, this looks good to me. I've submitted a CI job & I'll update with the results when it's done. - +1 CI: https://pre-ci.cassandra.apache.org/job/cassandra/192/", "output": "Status: Requires Testing\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21032", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21032"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Test failure: TestClientRequestMetrics.test_client_request_metrics\nDescription: TestClientRequestMetrics.test_client_request_metrics fail consistency on trunk. For example: [https://ci-cassandra.apache.org/job/Cassandra-trunk/2325/#showFailuresLink] {code:java} AssertionError: org.apache.cassandra.metrics:type=ClientRequest,scope=Write-ANY,name=LatencyMean assert 0.0 is None {code} {code} self = <client_request_metrics_test.TestClientRequestMetrics object at 0x7f5586135640> def test_client_request_metrics(self): # this is written as a single test method in order to reuse the same cluster for all tests # setup_once configures and starts the cluster with all schema and preconditions required by all tests. self.setup_once() > self.write_nominal() client_request_metrics_test.py:85: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ client_request_metrics_test.py:116: in write_nominal global_diff, cl_diff = self.validate_nominal('Write', client_request_metrics_test.py:148: in validate_nominal b.validate(cassandra_version) client_request_metrics_test.py:670: in validate v.validate(cassandra_version) client_request_metrics_test.py:652: in validate validate_stat_values(self.mbean, self.values, cassandra_version) client_request_metrics_test.py:596: in validate_stat_values validate_zero_histogram_values(prefix, values) client_request_metrics_test.py:629: in validate_zero_histogram_values validators[k](f\"{prefix}{k}\", v) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ k = 'org.apache.cassandra.metrics:type=ClientRequest,scope=Write-ANY,name=LatencyMean' v = 0.0 def is_none(k, v): > assert v is None, k E AssertionError: org.apache.cassandra.metrics:type=ClientRequest,scope=Write-ANY,name=LatencyMean E assert 0.0 is None client_request_metrics_test.py:767: AssertionError {code} {code} def validate_zero_histogram_values(prefix, values): validators = defaultdict(lambda: is_zero) validators['RecentValues'] = is_zero_list validators['Mean'] = is_none # <============== we fail here validators['DurationUnit'] = is_microseconds for k, v in values.items(): validators[k](f\"{prefix}{k}\", v) {code}\nComments: - It looks like we have changed org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir.AbstractSnapshot#getMean behaviour in CASSANDRA-20822 (https://github.com/apache/cassandra/commit/ae673a31cc32a1ea3d397723c0dd893218cec5ea), so correspondently it affects our JMX contract for histogram/timer beans where we return 0 instead of None as Mean value now if count = 0. I see two options to fix the failed test: # return getMean behaviour back to return None if count = 0 # change dtest validator to accept both: None and 0 - [~ifesdjeen] Could you please help me understand the reason for this change? Do we really need it? - I've discussed with [~benedict] and decided to go with the test update option to support 0 as a valid value - consistency of API (we return 0 in case of max, min and stddev) looks more important here than attempts to preserve the old behaviour - CI run: [https://pre-ci.cassandra.apache.org/job/cassandra/179/] [^CASSANDRA-21032_ci_summary.htm] [^CASSANDRA-21032_results_details.tar.xz] The failed tests are not related to the changes. TestClientRequestMetrics.test_client_request_metrics passed.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21031", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21031"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness AST tests\nDescription: A lot of this we get for free piggybacking off of extensions/forks of AST tests for mutation tracking. Need make sure there is coverage and parity with mutation tracking for the AST tests.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21030", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21030"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: CDC with witnesses\nDescription: Depends on how CDC works with mutation tracking (might just work), but need to validate it works.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21029", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21029"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Validate Witness expand/shrink/move\nDescription: In theory this still works. In practice the existing tests need to be revisited and we need to be sure.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21028", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21028"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness support for RF=6/3 or similar\nDescription: With few enough full replicas it starts to get more complicated to support things effectively. Investigate how to accomplish this.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21027", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21027"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Validate logged/unlogged batches with witnesses\nDescription: I think there is a unit test for this already, but we need to be sure once mutation tracking supports these.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21026", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21026"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Reusing the address of a removed node is not possible with Accord enabled\nDescription: If the address of a decommissioned node is re-used by another new node at some later time, any node in the cluster with Accord enabled will be unable to start up, including the new node. As the new node comes up and registers with the {{ClusterMetadataService}} it is added to the {{Directory}}. The decommissioned node's details are also preserved in the directory present to ensure that transactions which were in-flight can be completed after the node has left. (https://issues.apache.org/jira/browse/CASSANDRA-20142) During AccordService initialization building the endpoint mapping will fail because of this check in {{EndpointMapping.Builder}}: {code} Invariants.requireArgument(!mapping.containsValue(endpoint), \"Mapping already exists for %s\", endpoint); {code} Additionally, it seems possible that the wrong method is being called in {{AccordTopology::directoryToEndpointMapping}} {code} // There are cases where nodes are removed from the cluster (host replacement, decom, etc.), but inflight events // may still be happening; keep the ids around so pending events do not fail with a mapping error for (Directory.RemovedNode removedNode : directory.removedNodes()) builder.add(removedNode.endpoint, tcmIdToAccord(removedNode.id)); {code} which should probably call {{builder::removed}} rather than {{builder::add}} but that also contains the the same invariant check.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21025", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21025"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Failure detector max interval value is calculated incorrectly\nDescription: If this setting is not overridden via the {{cassandra.fd_max_interval_ms}} system property ({{{}CassandraRelevantProperties.FD_MAX_INTERVAL_MS{}}}), then it is seeded with the value of {{{}FailureDetector.INITIAL_VALUE_NANOS{}}}. However, a bug in the logic of {{FailureDetector$ArrivalWindow::getMaxInterval}} means in this case there is an incorrect conversion between time units. {code:java} public static long getMaxInterval() { long newValue = FD_MAX_INTERVAL_MS.getLong(FailureDetector.INITIAL_VALUE_NANOS); if (newValue != FailureDetector.INITIAL_VALUE_NANOS) logger.info(\"Overriding {} from {}ms to {}ms\", FD_MAX_INTERVAL_MS.getKey(), FailureDetector.INITIAL_VALUE_NANOS, newValue); return TimeUnit.NANOSECONDS.convert(newValue, TimeUnit.MILLISECONDS); } {code} If {{FD_MAX_INTERVAL_MS}} is not set, the supplied default {{INITIAL_VALUE_NANOS}} is used, but this is then converted as if it were a value in millis, inflating it 1000000x. The effective max interval in this case should be 2 seconds, but instead becomes 23 days, 3 hours, 33 minutes & 20 seconds. The net effect is that intervals way longer than expected can be recorded if nodes are intermittently partitioned but not restarted (meaning they retain the same gossip generation). In turn this can cause the phi calculation to react to those nodes much more slowly as the mean arrival time interval is much bigger than expected, leaving them marked as {{UP}} when they should be {{{}DOWN{}}}. If {{FD_MAX_INTERVAL_MS}} is overridden then the conversion, and so the returned value, is correct (assuming an appropriately scaled values is supplied, there is no guardrail to ensure that). Versions earlier than 5.0 are not affected.\nComments: No comments.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21024", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21024"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add configuration to disk usage guardrails to stop writes across all replicas of a keyspace when any node replicating that keyspace exceeds the disk usage failure threshold.\nDescription: [CASSANDRA-17150|https://issues.apache.org/jira/browse/CASSANDRA-17150] introduced disk usage guardrails that stop writes for specific tokens when any replica responsible for those tokens exceeds the configured failure threshold. This mechanism protects individual nodes from running out of disk space but can result in inconsistent write availability when only a subset of replicas or token ranges are affected. This in turn pushes the responsibility onto the application owner to decide how to handle the partial write unavailability. We propose adding a new configuration option, data_disk_usage_stop_writes_for_keyspace_on_fail, that extends this behavior to the keyspace level. When enabled, if any node that participates in replication for a keyspace exceeds the disk usage failure threshold, writes to that keyspace will be stopped across all nodes who replicate that keyspace. This change provides operators with finer control over guardrail enforcement, allowing them to choose between the current per-token behavior or a stricter, keyspace-wide policy that prioritizes simplicity and operational predictability over partial write availability.\nComments: - WIP branch available for trunk: https://github.com/isaacreath/cassandra/commit/b2b738335f67692bed725cb74f50a2eafdec2b25. General idea is to track if a given datacenter contains a full node in the `DiskUsageBroadcaster`. On write, we'll check all nodes that replicate data for a given partition key and if that node is in a datacenter that contains a full node, we'll reject the write.", "output": "Status: In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21023", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21023"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: sstableloader time-out on Cassandra 4.1.x with SSL Configuration\nDescription: Hello Community !!! I am trying to use _sstableloader_ to restore a snapshot. Cassandra is configured with {color:#00875a}_internode_encryption_{color} and {color:#00875a}_client_encryption_{color} enabled. I have a test cluster with 3 nodes and a recovery cluster with 42 nodes ({_}3 racks{_} in each environment). Both clusters have Cassandra _4.1.4_ installed. When I try sstableloader, it terminates the connection with the error: `{_}{color:#4c9aff}Operation timed out{color}{_}`. I test with Cassandra _4.1.10_ and it is the same output. Below is the command and the trace output of the sstableloader run: ```bash /var/opt/cassandra/apache-cassandra-4.1.4/bin/sstableloader -d cassandra-test-rack1-01,cassandra-test-rack2-01,cassandra-test-rack3-01 -f /var/opt/cassandra/apache-cassandra-4.1.4/conf/cassandra.yaml -u user1 -pw password /data/cassandra-snap/test/testtable ``` ```log EBUG 13:55:45,457 Loaded default ResourceLeakDetector: com.datastax.shaded.netty.util.ResourceLeakDetector@3c7175ae DEBUG 13:55:45,555 [id: 0x446e44c3, L:/10.10.10.214:52930 - R:cassandra-test-rack1-01/10.10.10.214:9042] HANDSHAKEN: TLS_AES_256_GCM_SHA384 DEBUG 13:55:57,532 Defuncting Connection[cassandra-test-rack1-01/10.10.10.214:9042-1, inFlight=0, closed=false] because: [cassandra-test-rack1-01/10.10.10.214:9042] Operation timed out DEBUG 13:55:57,533 [cassandra-test-rack1-01/10.10.10.214:9042] preventing new connections for the next 1000 ms DEBUG 13:55:57,533 [cassandra-test-rack1-01/10.10.10.214:9042] Connection[cassandra-test-rack1-01/10.10.10.214:9042-1, inFlight=0, closed=false] failed, remaining = 0 DEBUG 13:55:57,533 Connection[cassandra-test-rack1-01/10.10.10.214:9042-1, inFlight=0, closed=true] closing connection DEBUG 13:55:57,533 Not terminating Connection[cassandra-test-rack1-01/10.10.10.214:9042-1, inFlight=0, closed=true]: there are still pending requests DEBUG 13:55:57,534 [Control connection] error on cassandra-test-rack1-01/10.10.10.214:9042 connection, trying next host com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack1-01/10.10.10.214:9042] Operation timed out at com.datastax.driver.core.Connection$Future.onTimeout(Connection.java:1557) at com.datastax.driver.core.Connection$ResponseHandler$1.run(Connection.java:1636) at com.datastax.shaded.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:663) at com.datastax.shaded.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:738) at com.datastax.shaded.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:466) at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829) DEBUG 13:55:57,534 [cassandra-test-rack2-01/10.10.10.213:9042] preparing to open 1 new connections, total = 1 DEBUG 13:55:57,535 Connection[cassandra-test-rack1-01/10.10.10.214:9042-1, inFlight=0, closed=true], stream 0, Error writing request STARTUP \\{CQL_VERSION=3.0.0, DRIVER_VERSION=3.11.0, DRIVER_NAME=DataStax Java Driver} DEBUG 13:55:57,536 Connection[cassandra-test-rack2-01/10.10.10.213:9042-1, inFlight=0, closed=false] Connection established, initializing transport DEBUG 13:55:57,574 [id: 0x870adc85, L:/10.10.10.214:59728 - R:cassandra-test-rack2-01/10.10.10.213:9042] HANDSHAKEN: TLS_AES_256_GCM_SHA384 DEBUG 13:56:09,631 Defuncting Connection[cassandra-test-rack2-01/10.10.10.213:9042-1, inFlight=0, closed=false] because: [cassandra-test-rack2-01/10.10.10.213:9042] Operation timed out DEBUG 13:56:09,632 [cassandra-test-rack2-01/10.10.10.213:9042] preventing new connections for the next 1000 ms DEBUG 13:56:09,632 [cassandra-test-rack2-01/10.10.10.213:9042] Connection[cassandra-test-rack2-01/10.10.10.213:9042-1, inFlight=0, closed=false] failed, remaining = 0 DEBUG 13:56:09,632 Connection[cassandra-test-rack2-01/10.10.10.213:9042-1, inFlight=0, closed=true] closing connection DEBUG 13:56:09,632 Not terminating Connection[cassandra-test-rack2-01/10.10.10.213:9042-1, inFlight=0, closed=true]: there are still pending requests DEBUG 13:56:09,633 [Control connection] error on cassandra-test-rack2-01/10.10.10.213:9042 connection, trying next host com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack2-01/10.10.10.213:9042] Operation timed out at com.datastax.driver.core.Connection$Future.onTimeout(Connection.java:1557) at com.datastax.driver.core.Connection$ResponseHandler$1.run(Connection.java:1636) at com.datastax.shaded.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:663) at com.datastax.shaded.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:738) at com.datastax.shaded.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:466) at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829) DEBUG 13:56:09,633 [cassandra-test-rack3-01/10.10.10.215:9042] preparing to open 1 new connections, total = 1 DEBUG 13:56:09,633 Connection[cassandra-test-rack2-01/10.10.10.213:9042-1, inFlight=0, closed=true], stream 0, Error writing request STARTUP \\{CQL_VERSION=3.0.0, DRIVER_VERSION=3.11.0, DRIVER_NAME=DataStax Java Driver} DEBUG 13:56:09,634 Connection[cassandra-test-rack3-01/10.10.10.215:9042-1, inFlight=0, closed=false] Connection established, initializing transport DEBUG 13:56:09,661 [id: 0x08eded5b, L:/10.10.10.214:45150 - R:cassandra-test-rack3-01/10.10.10.215:9042] HANDSHAKEN: TLS_AES_256_GCM_SHA384 DEBUG 13:56:21,731 Defuncting Connection[cassandra-test-rack3-01/10.10.10.215:9042-1, inFlight=0, closed=false] because: [cassandra-test-rack3-01/10.10.10.215:9042] Operation timed out DEBUG 13:56:21,732 [cassandra-test-rack3-01/10.10.10.215:9042] preventing new connections for the next 1000 ms DEBUG 13:56:21,732 [cassandra-test-rack3-01/10.10.10.215:9042] Connection[cassandra-test-rack3-01/10.10.10.215:9042-1, inFlight=0, closed=false] failed, remaining = 0 DEBUG 13:56:21,732 Connection[cassandra-test-rack3-01/10.10.10.215:9042-1, inFlight=0, closed=true] closing connection DEBUG 13:56:21,732 Not terminating Connection[cassandra-test-rack3-01/10.10.10.215:9042-1, inFlight=0, closed=true]: there are still pending requests DEBUG 13:56:21,732 [Control connection] error on cassandra-test-rack3-01/10.10.10.215:9042 connection, no more host to try com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack3-01/10.10.10.215:9042] Operation timed out at com.datastax.driver.core.Connection$Future.onTimeout(Connection.java:1557) at com.datastax.driver.core.Connection$ResponseHandler$1.run(Connection.java:1636) at com.datastax.shaded.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:663) at com.datastax.shaded.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:738) at com.datastax.shaded.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:466) at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829) DEBUG 13:56:21,732 Connection[cassandra-test-rack3-01/10.10.10.215:9042-1, inFlight=0, closed=true], stream 0, Error writing request STARTUP \\{CQL_VERSION=3.0.0, DRIVER_VERSION=3.11.0, DRIVER_NAME=DataStax Java Driver} DEBUG 13:56:21,733 Shutting down DEBUG 13:56:21,740 Freed 12 thread-local buffer(s) from thread: cluster1-nio-worker-0 DEBUG 13:56:21,740 Freed 12 thread-local buffer(s) from thread: cluster1-nio-worker-1 DEBUG 13:56:23,943 Freed 12 thread-local buffer(s) from thread: cluster1-nio-worker-2 DEBUG 13:56:23,950 Connection[cassandra-test-rack1-01/10.10.10.214:9042-1, inFlight=0, closed=true] has already terminated Unable to initialise org.apache.cassandra.utils.NativeSSTableLoaderClient com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra-test-rack2-01/10.10.10.213:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack2-01/10.10.10.213:9042] Operation timed out), cassandra-test-rack3-01/10.10.10.215:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack3-01/10.10.10.215:9042] Operation timed out), cassandra-test-rack1-01/10.10.10.214:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack1-01/10.10.10.214:9042] Operation timed out)) java.lang.RuntimeException: Unable to initialise org.apache.cassandra.utils.NativeSSTableLoaderClient at org.apache.cassandra.utils.NativeSSTableLoaderClient.init(NativeSSTableLoaderClient.java:102) at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:167) at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:91) at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:58) Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra-test-rack2-01/10.10.10.213:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack2-01/10.10.10.213:9042] Operation timed out), cassandra-test-rack3-01/10.10.10.215:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack3-01/10.10.10.215:9042] Operation timed out), cassandra-test-rack1-01/10.10.10.214:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack1-01/10.10.10.214:9042] Operation timed out)) at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:270) at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:109) at com.datastax.driver.core.Cluster$Manager.negotiateProtocolVersionAndConnect(Cluster.java:1813) at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1726) at com.datastax.driver.core.Cluster.init(Cluster.java:214) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:387) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:366) at com.datastax.driver.core.Cluster.connect(Cluster.java:311) at org.apache.cassandra.utils.NativeSSTableLoaderClient.init(NativeSSTableLoaderClient.java:70) ... 3 more Exception in thread \"main\" org.apache.cassandra.tools.BulkLoadException: java.lang.RuntimeException: Unable to initialise org.apache.cassandra.utils.NativeSSTableLoaderClient at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:104) at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:58) Caused by: java.lang.RuntimeException: Unable to initialise org.apache.cassandra.utils.NativeSSTableLoaderClient at org.apache.cassandra.utils.NativeSSTableLoaderClient.init(NativeSSTableLoaderClient.java:102) at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:167) at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:91) ... 1 more Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: cassandra-test-rack2-01/10.10.10.213:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack2-01/10.10.10.213:9042] Operation timed out), cassandra-test-rack3-01/10.10.10.215:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack3-01/10.10.10.215:9042] Operation timed out), cassandra-test-rack1-01/10.10.10.214:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [cassandra-test-rack1-01/10.10.10.214:9042] Operation timed out)) at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:270) at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:109) at com.datastax.driver.core.Cluster$Manager.negotiateProtocolVersionAndConnect(Cluster.java:1813) at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1726) at com.datastax.driver.core.Cluster.init(Cluster.java:214) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:387) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:366) at com.datastax.driver.core.Cluster.connect(Cluster.java:311) at org.apache.cassandra.utils.NativeSSTableLoaderClient.init(NativeSSTableLoaderClient.java:70) ``` However, using the *sstableloader from Cassandra _5.0.6_* to load sstables on the Cassandra Cluster 4.1.4 {*}works as expected{*}. Based on my initial investigation, the {*}cassandra-driver-core versions 3.11.0 and 3.11.1 seem to be incompatible with my Cassandra 4.1.x configuration{*}. Starting from cassandra-driver-core version 3.11.2, the issue is resolved. To verify this, I created a Java project based on an example I found in the cassandra-java-driver version 3.11.1. (Reference: ReadTopologyAndSchemaMetadata.java) I simply added .ssl() to the cluster constructor in the example code.\nComments: - I have updated driver to 3.11.5 in 4.1.11. I strongly believe that, based on your observation that 3.11.2 is resolving the issue, when we release 4.1.11 your problem will be solved automatically. https://github.com/apache/cassandra/blob/cassandra-4.1/CHANGES.txt#L9 https://issues.apache.org/jira/browse/CASSANDRA-20904 - Thank you Stefan Good job ! I'm waiting looking forward toversion 4.1.11.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21022", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21022"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Deadlock during bootstrap: MigrationStage and PendingRangeCalculator block on keyspace initialization via LoadingMap\nDescription: *Overview* During node bootstrap, a deadlock occurs between the MigrationStage thread loading keyspaces and the PendingRangeCalculator thread. This causes the node to hang indefinitely during startup, preventing a successful bootstrap. This happened in a massive cluster of 600 nodes while an additional 300 nodes were being added simultaneously. *Deadlock Pattern* The deadlock involves three key components interacting through the LoadingMap keyspace cache: *Thread 1: MigrationStage:1* (holds the LoadingMap promise for a keyspace) Schema.merge() \u2192 Schema.createKeyspace() \u2192 Keyspace.open() \u2192 LoadingMap.blockingLoadIfAbsent() [line 105] - Creates promise, executes load function \u2192 Keyspace.<init>() [line 448] \u2192 ColumnFamilyStore.<init>() [line 521] \u2192 CompactionStrategyManager.<init>() [line 177] \u2192 getDiskBoundaries() \u2192 DiskBoundaryManager.getLocalRanges() [line 130] \u2192 PendingRangeCalculatorService.blockUntilFinished() \u2190 BLOCKS waiting for Thread 2 *Thread 2: PendingRangeCalculator:1* (triggered by gossip state changes) StorageService.handleStateNormal() [line 3499] \u2192 PendingRangeCalculatorService.update() \u2192 PendingRangeCalculatorService task [line 81] \u2192 Keyspace.open(keyspaceName) \u2192 LoadingMap.blockingLoadIfAbsent() [line 122] \u2190 BLOCKS waiting for promise held by Thread 1 {*}Result{*}: Classic circular wait deadlock Additional Blocked Threads Multiple other threads also block waiting for the same keyspace to load: - *main* thread (during StorageService.bootstrap() \u2192 invalidateLocalRanges()) - *GossipStage:1* (during handleStateNormal() \u2192 updateTokenMetadata() \u2192 invalidateLocalRanges()) - *ScheduledTasks:1* (memtable flush attempting getDiskBoundaries()) - {*}IndexSummaryManager:1{*}{*}{*} - *jmxtrans-agent-1* (JMX metrics collection) *Root Cause* The issue stems from a circular dependency during bootstrap: 1. Keyspace initialization requires disk boundaries (CompactionStrategyManager initialization) 2. Disk boundaries require pending ranges when StorageService.instance.isBootstrapMode() is true (DiskBoundaryManager.java:127-130) 3. Pending ranges require PendingRangeCalculator to finish (blockUntilFinished()) 4. PendingRangeCalculator tries to open keyspaces (PendingRangeCalculatorService.java:81) 5. Opening keyspaces blocks on the LoadingMap promise held by step 1 From DiskBoundaryManager.java:127-130: if (StorageService.instance.isBootstrapMode() && !StorageService.isReplacingSameAddress()) { PendingRangeCalculatorService.instance.blockUntilFinished(); // \u2190 Blocking call during init localRanges = tmd.getPendingRanges(cfs.keyspace.getName(), FBUtilities.getBroadcastAddressAndPort()); } Thread Dump Evidence \"MigrationStage:1\" - Thread t@82 java.lang.Thread.State: WAITING at org.apache.cassandra.service.PendingRangeCalculatorService.blockUntilFinished(PendingRangeCalculatorService.java:77) at org.apache.cassandra.db.DiskBoundaryManager.getLocalRanges(DiskBoundaryManager.java:130) at org.apache.cassandra.db.DiskBoundaryManager.getDiskBoundaries(DiskBoundaryManager.java:56) at org.apache.cassandra.db.compaction.CompactionStrategyManager.<init>(CompactionStrategyManager.java:177) at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:521) at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:448) at org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105) at org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:241) \"PendingRangeCalculator:1\" - Thread t@84 java.lang.Thread.State: WAITING at org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:122) at org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:241) at org.apache.cassandra.db.Keyspace.open(Keyspace.java:175) at org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:81) \"main\" - Thread t@1 java.lang.Thread.State: WAITING at org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:122) at org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:241) at org.apache.cassandra.db.Keyspace.open(Keyspace.java:175) at org.apache.cassandra.service.StorageService.invalidateLocalRanges(StorageService.java:2372) at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:2321) {*}Steps to Reproduce{*}{*}{*} The issue appears to be a race condition that occurs during bootstrap when: 1. Node is joining the cluster (bootstrap mode) 2. Schema migrations are being processed (MigrationStage active) 3. Gossip state changes trigger handleStateNormal() events 4. Multiple keyspaces are being loaded concurrently Reproduction may be more likely in environments with: - Many keyspaces - Active schema changes during bootstrap - NetworkTopologyStrategy with many tokens (increases pending range calculation time) {*}Impact{*}{*}{*} - Node becomes completely stuck during bootstrap - Requires process restart, but restart hits the same deadlock - Prevents cluster expansion - Manual intervention required (potentially removing schema changes or manipulating gossip state)\nComments: - Fixing the actual deadlock might take time, but one workaround to prevent the deadlock is increasing _ring_delay_ms_ from the default 30s to 120s. This workaround helps because the call to \"StorageService::invalidateLocalRanges\" is delayed up to the timeout, and that allows this node to drain all the \"StorageService::handleStateNormal\" from Gossip, so it avoids the possible deadlock.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21021", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21021"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Limit level for ZstdDictionaryCompressor to be max 9\nDescription: The high compression level after 9 does not yield much more compression ratio, but leads to significant memory footprint.\nComments: - https://github.com/apache/cassandra/pull/4475", "output": "Status: Needs Committer\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21020", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21020"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optimize thread local for metrics and tracing\nDescription: Currently Netty thread local logic is used in tracing to keep state and in metrics (thread local counters logic introduced in CASSANDRA-20250), so we do the thread local lookups many times during a request processing. These cases can be optimized by placing these objects as field variables to Thread itself by introducing CassandraThread as a child of FastThreadLocalThread. Similar idea can be found even in JDK (ThreadLocalRandom logic was introduced for ForkJoinPool speedup)\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21019", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21019"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Memtable allocator: separate memory usage tracking and limit checking\nDescription: The following optimization idea has been suggested by @blambov in CASSANDRA-20226: {quote} There's another option to consider here. The allocation mechanism does not need to check the limit for individual cell writes. We could just as well track the usage of a mutation in a single {{allocate}} call after it completes, or track the allocations with a {{LongAdder}} without checking if the limit is hit, and check if we need to wait for room before starting to apply a mutation. We use the {{allocate}} code to decide: - whether to initiate a flush, when the chosen memory limit is filled to some ratio - whether to pause accepting writes, when the chosen memory limit has been exhausted For the former use there is absolutely no benefit to make these decisions at the individual allocation level, as we will wait for the mutation to complete anyway before flushing anything. For the latter, I'd argue that the allocation-level tracking is actually hurting us. The reason for this is that we can have the limit be hit at any time during the application of a mutation, holding multiple locks (which necessitates the complexity of the {{isBlocking}} mutation signal), a partial copy of the mutation already written to the memtable structures, and a likely expanded version of the mutation to be applied on heap, keeping hold of more total memory than we would if we allowed the operation to continue. If, instead, we check the allocation limits _before starting_ a mutation and, once started, allow it to fully progress to completion, we can avoid this situation at the cost of being somewhat late to notice that the limit has been reached. This means that the limit will be breached, but this also happens as it stands now because we will permit operations to run to completion if the memtable they have been marked for is scheduled for a flush \u2013 which is effectively the same thing as not having noticed the memory limit would be breached by this mutation at the time when we decided to start it. {quote}\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21018", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21018"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: AssertionError in ByteBufferCloner.clone due to Range Tombstone on clustering key DELETE (4.1.10)\nDescription: h2. *Summary* In Apache Cassandra *4.1.10* an assertion error occurs reproducibly when executing a {{DELETE}} statement that produces a {*}range tombstone on the clustering key{*}. The crash happens inside the write path while applying the mutation on replicas, specifically in {{ByteBufferCloner.clone()}} invoked from {{{}RangeTombstoneList.clone(){}}}. The problem persists: * after {{nodetool scrub}} on all affected tables, * after deleting all hints, * even after completely *dropping and recreating* the keyspaces and tables. It only appears when executing a {{{}DELETE WHERE <partition key> AND <clustering key> >= ?{}}}. Removing this DELETE from the workload makes the issue disappear completely, even under heavy concurrent writes. This suggests a *bug in the handling of range tombstones* in the write path of Cassandra 4.1.x. h2. *Environment* * Cassandra version: *4.1.10* * 3-node cluster, RF = 3 * No hints pending * Reproducible with clean keyspace & empty datasets * Client performs parallel INSERTs and one DELETE with a clustering key range h2. *Table Schema* {code:java} CREATE TABLE kkav3.ranked_kka_products ( left text, rank int, kka_factor double, kka_factor_boosted double, left_factor double, left_factor_boosted double, left_picks int, right text, right_factor double, right_factor_boosted double, right_picks int, shared_picks int, shared_picks_boosted int, calculation_period int, last_update bigint, PRIMARY KEY ((left), rank) ) WITH CLUSTERING ORDER BY (rank ASC); {code} h2. *Queries that trigger the bug* Works fine alone: {code:java} INSERT INTO kkav3.ranked_kka_products (...) VALUES (...);{code} Triggers the bug (range tombstone on clustering key): {code:java} DELETE FROM kkav3.ranked_kka_products WHERE left = ? AND rank >= ?;{code} As soon as multiple clients insert rows and execute the above DELETE concurrently, Cassandra nodes throw an AssertionError inside the mutation processing path. h2. *Stacktrace* {code:java} ERROR [MutationStage-6] 2025-11-13 11:48:24,335 JVMStabilityInspector.java:68 - Exception in thread Thread[MutationStage-6,10,SharedPool] java.lang.RuntimeException: java.lang.AssertionError at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:108) at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:45) at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:430) at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:142) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Unknown Source) Caused by: java.lang.AssertionError: null at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:73) at org.apache.cassandra.db.RangeTombstoneList.clone(RangeTombstoneList.java:131) at org.apache.cassandra.db.RangeTombstoneList.clone(RangeTombstoneList.java:120) at org.apache.cassandra.db.MutableDeletionInfo.clone(MutableDeletionInfo.java:91) at org.apache.cassandra.db.MutableDeletionInfo.clone(MutableDeletionInfo.java:33) at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDeltaInternal(AtomicBTreePartition.java:132) at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:192) at org.apache.cassandra.db.memtable.SkipListMemtable.put(SkipListMemtable.java:135) at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1424) at org.apache.cassandra.db.CassandraTableWriteHandler.write(CassandraTableWriteHandler.java:40) at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:672) at org.apache.cassandra.db.Keyspace.applyFuture(Keyspace.java:489) at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:228) at org.apache.cassandra.hints.Hint.applyFuture(Hint.java:109) at org.apache.cassandra.hints.HintVerbHandler.doVerb(HintVerbHandler.java:116) at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78) at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97) ... 6 common frames omitted{code} h2. *Observed Behavior* * Node logs show *AssertionError* during mutation apply. * Tracing reports failing mutations with {{{}FAILURE_RSP{}}}. * Only operations involving the range delete ({{{}rank >= ?{}}}) cause the assertion. * Removing the DELETE eliminates the issue entirely. h2. *Expected Behavior* * Range deletes on clustering key ({{{}DELETE ... AND rank >= ?{}}}) should be processed safely. * No assertion failures during normal write path operations. * Tombstone handling in {{RangeTombstoneList.clone()}} should not produce null buffer conditions.\nComments: No comments.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21017", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21017"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: More Metrics\nDescription: Add AccordExecutorMetrics Also: - Test/fix Sharded/LogLinearHistogram - Introduce Sharded/LogLinearDecayingHistogram\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21016", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21016"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support resumption of bulk transfers after coordinator down or replaced\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21015", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21015"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support resumption of bulk transfers after restart\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21014", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21014"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add fuzz testing for read monotonicity properties\nDescription: Mutation tracking is intended to provide read monotonicity, and we\u2019ve found and addressed several potential monotonicity violations in the design phase. We should introduce a read monotonicity test harness that helps us maintain this guarantee. Right now the AST fuzz tests run commands in serial and validate complete visibility of reads and writes. A concurrent version that only validates read monotonicity could enable more aggressive failures, like breaking quorum availability. Modeling read monotonicity is simpler than complete read-write visibility, because we only need to track the latest value for each key read, and ensure that subsequent reads do not see an older value. We could exercise partial repair (via --force) and failed writes with message filters (org.apache.cassandra.distributed.impl.AbstractCluster#filters).\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21013", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21013"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Improve DurabilityQueue\nDescription: Improve DurabilityQueue: - All retries are appended to a delay queue so overlaps can be pruned - Quorum successes are not retried if there are superseding sync points covering the ranges - User-initiated requests are not purged unless the request has timed out or otherwise completed - Overlapping requests are queued up against the next to run Alsp (C*): - Catch-up with quorums on restart - Manage an ordered set of keys in cache for faster range searches Also (Accord): - Update copy of BTree and import IntervalBTree - Fix RedundantStatus WAS_OWNED_OVERRIDE_MASK - Add Catchup mechanism to reach parity with a quorum on restart\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21012", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21012"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make CQLSSTableWriter to be able to specify id's of created tables (legacy / uuid)\nDescription: There is no way to specify that a user wants SSTables to be created with UUID identifiers (as introduced / backed by uuid_sstable_identifiers_enabled: true flag in cassandra.yaml). We have evaluated that this is not necessary for Cassandra Analytics jobs because even they are created with legacy naming, they will be effectively renamed when SSTables are _imported_. However, if somebody looks for writing SSTables programmatically by CQSSTableWriter, outside of the context of Cassandra Analytics, and they want to just dump such SSTables to a (running) node with {{uuid_sstable_identifiers_enabled: true}}, and then they either start a node or refresh it (nodetool refresh), SSTables are not \"renamed\". Hence, there should be a possibility to specify how we want to name our SSTables upon creation.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21011", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21011"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add CQL syntax in Cassandra for CEP-44\u2019s Data Source/Sink Support\nDescription: {color:#000000}This proposal is Phase 1 in a larger effort to include CQL support for CEP-44 logic. This task will make the necessary CQL syntax in apache/cassandra, including keyword parsing, grammars and their respective Java statements for DATA SOURCE and DATA SINK.{color} [Section of CEP-44|[https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-44:+Kafka+integration+for+Cassandra+CDC+using+Sidecar#CEP44:KafkaintegrationforCassandraCDCusingSidecar-CQLconfiguration]]\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21010", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21010"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness enable/disable implementation, testing, documentation\nDescription: The original process for enabling/disabling witnesses was that enabling them entailed changing the schema and then running nodetool cleanup. I believe this is still the case. The original process for disabling witnesses is reduce the number of witnesses to 0 and then increase the number of full replicas. With say 6 replicas total and only 4 full replicas this means f will be 1 in terms of availability. In terms of write durability since quorum will be 3/4 you can still lose 2 nodes without losing any writes. This may be undesirable, but fine. If you ran RF=5 (or had only 3 full replicas) write durability would only allow for 1 lost replica which is more problematic. You would also immediately lose availability with only 3 full replicas and 3 witnesses when you attempted to disable the witnesses. The code that documents this is: {code:java|title=AlterKeyspaceStatement.java} //This is true right now because the transition from transient -> full lacks the pending state //necessary for correctness. What would happen if we allowed this is that we would attempt //to read from a transient replica as if it were a full replica. if (oldFull > newFull && oldTrans > 0) throw new ConfigurationException(\"Can't add full replicas if there are any transient replicas. You must first remove all transient replicas, then change the # of full replicas, then add back the transient replicas\"); {code} It would be more optimal to have a pending state for witnesses and then add them back to the read data placement when repair completes.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21009", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21009"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: nodetool sjk mxdump ignores -q option\nDescription: If I invoke a command like this: {code:java} ./bin/nodetool sjk mxdump -q \"org.apache.cassandra.metrics:name=BlockedOnAllocation,type=MemtablePool\" {code} a full list of beans is printed, it looks like query option is not applied. Same command for 5.0.x works fine. So, it can be related to CASSANDRA-17445\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21008", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21008"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optimizing Randomized Testing for Reproducibility\nDescription: h4. Goals and Motivation Random Test is designed to help with testing systems that rely on randomness, such as those involving probabilistic behavior, concurrency, or randomized algorithms. It enables automated, repeatable testing of code that behaves differently on each run, ensuring more comprehensive test coverage. Key benefits: {*}Test Uncertainty{*}: It provides tools for testing systems where inputs, behaviors, or results vary across test runs. {*}Reproducibility{*}: It helps ensure tests can be reliably reproduced, even with randomness involved. {*}Stress Testing{*}: It is ideal for identifying edge cases and bugs that might not be easily found in deterministic tests. {*}Better Test Coverage{*}: Randomized tests provide more varied input combinations, leading to more thorough testing. In short, Random Test ensures the robustness and reliability of systems dealing with randomness, making it essential for testing complex or non-deterministic behavior. h4. Approach {code:java} private static final String RANDOMIZED_CONTEXT_FQN = \"com.carrotsearch.randomizedtesting.RandomizedContext\"; Class.forName(RANDOMIZED_CONTEXT_FQN); // sets isRandomTestMode = true{code} We use this approach to determine whether we are in Random Test Mode or Normal Mode (if we can find the FQN, we activate the test infrastructure). {code:java} Randomness.java public static Random get() { if (isRandomTestMode()) { return our ThreadLocal seedable random implementation; } else { return java native Random implementation; } } {code} We centralize all random operations in *_Randomness.java_* to make them repeatable, unified, and aggregated (when in Random Test Mode, all randomness becomes reproducible and deterministic). For example: 1. Java Random APIs (SecureRandom, Collections.shuffle, UUID.randomUUID(), Math.random(), etc.) 2. Additional random utilities (randomUTF8, randomLocale, randomDate, randomAsciiLetters, etc.) We print the seed when a unit test fails: {code:java} java.lang.RuntimeException: java.lang.ExceptionInInitializerError at __randomizedtesting.SeedInfo.seed([FFB4961AF9935703:3AFD18F7B24FCA34]:0) at org.apache.cassandra.distributed.impl.Instance.lambda$startup$7(Instance.java:675) ... {code} Then use the following command to reproduce the failure: {code:java} ant testsome -Dtest.name=org.apache.cassandra.db.marshal.MyRandomizedTest -Dtests.seed=FFB4961AF9935703 {code} We can also print JVM system properties, memory, and CPU information for additional debugging context. h4. Proposed Changes {code:java} Gossiper#maybeGossipToUnreachableMember // Before double randDbl = random.nextDouble(); // After double randDbl = Randomness.get().nextDouble(); {code} h4. Test Plan We will refactor all random test suites. Existing test suites will verify these changes. Additionally, we will add unit tests for Randomness. h4. Rejected Alternatives: Why not use carrotsearch randomizedtesting directly? Elasticsearch and Lucene have successfully used randomizedtesting (Apache-2.0 license) for decades([7]), and it's already present in Cassandra. However, we reject this approach for the following reasons: {*}1. Initialization timing issues{*}: Using Java reflection to call *_{{RandomizedContext.current().getRandom()}}_* ([7]) causes {{java.lang.ExceptionInInitializerError}} due to class initialization timing at startup. {*}2. Boundary limitations{*}: randomizedtesting accesses random contexts within {{@BeforeClass}} and {{@AfterClass}} boundaries, but static test class initializers cannot access random contexts. However, we need random behavior during server startup. {*}3. Distributed JVM test incompatibility{*}: randomizedtesting is not compatible with distributed JVM tests. Our native Java approach works seamlessly with distributed JVM tests, simulators, and QuickTheories. *3.1 Possible* {*}approach{*}{*}:{*} For unit tests: {code:java} public class UUIDTypeTest extends RandomTestSuite @Listeners({ ReproduceInfoPrinter.class }) @ThreadLeakScope(ThreadLeakScope.Scope.NONE) @SeedDecorators({ MixWithSuiteName.class }) @TestMethodProviders(value = { JUnit4MethodProvider.class }) public abstract class RandomTestSuite extends RandomizedTest{code} For distributed JVM tests: {code:java} @RunWith(RandomizedRunner.class) @Listeners({ ReproduceInfoPrinter.class }) @ThreadLeakScope(ThreadLeakScope.Scope.NONE) @SeedDecorators({ MixWithSuiteName.class }) public class HintDataReappearingTest extends AbstractHintWindowTest{code} {*}3.1.1 Limitations of this p{*}{*}ossible{*} *approaches:* Some distributed JVM tests cannot obtain {{RandomizedContext}} Tests using {{@RunWith(Parameterized.class)}} or {{@RunWith(BMUnitRunner.class)}} cannot work with {{@RunWith(RandomizedRunner.class)}} since only one {{@RunWith}} annotation is allowed per test Extending {{RandomTestSuite}} or using {{@RunWith(RandomizedRunner.class)}} requires a base test class, causing extensive code changes that are difficult to review(every test needs to change) h4. Reference [1] [https://labs.carrotsearch.com/randomizedtesting-concept.html] [2] [https://get.carrotsearch.com/presentations/dweiss-barcelona-testing-2011.pdf] [3] [https://spinscale.de/posts/2020-04-22-testing-and-releasing-elasticsearch-and-the-elastic-stack.html] [4] [https://github.com/randomizedtesting/randomizedtesting] [5] [https://labs.carrotsearch.com/randomizedtesting.html] [6] [https://github.com/randomizedtesting/randomizedtesting/?tab=Apache-2.0-1-ov-file#] [7]https://github.com/elastic/elasticsearch/blob/004fab6627b0ae65cf939e0d69a4db8dbd175798/server/src/main/java/org/elasticsearch/common/Randomness.java#L93 [8] [https://www.youtube.com/watch?v=zD57QKzqdCw]\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21007", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21007"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: cassandra-stress defaults to deprecated TLS 1.2 cipher suite\nDescription: From what I understand, this cipher, used as the default in {*}cassandra-stress{*}, is not compatible with TLS 1.3's handshake mechanism as it lacks Perfect Forward Security (PFS). {code:java} final OptionSimple ciphers = new OptionSimple(\"ssl-ciphers=\", \".*\", \"TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA\", \"SSL: comma delimited list of encryption suites to use\", false); {code} [stress/settings/SettingsTransport.java|https://github.com/apache/cassandra/blob/cassandra-4.1/tools/stress/src/org/apache/cassandra/stress/settings/SettingsTransport.java#L80]\nComments: No comments.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21006", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21006"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: PrepareJoin should be rejected if tokens are already assigned\nDescription: Currently a {{PrepareJoin}} transform can succeed if the tokens to be assigned to the joining peer already belong to another node. This is not especially unsafe as the join operation itself will be rejected at the {{StartJoin}} phase, but it is suboptimal as the planned operation has to then be cancelled in order to unlock the ranges.\nComments: - CI results to follow", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21005", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21005"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Node registration can use incorrect Location if its address belongs to a previously decommissioned peer\nDescription: When a node is decommissioned, its entry in the cluster metadata directory is retained with a LEFT status. This can cause a new node joining later with the same address to adopt the previous node's {{Location}} info (rack/dc) and use that when it registers rather than obtaining its actual location from the configured {{InitialLocationProvider}}.\nComments: - CI results to follow", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21004", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21004"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Potential NPE when meta keyspace placements are empty before CMS is initialized\nDescription: During an upgrade to trunk there is a window before the CMS is initialized where the {{system_cluster_metadata}} keyspace exists but is not replicated by any peers. At this time, if a JMX client invokes {{getRangeToEndpointMap}} on the {{org.apache.cassandra.db:type=StorageService}} mbean for this keyspace, it will trigger an NPE on the server.\nComments: - CI results to follow - CI looks reasonable, only previously failing tests and nothing related to this patch.", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21003", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21003"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Truncate throws NPE in presence of a vestigial hibernating instance in gossip\nDescription: It is possible for peers which have never joined the ring to leave traces in gossip state long after the actual process has died. These vestigial entries in gossip state can persist indefinitely and due to recent changes in the {{Gossiper}} they can impact the check that all known peers are UP that is performed when truncating a table.\nComments: - CI results to follow", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21002", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21002"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Retry without time limit calculates wait time incorrectly\nDescription: The Retry instances returned by {{Retry::withNoTimeLimit}} don't properly convert from the wait time obtained from their delegate in nanos to the timeunit requested by the caller. For callers which request a wait time not in nanos such as {{Retry::maybeSleep}} which is used, for instance, when running Paxos repair during CMS reconfiguration, the sleep time is inflated by orders of magnitude. Compounding this issue, when a node starts up and detects that either its broadcast address or release version has changed, it submits a STARTUP transformation to the CMS with the updated details. For this one submission, a custom retry strategy is used which is intended to keep on trying to commit indefinitely rather than stopping after a maximum number of attempts. There are issues with the implementation of this strategy which causes the default sleep time between retries to be approximately 198 weeks.\nComments: - CI results to follow - CI looks reasonable, only previously failing tests and nothing related to this patch", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21001", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21001"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Don't submit AlterSchemaStatements which produce no effect locally to the CMS\nDescription: If a DDL statement produces no effect when the coordinator performs the pre-submission application of the schema transformation, we can skip sending it to the CMS and return a Void response. The initial implementation would submit any {{AlterSchemaStatement}} which did not generate an error but this can lead to many DDL operations which are in fact no-ops being committed to the log. One example would be a client application which performs a series of DDL statements with {{IF NOT EXISTS}} clauses whenever it starts up. This is strictly correct, as the operations will all be no-op'd when applied by the nodes in the cluster, but is somewhat wasteful and not intuitive.\nComments: - The CI results are not pretty, but all of failures are either preexisting, due to some environmental issue with our CI system (e.g. nodetool stderr includes {{JAVA_TOOL_OPTIONS}}), or cannot be reproduced locally. - Added CI results from https://pre-ci.cassandra.apache.org/job/cassandra/169/", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-21000", "url": "https://issues.apache.org/jira/browse/CASSANDRA-21000"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Deleted columns are forever part of SerializationHeader\nDescription: If you delete a column and rewrite the SSTable the column is removed from the data, but the serialization header refers to the deleted column still. This means if you drop a column and rewrite sstables (eg. nodetool upgradesstables -a) and that column is not in use, you still can not import or load those SSTables into another cluster without also having to add/drop columns. {noformat} ~/.ccm/test/node1/data0/test $ ~/bin/cqlsh Connected to repairtest at 127.0.0.1:9042 [cqlsh 6.2.0 | Cassandra 5.0.5-SNAPSHOT | CQL spec 3.4.7 | Native protocol v5] Use HELP for help. cqlsh> CREATE TABLE test.drop_test(id int primary key, message text, col_to_delete text); cqlsh> INSERT INTO test.drop_test(id, message, col_to_delete) VALUES (1, 'test', 'delete me'); cqlsh> SELECT * FROM test.drop_test; id | col_to_delete | message ----+---------------+--------- 1 | delete me | test (1 rows) ~/.ccm/test/node1/data0/test $ ccm flush ~/.ccm/test/node1/data0/test $ cd drop_test-7a20f690ba8611f09c6c3125f1cbdf37 ~/.ccm/test/node1/data0/test $ ls nb-1-big-CompressionInfo.db nb-1-big-Digest.crc32 nb-1-big-Index.db nb-1-big-Summary.db nb-1-big-Data.db nb-1-big-Filter.db nb-1-big-Statistics.db nb-1-big-TOC.txt ~/.ccm/test/node1/data0/test $ /.ccm/repository/5.0.3/tools/bin/sstabledump nb-1-big-Data.db [ { \"table kind\" : \"REGULAR\", \"partition\" : { \"key\" : [ \"1\" ], \"position\" : 0 }, \"rows\" : [ { \"type\" : \"row\", \"position\" : 18, \"liveness_info\" : { \"tstamp\" : \"2025-11-05T20:32:17.946616Z\" }, \"cells\" : [ { \"name\" : \"col_to_delete\", \"value\" : \"delete me\" }, { \"name\" : \"message\", \"value\" : \"test\" } ] } ] } ]% ~/.ccm/test/node1/data0/test $ ~/bin/cqlsh Connected to repairtest at 127.0.0.1:9042 [cqlsh 6.2.0 | Cassandra 5.0.5-SNAPSHOT | CQL spec 3.4.7 | Native protocol v5] Use HELP for help. cqlsh> ALTER TABLE test.drop_test DROP col_to_delete; cqlsh> SELECT * FROM test.drop_test; id | message ----+--------- 1 | test (1 rows) ~/.ccm/test/node1/data0/test $ ccm node1 nodetool upgradesstables -- -a test drop_test ~/.ccm/test/node1/data0/test $ ls nb-2-big-CompressionInfo.db nb-2-big-Digest.crc32 nb-2-big-Index.db nb-2-big-Summary.db nb-2-big-Data.db nb-2-big-Filter.db nb-2-big-Statistics.db nb-2-big-TOC.txt ~/.ccm/test/node1/data0/test $ ~/.ccm/repository/5.0.3/tools/bin/sstabledump nb-2-big-Data.db [ { \"table kind\" : \"REGULAR\", \"partition\" : { \"key\" : [ \"1\" ], \"position\" : 0 }, \"rows\" : [ { \"type\" : \"row\", \"position\" : 18, \"liveness_info\" : { \"tstamp\" : \"2025-11-05T20:32:17.946616Z\" }, \"cells\" : [ { \"name\" : \"message\", \"value\" : \"test\" } ] } ] } ]% ~/.ccm/test/node1/data0/test $ ~/.ccm/repository/5.0.3/tools/bin/sstablemetadata nb-2-big-Data.db | grep -E 'StaticColumns|RegularColumns' StaticColumns: RegularColumns: col_to_delete:org.apache.cassandra.db.marshal.UTF8Type, message:org.apache.cassandra.db.marshal.UTF8Type{noformat}\nComments: - I am not completely sure but this might be actually by design. I will try to get to the bottom of this. - Note the related code is here [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SerializationHeader.java#L79] . {noformat} Currently, we stick to a relatively naive merge of existing global stats because it's simple and probably good enough in most situation{noformat} The dropped columns are not part of Data.db , so on the surface it seems like it would be safe to dropped them from the merge if they are a dropped column. If not, what does this break? - [~smiklosovic] , Cameron created this Jira based on a support case he had with us (Stibo Systems). We have run into this \"challenge\" and we will need to implement a workaround to this problem. We run into this whenever we \"align\" (restore a snapshot) from the original cluster to a target cluster that do not have the system_schema.dropped_columns information. So the only working \"fix\" is to add the columns (datatype is irrelevant, can be anything) and then drop them again immediately, and rerun \"nodetool refresh <table>\". This can be done obviously, but really shouldn't be required. - Unfortunately, I think this is by design to prevent edge cases. One I can think of is changing the type of a dropped column to something incompatible with the underlying data. We could add a flag to ignore these though to get around situations like this. - I am wiling to take a shot at a flag introduced into upgradesstables to knowingly remove the column in metadata to overcome issues like this. - [~brandon.williams] , changing the type of a dropped column, when would that happen? You mean what I did as a workaround to be able to refresh? add and then drop ... I couldn't use the original type as that was \"<frozen> <table>\" where <table> no longer exists. - Yes, a drop and then addition of the same name with a different type. - > One I can think of is changing the type of a dropped column to something incompatible with the underlying data. [~brandon.williams] can you expand on this further. In the case of a dropped column the column does not appear in the Data.db . So what edge case is having a deleted column in the encoding stats needed for? On side note there is protection to prevent changing to incompatible data type: {code:java} if (!type.isSerializationCompatibleWith(droppedColumn.type)) { throw ire(\"Cannot re-add previously dropped column '%s' of type %s, incompatible with previous type %s\", name, type.asCQL3Type(), droppedColumn.type.asCQL3Type()); } {code} Not sure how to get into these edge cases where you have incompatible types for a column given these safeguards exist (schema mismatch and streaming perhaps?). What would that look like in the SSTable (assuming row would have to have a subset of columns to refer to the correct type for the conflicting column since the encoding stats would have both versions) in these edge cases? - I don't really know, this isn't my code, I'm just making logical inferences. I suspect that they aren't being removed when the column isn't in the Data.db because it's probably not a trivial thing to check and compaction isn't doing it right now. Maybe there are other cases I haven't thought about. - Okay I took a deeper look into this and it is not entirely true that \"deleted columns are there for ever\". When a column is dropped and I flush data into a new SSTable, that dropped column is not there anymore. I was thinking that we will add dropped into every SSTable but that is not true, I do not know why I was thinking that ... The example Cameron showed is correct, nodetool upgradesstables does not remove dropped column _from SSTables which have it_. I did a patch which removes all dropped columns from SSTable header but there are actually two ways how to upgrade SSTables: 1) the first method is \"nodetool upgradesstables\" - this will require _live Cassandra node_ against which this is performed. 2) the second method is doing it via \"StandaloneUpgrader\" which is used in \"./bin/sstableupgrade\" - this does not need to have a node running, it just reads it from a disk. Method 1), when SSTables are upgraded against a live node, it basically creates a new SSTable in a data dir for which it generates new ID. So if you rewrite it and you have a snapshot as well (hence hard link), then it will create a new SSTable in data dir but snapshot's SSTable will be intact. Method 2), you can also point sstableupgrade to a specific snapshot of keyspace and table and it will rewrite these (third optional parameter of sstableupgrade is a snapshot name to rewrite SSTables in). That means that after rewriting, the original SSTable in data dir will be intact. I am not completely sure what approach is better. I do not think that we should enable removing this in live data dir. This ticket also covers a use case when they are _restoring a snapshot_. I can imagine that they would 1) take a snapshot on node A into directory 1 2) _now they need to rewrite SSTables to remove the dropped columns_ - this can be done against snapshot directory itself (Method 2). 3) transfer SSTables to node B and import them without any other action needed However 2) would inflate the snapshot size if some SSTables contain columns to be dropped from header, because by rewriting, we would suddenly \"materialize\" otherwise hardlinked file. What method do you prefer? for completeness: {code} CREATE KEYSPACE test2 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; CREATE TABLE test2.drop_test(id int primary key, message text, col_to_delete text); INSERT INTO test2.drop_test(id, message, col_to_delete) VALUES (1, 'test', 'delete me'); ALTER TABLE test.drop_test DROP col_to_delete; exit ./bin/nodetool flush -> this will contain dropped columns in when inspected by sstablemetadata on the other hand CREATE KEYSPACE test2 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; CREATE TABLE test2.drop_test(id int primary key, message text, col_to_delete text); INSERT INTO test2.drop_test(id, message, col_to_delete) VALUES (1, 'test', 'delete me'); exit ./bin/nodetool flush ALTER TABLE test.drop_test DROP col_to_delete; INSERT INTO test2.drop_test(id, message) VALUES (1, 'test'); exit ./bin/nodetool flush -> this will not contain dropped column in the second SSTable {code} - patch here https://github.com/apache/cassandra/pull/4474 I spent a lot of time not touching anything else but just this path. - [~blambov] what do you think about this? Upgrading / rewriting SSTables goes through compaction path you know a lot about so your insights are indeed welcome. - [~smiklosovic], I am not sure I fully understand your fix as I am not that knowledgeable on the details. It sounds like you only want to fix the snapshots, but why won't you \"allow\" us to fix the sstables (live metadata)? We do these restores quite often which means that we will need to fix the snapshots every time. We would like to fix the sstables. Why would this be a problem? We are not changing the schema (adding/dropping columns, removing tables) very often, but when we do, we want everything gone. ... and thanks for looking into this! Morten - hi [~mortenjoenby], I can definitely do that as well / this is also possible. It is just about the agreement we reach here. The most ideal outcome is to be able to rewrite this everywhere - both in snapshots as well as in live data if somebody really wants that to happen, I just find this operation a little bit risky, based on the comments above, so ... [~brandon.williams] would you be OK with the patch / approach as such? - BTW [~smiklosovic] , can this be merged to 4.1 as well as 5.0+ ? - bq. would you be OK with the patch / approach as such? I'm not going to review this, we should loop in someone more knowledgeable who understands how and why this is like this now. [~mck] do you have any input here? - Do I understand correctly that the problem is that sstable files remember dropped columns for which they contain no data? I don't see a reason why this should be the case; the purpose of the columns definition in the sstable header is to be able to read the data correctly. If data for a column is not present, its definition should not be needed. The database state itself, however, should not forget dropped column definitions, because this may cause problems when the column is recreated with a different type. - I see it the same way, [~blambov], but then there is also this and I am not sure what was meant by that (1). Why not to just take the current state of columns in TableMetadata (as proposed in the patch) but we are instead of that adding all columns from all SSTables we go to compact? That will also contain the ones which were dropped in the meanwhile (as some SSTable to compact might contain columns in its header which were dropped by the time we hit this code execution). EDIT: ah wait ... we iterate over SSTables like that because these SSTables do not need to contain data for _current columns_. SSTable1 - columns a, b, c SSTAble 2 - columns a, b, c ALTER TABLE add a column D SSTable 3 - columns a, b, c, d Then when that set of SSTables going into \"make\" method is just SSTable1 and 2, then we do not want to use the latest columns for that table (a, b, c, d) because SSTable 1 and 2 actually do not contain it. I will manually run couple examples locally to see how it behaves under different conditions / scenarios like described above to verify. (1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SerializationHeader.java#L93-L100 - > Do I understand correctly that the problem is that sstable files remember dropped columns for which they contain no data? Yes > I don't see a reason why this should be the case; the purpose of the columns definition in the sstable header is to be able to read the data correctly. If data for a column is not present, its definition should not be needed. That was my thoughts too. I don't see why a dropped column that not present in the Data.db component being needed in the serialization header.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20999", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20999"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Cassandra 4.0.18. Apache Commons Codec Base32 Decoding Invalid String Causing Tunneling Weakness\nDescription: h1. Apache Commons Codec Base32 Decoding Invalid String Causing Tunneling Weakness\nComments: - Is Cassandra vulnerable to the above vulnerability reported in commons-codec-1.9.jar. [Apache Commons Codec information disclosure undefined Vulnerability Report|https://exchange.xforce.ibmcloud.com/vulnerabilities/177835] The CVSS 3.0 Base Score is 7.5 - This only affects base32/64 and can be suppressed. - [~brandon.williams] Will the upcoming 4.0.x version of Cassandra upgrade the jar, I see the same jar commons-codec-1.9.jar in 4.0.19 as well.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20998", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20998"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Improve Topology Management\nDescription: Merge ConfigurationService with TopologyManager to remove cyclic dependency and duplicated work. Also: - Improve visibility of work blocking topology processing - Ensure we cannot double-count peers when deciding an epoch's distributed readiness - Remove the possibility of distributed stalls, by processing new topologies as soon as a contiguous sequence is known locally, regardless of whether anyprior local epoch is ready to coordinate or has recorded this fact to peers. The notification of local readiness continues to be processed only once all prior epochs are ready, but we can begin using and readying later epochs immediately.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20997", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20997"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support Java 21 and Python 3.13\nDescription: Using the cassandra install for debian using the latest deb of 5.0.6, the database does not come up with JDK-21: {{java.lang.UnsupportedOperationException: The Security Manager is deprecated and will be removed in a future release}} This can be fixed with {{JVM_EXTRA_OPTS=\"$JVM_EXTRA_OPTS -Djava.security.manager=allow\"}} {{in /etc/cassandra/cassandra-env.sh}} then after {{systemctl restart cassandra}} the `cqlsh` fails with {{Warning: unsupported version of Python, required 3.6-3.11 but found 3.13}} {{No appropriate Python interpreter found.}} I would like to see a project of this size to support a more recent technology stack as well. Is there any plan to fix these issues? Thanks!\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20996", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20996"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Auto-repair scheduler should use LWTs for all auto-repair history operations\nDescription: The auto-repair scheduler seems to use LWTs only for a subset of the mutations on auto-repair history. This has lead to situations where we have one node trying to delete an entry from the auto-repair history and another node trying modify this entry at the same time. In the end, we end up with a corrupted entry in the auto-repair history. We should fix this by making the auto-repair scheduler use LWTs for all auto-repair system table mutations.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20995", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20995"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Auto-repair scheduler waits for repair interval to pass before cleaning up orphaned node history\nDescription: When a node in the ring goes down, the auto-repair scheduler of all other nodes in the cluster start voting for this node's auto-repair history to be removed. Once >50% of the cluster vote to delete said down node, it's auto-repair history is deleted from the auto-repair system tables. This cleanup process is important to maintain continuous execution of repair across the entire cluster. If a node goes down, it will no longer perform repair and will not update its repair history in the system tables. However, as it is still present in the auto-repair history table, the down node will still be considered as a candidate to run repair. As a result, it will occupy space in the auto-repair queue and in cases with low auto-repair parallelism may even completely block auto-repair within the cluster. This is exactly what happened on of our small clusters where auto-repair parallelism was just one node at a time. A node got replaced but its repair history did not get cleaned up which caused the entire auto-repair system to grind to a halt. Upon investigation we found out that the root cause lies in the ordering of operations within the auto-repair scheduler: # The scheduler will check when was the last time the local node ran repair. # If that duration is lower than the repair interval, it will immediately short circuit. # Otherwise, it will proceed with computing the auto-repair queue and determining if it's the local node's turn to run repair. Importantly, the auto-repair history cleanup happens inside of the auto-repair queue algorithm. This means that a given node will clean up orphaned entries in auto-repair history only once its repair interval passes. For example: if you use auto-repair parallelism of 1 node and a repair interval of 24 hours, the orphaned data will not get cleaned up for up to 24 hours and consequently auto-repair may get stuck for up to 24 hours as well.\nComments: - [~chovatia.jaydeep@gmail.com] what do you think? Could you review? - Sure [~smiklosovic], I will prioritize reviewing this. - please hold off for a tiny bit. I still want to cleanup the PR description and such.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20994", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20994"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Drop commons-lang3 dependency\nDescription: Currently Cassandra uses only a few classes from commons-lang3, and it would probably be worth dropping the dependency for the following reasons: 1) Better security. {{commons-*}} follows \"all features in a single jar\" pattern, so a CVE in one of the classes would impact Cassandra 2) Fewer bytes to ship with binary distribution. `commons-lang3` is ~690K I have raised a suggestion to make {{commons-lang3}} modular and extract modules like {{commons-stringutils}}, {{commons-arrayutils}}, however, Commons team does not seem to like the idea. Commons PMC members often suggest that users should clone the code or shade commons-lang, see https://lists.apache.org/thread/xzdhv57o9rnxtzn5fqbtkzj0hdkbm339 So I wonder what do you think of dropping commons-lang3 and replacing it with core Java?\nComments: - Could you please ask on Cassandra's development mailing list? I think it is better to be discussed there. https://cassandra.apache.org/_/community.html#discussions Could you also say what you want to replace and with what exactly? (please do so on ML). - Usages of commons-lang3, unfortunately it is not only StringUtils: {code:java} grep -r \"org.apache.commons.lang3\" --no-filename | sort | uniq import org.apache.commons.lang3.ArrayUtils; import org.apache.commons.lang3.ObjectUtils; import org.apache.commons.lang3.StringUtils; import org.apache.commons.lang3.builder.EqualsBuilder; import org.apache.commons.lang3.builder.HashCodeBuilder; import org.apache.commons.lang3.builder.ToStringBuilder; import org.apache.commons.lang3.builder.ToStringStyle; import org.apache.commons.lang3.exception.ExceptionUtils; import org.apache.commons.lang3.math.NumberUtils; import org.apache.commons.lang3.mutable.Mutable; import org.apache.commons.lang3.mutable.MutableByte; import org.apache.commons.lang3.mutable.MutableDouble; import org.apache.commons.lang3.mutable.MutableFloat; import org.apache.commons.lang3.mutable.MutableInt; import org.apache.commons.lang3.mutable.MutableLong; import org.apache.commons.lang3.mutable.MutableShort; import org.apache.commons.lang3.text.StrBuilder; import org.apache.commons.lang3.time.DurationFormatUtils; import org.apache.commons.lang3.tuple.ImmutableTriple; import org.apache.commons.lang3.tuple.Pair; import org.apache.commons.lang3.tuple.Triple; import static org.apache.commons.lang3.ArrayUtils.EMPTY_STRING_ARRAY; import static org.apache.commons.lang3.ArrayUtils.isEmpty; import static org.apache.commons.lang3.StringUtils.EMPTY; import static org.apache.commons.lang3.StringUtils.isEmpty; import static org.apache.commons.lang3.StringUtils.isNotEmpty; import static org.apache.commons.lang3.StringUtils.join; import static org.apache.commons.lang3.time.DateUtils.MILLIS_PER_DAY; import static org.apache.commons.lang3.time.DurationFormatUtils.formatDurationWords; {code}", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20993", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20993"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Unnecessary write timeouts during node replacement due to conservative pending node accounting in consistency level calculations\nDescription: h3. Background Cassandra includes pending nodes in the write quorum calculation to ensure consistency level guarantees are maintained during topology changes (e.g., bootstrap, node replacement). This is implemented in ConsistencyLevel.blockForWrite() where pending replicas are added to the required blockFor count. h3. Current Behavior When a node is bootstrapping or being added to the cluster, all pending replicas are unconditionally added to the blockFor requirement, regardless of whether they are replacement nodes or new capacity additions.Example with RF=3, CL=LOCAL_QUORUM: * New node bootstrap (increasing capacity): blockFor = 2 (quorum) \u2192 with pending = 3 * Node replacement (old node being replaced): blockFor = 2 \u2192 with pending = 3 h3. Problem Statement During a node replacement scenario where a pending node is joining to replace an existing node:Current situation: * Natural replicas: A, B, C (C is being replaced) * Pending replica: D (replacement for C) * Live replicas: A, B, D (3 live nodes after C goes down) * Required ACKs: 3 (base quorum of 2 + 1 pending) The issue: # Although we have 3 live replicas capable of responding, the blockFor requirement of 3 is artificially inflated # The pending replica (D) is typically busy during bootstrap and may respond slowly # Even though we've received ACKs from A and B (satisfying quorum of natural replicas), the write blocks waiting for an ACK from the slow pending replica D # This unnecessary dependency on the replacement pending replica's responsiveness causes write timeouts during node replacement operations h3. Root Cause As the original comment in the ticket https://issues.apache.org/jira/browse/CASSANDRA-833 mentioned: {quote}we want to satisfy CL for both the pre- and post-bootstrap nodes (in case bootstrap aborts). This requires treating the old/new range owner as a unit: both D *and* C need to accept the write for it to count towards CL. So rather than considering Unknown macro: \\{A, B, C, D} we should consider Unknown macro: \\{A, B, (C, D)} This is a lot of complexity to introduce. {quote} the current implementation is a \"simplification\" idea. The current implementation conflates natural and pending replicas into a single blockFor calculation: * blockFor = quorum(all replicas including pending) However, the two replica types serve different purposes: * Natural replicas: The authoritative owners of the data (by replication strategy) * Pending replicas: Temporary nodes either adding capacity (new bootstrap) or replacing an existing node (node replacement) The current approach treats all pending nodes identically, but they should be handled differently based on their topology role. h3. Proposed Solution For quorum consistency level: Decouple blockFor calculation into separate requirements for natural and pending replicas:For normal operations and new bootstrap (Keep current behavior as the implementation might be complex and we can do later): * blockFor = quorum(natural replicas) + all pending replicas * This ensures: quorum of natural replicas respond, PLUS any pending nodes respond * Protects against topology change cancellation {color:#de350b}For node replacement:{color} * {color:#de350b}blockFor = quorum(natural replicas) only{color} * {color:#de350b}This ensures: only quorum of natural replicas need to respond{color} * {color:#de350b}Pending replacement node responds when available but is not required{color} * {color:#de350b}Eliminates unnecessary dependency on busy pending replica during replacement{color} h3. Implementation For normal node bootstrap, we will block as what we are doing now as it is too complicate to determine the unit of a (new owner and old owner). For node replacements, as the old node is down, the (new node + old node) unit cannot response the write anyway. We will just block for the consistency level(natural replicas). The implementation should not be complicated as the node replacement relationship is clear.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20992", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20992"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix ExceptionsTable when stacktrace has zero elements\nDescription: I saw this in one of pre-ci runs: {code} 02:01:57 [junit-timeout] ERROR [Reconcile:2] <main> 2025-10-30T02:01:57,672 ExecutionFailure.java:77 - Unexpected error while handling unexpected error 02:01:57 [junit-timeout] java.lang.IndexOutOfBoundsException: Index 0 out of bounds for length 0 02:01:57 [junit-timeout] at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64) 02:01:57 [junit-timeout] at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70) 02:01:57 [junit-timeout] at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248) 02:01:57 [junit-timeout] at java.base/java.util.Objects.checkIndex(Objects.java:374) 02:01:57 [junit-timeout] at java.base/java.util.ArrayList.get(ArrayList.java:459) 02:01:57 [junit-timeout] at org.apache.cassandra.db.virtual.ExceptionsTable.persist(ExceptionsTable.java:182) 02:01:57 [junit-timeout] at org.apache.cassandra.utils.JVMStabilityInspector.uncaughtException(JVMStabilityInspector.java:74) 02:01:57 [junit-timeout] at org.apache.cassandra.concurrent.ExecutionFailure.handle(ExecutionFailure.java:72) 02:01:57 [junit-timeout] at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:142) 02:01:57 [junit-timeout] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) 02:01:57 [junit-timeout] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) 02:01:57 [junit-timeout] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) {code} That is because we do \"stackTrace.get(0)\" in ExceptionsTable.persist(Throwable) and there is wrong assumption that stacktrace will always have some elements but it might be actually also empty.\nComments: - https://github.com/apache/cassandra/pull/4453 - +1 if CI looks good - +1 - I dont see failing tests related to this change https://pre-ci.cassandra.apache.org/job/cassandra/141/#showFailuresLink", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20991", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20991"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Drop table vs truncate table commands behaviors\nDescription: We are experimenting with drop table and truncate command and are observing conflicting behaviors while having nodes down. Drop table is able to execute even though nodes are down, if the dropped table is later re-created it will get a new schema ID assigned. However, truncate requires all nodes to be up for truncate to work. Even though the nodes are not part of the replicas for the keyspace. Is this expected behavior for the two commands? I would expect the following: - Truncate to only require replicas of the keyspace to be up - Drop table to require all nodes to be up as schema ID is re-created (if table is re-created)\nComments: - Truncate requires all nodes to be up until CASSANDRA-19130 is completed.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20990", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20990"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Windows build failure: Ant task cannot execute Gradle wrapper under modules/accord (error=193)\nDescription: The build process fails on Windows when running the Ant task that invokes the Gradle wrapper inside the {{modules/accord}} directory. The failure is caused by the {{gradlew}} script being a Unix shell file ({{{}#!/bin/bash{}}}) rather than a Windows batch file, which triggers the {{CreateProcess error=193, %1 is not a valid Win32 application}} exception. *Environment:* OS: Windows 11 Pro (Version 10.0.26200 Build 26200) Java: Java(TM) SE Runtime Environment 18.9 (build 11.0.21+9-LTS-193) JVM: Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.21+9-LTS-193, mixed mode) Ant: Apache Ant(TM) version 1.10.15 compiled on August 25 2024 Cassandra branch: trunk (latest, cloned from apache/cassandra) *Steps to reproduce:* 1) git clone [https://github.com/apache/cassandra.git] 2) {{cd cassandra }} 3) ant build Observe the failure when the {{build-accord.xml}} script is executed: BUILD FAILED D:\\...\\cassandra\\.build\\build-accord.xml:22: Execute failed: java.io.IOException: Cannot run program \"D:\\...\\cassandra\\modules\\accord\\gradlew\" (in directory \"D:\\...\\cassandra\\modules\\accord\"): CreateProcess error=193, %1 is not a valid Win32 application\nComments: - Windows support was removed in 4.0 https://www.mail-archive.com/dev@cassandra.apache.org/msg15583.html - I hadn't found any references about it. Thank you.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20989", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20989"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Fix in-JVM tests that involve the integration of mutation tracking and witnesses\nDescription: The following tests need simple changes to enable tracked keyspaces, but that might not be enough to get them passing. Range movements are not yet supported with mutation tracking, for instance... {{TransientRangeMovement2Test}} {{TransientRangeMovementTest}} {{NTSSimulationTest}} {{CleanupTransientTest}} {{OperationalEquivalenceTest}} {{SimpleStrategySimulationTest}}\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20988", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20988"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: rpc_interface won't find veth peer interfaces\nDescription: If the network is using peered interfaces, the ability to use \"rpc_interface\" is lost since it only accepts the exact naming. This naming is however dynamic since it depends on the time the interface was generated. Virtual networks with peered interfaces have the format <network-interface>@if<running-number>. For example, the following output is possible: {noformat} 11: eth0@if20: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65535 qdisc noqueue state UP group default qlen 1000 link/ether 32:41:2a:58:15:2d brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fd00:10:244:4::a/64 scope global valid_lft forever preferred_lft forever inet6 fe80::3041:2aff:fe58:152d/64 scope link proto kernel_ll valid_lft forever preferred_lft forever {noformat} This is pretty common in virtualized environments (or Kubernetes). The way that rpc_interface works is by using NetworkInterface.getByName(intf) and this leads to exact name matching (at least in the current JDK, https://github.com/openjdk/jdk/blob/e7c7892b9f0fcee37495cce312fdd67dc800f9c9/src/java.base/unix/native/libnet/NetworkInterface.c#L202 which does not understand the @ separator. This leads to an issue where setting \"eth0\" on every node as the preferred rpc_interface (due to existence of multiple interfaces in the system with dynamic IPs) will result in Cassandra not starting. A similar issue might happen with VLANs where the naming has a running index also.\nComments: - I did quick investigation of this issue and there does not seem to be a fix. There is also {{NetworkInterface.getNetworkInterfaces()}} but that one does not seem to return peered interfaces in the above-mentioned format. So I think that {{rpc_interface}} can be used just in connection with non-peered interfaces. Maybe I am wrong though ... - This appears to be the case indeed. The syscall RTM_GETLINK just returns it without the non-peer part: Found interface: {Index:11 MTU:65535 Name:eth0 HardwareAddr:06:e3:26:48:65:e1 Flags:up|broadcast|multicast|running} Perhaps I'll just go around this with some logic in our config builder, fetching the interface and then mapping that interface to an IP instead and remove the rpc_interface parameter if user has used it. This should allow a bit easier automated interface targeting (and just reject it if there's more than one peered one). I'll close this ticket for now since it's not possible to do anything sane in Cassandra itself.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20987", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20987"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add virtual tables - MutationJournal and MutationShard\nDescription: Summary: We are adding a new virtual tables for MutationShard and MutationJournal to enable ease of operation for Mutation Tracking functionality. MutationShardTable: 1. Witnessed Offsets 2. Reconciled Offsets 3. Persisted Offsets MutationJournalTable 1. Size 2. Number of Segments 3. Segment Contents\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20986", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20986"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add metrics for Mutation Tracking\nDescription: Summary: We\u2019re going to need new metrics to make the new Mutation Tracking functionality easy to operate. Here\u2019s a basic list of what we have in mind: 1. Offsets applied via broadcast 2. Read summary sizes 3. Number of unreconciled mutations 4. Size of MutationJournal on disk\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20985", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20985"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix broken link in README.asc\nDescription: One word fix for getting started link {code:java} diff --git a/README.asc b/README.asc index e6e4fcca16..82c33c5d4c 100644 --- a/README.asc +++ b/README.asc @@ -31,7 +31,7 @@ Getting started --------------- This short guide will walk you through getting a basic one node cluster up -and running, and demonstrate some simple reads and writes. For a more-complete guide, please see the Apache Cassandra website's https://cassandra.apache.org/doc/latest/cassandra/getting_started/index.html[Getting Started Guide]. +and running, and demonstrate some simple reads and writes. For a more-complete guide, please see the Apache Cassandra website's https://cassandra.apache.org/doc/latest/cassandra/getting-started/index.html[Getting Started Guide]. First, we'll unpack our archive: {code}\nComments: - PR - https://github.com/apache/cassandra/pull/4440 - Reviewed and Tested. Changes made sense.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20984", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20984"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix java.lang.ClassCastException: Streaming Incompatible versions\nDescription: On version mismatch or protocol incompatibility between the two communicating nodes, cassandra server does not handle the errors correctly. There is no proper error handling when ClassCastException occurs, since it is not consistently reproducible. In [https://lists.apache.org/thread/ykkwhjdpgyqzw5xtol4v5ysz664bxxl3.,|https://lists.apache.org/thread/ykkwhjdpgyqzw5xtol4v5ysz664bxxl3.] the \"Stream failed\" message points to a streaming operation, which is used for processes like node repairs or adding new nodes (bootstrapping). I found a similar Jira that was already raised - https://issues.apache.org/jira/browse/CASSANDRA-19218. Not sure what to do with this JIRA. Looks like a duplicate.\nComments: - Looks like the Streaming part of cassandra is not robust like the Message handling code side of cassandra. The message handling part of the cassandra server code i.e org.apache.cassandra.net.OutboundConnection has all available information to handle every condition within code. Esp, the class org.apache.cassandra.net.OutboundConnectionInitiator$Result.Outcome is package private in org.apache.cassandra.net. I've taken a closer look at the code. I've been able to reproduce the issue with the unit test and have produced a fix. Here are my observations. The streaming side of code mostly located in class org.apache.cassandra.streaming.async.NettyStreamingConnectionFactory invokes the method initiateStreaming in class org.apache.cassandra.net.OutboundConnectionInitiator. Since they are located in different packages, the streaming code lacks the ability to perform any checks based on org.apache.cassandra.net.OutboundConnectionInitiator$Result.Outcome. Put simply the outcome field & enum of Result is inaccessible in NettyStreamingConnectionFactory class. I patched the code to perform checks based on Result.Outcome overcoming the limitation. When working on the Unit test, I also saw the inconsistency in the way casts are performed between retry, success and incompatible. In NettyStreamingConnectionFactory, there appears to be some confusion in the invocation of isSuccess() method. It actually is making the invocation on Netty Future. It should have been on the Result object. On making a successful connect, NettyStreamingConnectionFactory calls success() on Future' s getNow() without checking the type of the cast. There are no tests for initiateStreaming() method of OutboundConnectionInitiator as there are for initiateMessaging() method of OutboundConnectionInitiator. Reproduction I wrote a test (StreamingTest) that reproduces the issue in https://lists.apache.org/thread/ykkwhjdpgyqzw5xtol4v5ysz664bxxl3. Code Change I used the instanceof in https://openjdk.org/jeps/394 to make incorrect comparisons a compile-time error. This is done in OutboundConnection where I check if result.success() instanceof MessagingSuccess and OutboundConnectionInitiator where I return Success safely instead. GitHub Pull request: https://github.com/apache/cassandra/pull/4438 Please note: this change makes use of a feature in JDK 16 and thus needs a higher minimum JDK. - Related issue that was earlier reported - [~vkoya] thank you for your work! 4.1 runs at least on Java 8. CASSANDRA-19218 says this issue exists in 4.1 as well. So we will need to produce a patch which compiles to Java 8 as well. For 5.0+ branches the minimum is Java 11. If you have a patch for Java 16, can you look into how to make it work with 8 / 11, please? - Yes, thanks for the update. I incorporated your feedback in StreamingTest.java. Now the code should work with JDK 8. - Could you also please create patches / pull requests for 4.1 and 5.0 branch? The current PR is against trunk only. I think that is should be without any issues but you never know. There might be subtle differences when a patch is going to be re-applied to lower branches. Figuring out all the potential problems on author's side is preferable so busy maintainer has way less work. - I took a deeper look into the patch and I think that we should actually do something slightly different, I did it here (1). The overall hardening seems to be the right way, but we need to test whole NettyStreamingConnectionFactory.connect (and what it returns / throws). It is not enough to just call initiateStream method. There is the complication in connect method of NettyStreamingConnectionFactory that it runs over all SSL fallbacks and additionally in inner loop of max connection attempts and so on. I think we need to handle Future and its result the correct way, check if it is success and so on, this is not done right now. I also think that the change in OutboundConnection is not completely right because I am not sure what happens if this is not true: {{result.success() instanceof MessagingSuccess}}. Then what happens? It will just skip it and break? Then what? I think it is OK to put there simple assert but all changes should go to streaming code imho, as we are trying to fix the reported issue in this ticket in the first place. The patch is also clearly taking the inspiration from HandshakeTest, I am not completely sure if we should go over all of what it tests in StreamingTest. [~mck] what do you think about this? I see you have done CASSANDRA-18314 and was touching HandshakeTest. (1) https://github.com/apache/cassandra/compare/trunk...smiklosovic:cassandra:CASSANDRA-20984-trunk?expand=1 - Since we already check the outcomes we don't need to perform an instanceof check on MessagingSuccess in the code path (OutboundConnection.java). Here is a patch for 4.1: [cassandra-4.1|https://github.com/vivekkoya/cassandra/tree/cassandra-4.1] - Included tests for 4.1 - [~smiklosovic] Can we limit the scope to the defect? - hey I am busy now, please try to find somebody else to review. - Hi [~vkoya], I looked into your work and there I see a lot of commits directly in 4.1 branch. The way we prefer is to create a pull request on GitHub against 4.1 branch from your dedicated git branch (e.g. called {{CASSANDRA-20984-4.1}}) so we are able to comment on your patch and participate in the review process (not only me). You can also go over (2) to know more about the process and related details. (1) https://github.com/vivekkoya/cassandra/commits/cassandra-4.1/ (2) https://cassandra.apache.org/_/development/patches.html - Got it. Thanks for pointing me in the right direction. Pull Requests 4.1 - https://github.com/apache/cassandra/pull/4472 Trunk - https://github.com/apache/cassandra/pull/4473 5.0 - Work in Progress https://github.com/apache/cassandra/pull/4471", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20983", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20983"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Paxos repair blocks response thread\nDescription: CASSANDRA-20469 prevented paxos repairs from preempting in progress paxos transactions by holding repairs until after they would have timed out. However, it added a sleep that\u2019s often called in the request response stage, and with the PaxosCleanupLocalCoordinator lock held. This patch adjusts this by putting a delayed repair into a delayed queue, and scheduling a wakeup task for after the timeout expires.\nComments: - PR: https://github.com/apache/cassandra/pull/4434 - +1'd on PR", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20982", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20982"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: BytesType should only be compatible with simple scalar types\nDescription: Discovered while backporting CASSANDRA-18760, see https://github.com/datastax/cassandra/commit/b5f7d239ce92db7d9c5f79c69cad0caea67fcb23 Basically, things like https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/cql3/validation/operations/AlterTest.java#L71 shouldn't be allowed to pass, because converting a frozen collection to bytes allows reading the raw bytes, which is nonsensical. This was found because the AbstractTypeTest is beefed up [here|https://github.com/datastax/cassandra/blob/main/test/unit/org/apache/cassandra/db/marshal/AbstractTypeTest.java]\nComments: - [~brandon.williams], can I take this task? - Certainly! - Thanks, I'm on it! - Thank you! I'm happy to review when ready. - [~brandon.williams], done: https://github.com/apache/cassandra/pull/4439 - When are you going to have more time [~brandon.williams] for checking this? :) I was happy to help regarding the task :) - I am reading this comment, I do not remember I was involved in this anymore. It was quite a while ... https://issues.apache.org/jira/browse/CASSANDRA-18760?focusedCommentId=17755014&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17755014 I put couple comments into the PR. [~adelapena] what is the motivation behind that first sentence in the second paragraph you wrote in the above comment? - \"it should be possible to make blob columns from frozen collections.\" - Left comments on the PR, but let me clarify what I think about \"it should be possible to make blob columns from frozen collections.\" I think it should be possible if the user is being explicit, meaning a CAST to bytes. Sure, they get raw, perhaps nonsensical bytes, but they asked for them. If they are dropping a column that is a frozen collection and then adding a column with the same name as bytes, I don't think we should surprise them with junk in the column. - Ok that makes sense. I was just wondering if there was some not-so-obvious reason why we should support such a conversion. - {quote}[~adelapena] what is the motivation behind that first sentence in the second paragraph you wrote in the above comment? - \"it should be possible to make blob columns from frozen collections.\" {quote} IIRC I meant that frozen multi-value types are serialized as a single blob, whereas non-frozen multi-value types are multi-cell and don't have a clear representation as a single blob. They are rather a series of blobs spread across multiple cells, sometimes in the values of those cells, sometimes in the names, sometimes both. But I guess the casting functions could produce and consume the type of representation that is used for their frozen equivalents? - bq. But I guess the casting functions could produce and consume the type of representation that is used for their frozen equivalents? I don't have a clear use case in mind, but given it is explicit the user shouldn't be surprised at the result, and perhaps it's useful in debugging or some unconventional situation, so I don't see a need to close that door. - [~brandon.williams], [~smiklosovic], any updates on a prospective merging that solution? - stay tuned! we are doing everything we can to deliver this, patience please. - [~smiklosovic], thank you for quick response! I do not know all processes that's why I was asking. Is there any chance for merging it that month? :) - Looks good to me, let's check CI: ||Branch||CI|| |[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-20982-4.0]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/1879/workflows/80f97363-424f-4f72-b4ee-ac25586f2cdd], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/1879/workflows/6bff13fc-d977-4880-b474-2134fa1ee765]| |[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-20982-4.1]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/1878/workflows/ea1bb020-2106-43a1-88b7-dda977224414], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/1878/workflows/338ecda5-3d7e-4b5c-897c-9795e8327011]| |[5.0|https://github.com/driftx/cassandra/tree/CASSANDRA-20982-5.0]|[j11|https://app.circleci.com/pipelines/github/driftx/cassandra/1880/workflows/fdeb926e-db6f-4c67-ba7b-0456c54f2ad1], [j17|https://app.circleci.com/pipelines/github/driftx/cassandra/1880/workflows/e5963ce1-d290-4fe0-aa03-0de259aa0529]| |[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-20982-trunk]|[j11|https://app.circleci.com/pipelines/github/driftx/cassandra/1881/workflows/0fdd73e2-1dec-47a9-9de4-e52a69a9da59], [j17|https://app.circleci.com/pipelines/github/driftx/cassandra/1881/workflows/058db14a-98ef-47ab-9947-9fb3527c2c4e]| - It looks like testInvalidDroppingAndAddingOfCollections is failing now. I also removed the commit message file and made the test declare itself Throwable. - [~dekrate] hey, do you plan to return to this work? Seems like we are quite close! - [~smiklosovic], sure! What would you like me to do? - Fixed the test.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20981", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20981"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add aber's gpg key to project's KEYS file\nDescription: In the future I'd like to help in the java driver release process, so would like to have my gpg public key added to KEYS. The patch adds my gpg public key to the project's KEYS file found at https://dist.apache.org/repos/dist/release/cassandra/KEYS My gpg public key here has the fingerprint 095EDACC76DD62020FC8501B77C04F6CCB94CE02 References: https://www.apache.org/dev/release-signing#keys-policy http://www.apache.org/legal/release-policy.html\nComments: - Done in r80221.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20980", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20980"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: GCInspector should use different thresholds on GC events for warning messages\nDescription: Running Cassandra 4.1 and recently testing with ShenandoahGC. Noticed that the log message is flooded with the warning message coming from GCInspector. Taking a closer look, that seems that we're warning on every GC event that takes longer than 1 second (default). And from document that the intention was to monitor GC pause time. But for ShenandoahGC and ZGC, there are concurrent phases (no STW) and phases that actually STW. For those concurrent phases, it's expected to take longer time (if we're on the non-generational version), and I feel that these phases: 1. require STW 2. doesn't require STW should use different thresholds in warning message. At least to me, phases require STW actually pause the thread, and should use lower threshold compared to those concurrent phases. Will share a patch soon\nComments: - Trunk pr: https://github.com/apache/cassandra/pull/4433 - I've done few e2e tests for the changes by running Cassandra node built from [https://github.com/apache/cassandra/pull/4454] There are few non-committed debug logging changes to simplify analysis: logging of listen MBeans + logging of all GC events): [^debug_logging_gc_inspector.patch] The behaviour looks as expected for me. Used JDK: {code:java} openjdk version \"17.0.15\" 2025-04-15 OpenJDK Runtime Environment Temurin-17.0.15+6 (build 17.0.15+6) {code} I've run a few load tests to cause GC. Default GC inspector limits were used: gc_log_threshold = 200 ms gc_warn_threshold = 1s gc_concurrent_phase_log_threshold = 1s gc_concurrent_phase_warn_threshold = 2s h3. G1 [^system_g1.log] [^gc_g1.log] MBeans {code:java} java.lang:name=G1 Young Generation,type=GarbageCollector java.lang:name=G1 Old Generation,type=GarbageCollector {code} Pauses detection still works: {code:java} INFO [Notification Thread] 2025-11-15T16:19:13,080 GCInspector.java:312 - G1 Young Generation GC in 233ms. G1 Eden Space: 6476005376 -> 0; G1 Old Gen: 22573666304 -> 23790014464; G1 Survivor Space: 1258291200 -> 1073741824; {code} Note: we do not have an official support of JDK 21 as of now, I so have not tested G1 concurrent mode, A new JMX bean for it has been added in JDK20 ([https://bugs.openjdk.org/browse/JDK-8297754]). h3. Shenandoah [^system_shenandoah.log] [^gc_shenandoah.log] MBeans {code:java} java.lang:name=Shenandoah Pauses,type=GarbageCollector java.lang:name=Shenandoah Cycles,type=GarbageCollector {code} We log Shenandoah Cycles concurrent GCs with time > 1 sec as INFO: {code:java} INFO [Notification Thread] 2025-11-15T16:31:21,372 GCInspector.java:300 - Shenandoah Cycles GC in 1762ms. CodeHeap 'non-nmethods': 2047744 -> 3201536; CodeHeap 'non-profiled nmethods': 18649088 -> 18653952; CodeHeap 'profiled nmethods': 19999488 -> 20063744; Metaspace: 63209472 -> 63218224; Shenandoah: 17291619784 -> 6818925104 {code} We log Shenandoah Cycles concurrent GCs with time > 2 sec as WARN: {code:java} WARN [Notification Thread] 2025-11-15T16:30:53,812 GCInspector.java:298 - Shenandoah Cycles GC in 3813ms. CodeHeap 'non-nmethods': 3778432 -> 2047744; CodeHeap 'non-profiled nmethods': 18428800 -> 18444032; CodeHeap 'profiled nmethods': 19460992 -> 19521024; Metaspace: 63072296 -> 63089704; Shenandoah: 22980826480 -> 22186858112 {code} We do not log Shenandoah Cycles concurrent GCs with time < 1 sec and > 300 ms (this is the main goal of this improvement): {code:java} // it is present only in debug output but not reported by GCInspector real logging INFO [Notification Thread] 2025-11-15T16:31:50,315 GCInspector.java:265 - debug, gcId: 51, gcName: Shenandoah Cycles, gcCause: Concurrent GC, gcAction: end of GC cycle, event duration: 349, calculated duration: 348 {code} h3. ZGC [^system_zgc.log] [^gc_zgc.log] MBeans {code:java} java.lang:name=ZGC Cycles,type=GarbageCollector java.lang:name=ZGC Pauses,type=GarbageCollector {code} We log ZGC Cycles concurrent GCs with time > 1 sec as INFO: {code:java} INFO [Notification Thread] 2025-11-15T16:38:53,877 GCInspector.java:300 - ZGC Cycles GC in 1694ms. CodeHeap 'non-nmethods': 3414912 -> 1684224; CodeHeap 'non-profiled nmethods': 13763712 -> 14028160; CodeHeap 'profiled nmethods': 19596416 -> 18350848; Compressed Class Space: 7549152 -> 7551488; Metaspace: 62210520 -> 62218096; ZHeap: 6757023744 -> 9124708352 {code} We log ZGC Cycles concurrent GCs with time > 2 sec as WARN: {code:java} WARN [Notification Thread] 2025-11-15T16:38:56,746 GCInspector.java:298 - ZGC Cycles GC in 2660ms. CodeHeap 'non-nmethods': 3414912 -> 1684224; CodeHeap 'non-profiled nmethods': 14354048 -> 14567424; CodeHeap 'profiled nmethods': 18724352 -> 18601856; Compressed Class Space: 7556032 -> 7556544; Metaspace: 62223720 -> 62262456; ZHeap: 10284433408 -> 17983078400 {code} We do not log ZGC Cycles concurrent GCs with time < 1 sec and > 300 ms (this is the main goal of this improvement): {code:java} // it is present only in debug output but not reported by GCInspector real logging INFO [Notification Thread] 2025-11-15T16:42:49,461 GCInspector.java:265 - debug, gcId: 50, gcName: ZGC Cycles, gcCause: Allocation Rate, gcAction: end of GC cycle, event duration: 379, calculated duration: 380 {code} - CI run results: [https://pre-ci.cassandra.apache.org/job/cassandra/167/#showFailuresLink] [^CASSANDRA-20980_ci_summary.htm] [^CASSANDRA-20980_results_details.tar.xz] The test failures are not related to the changes: Tests / jvm-dtest jdk17 1/16 / org.apache.cassandra.distributed.test.accord.MigrationFromAccordWriteRaceTest.testSplitAndRetryNonSerialLoggedBatchTwoTablesTwoPkey-_jdk17_x86_64 Tests / jvm-dtest jdk17 1/16 / org.apache.cassandra.fuzz.sai.AccordFullSingleNodeSAITest.indexOnlySaiTest-cassandra.testtag_IS_UNDEFINED Tests / jvm-dtest jdk17 4/16 / org.apache.cassandra.distributed.test.CASMultiDCTest.testLocalSerialLocalCommit-_jdk17_x86_64 Tests / simulator-dtest jdk11 / org.apache.cassandra.simulator.test.HarrySimulatorTest.test-_jdk11_x86_64 Tests / jvm-dtest jdk17 8/16 / org.apache.cassandra.fuzz.topology.AccordBootstrapTest.bootstrapFuzzTest-_jdk17_x86_64 Tests / simulator-dtest jdk11 / org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest-cassandra.testtag_IS_UNDEFINED Tests / simulator-dtest jdk11 / org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest-_jdk11_x86_64 Tests / dtest jdk11 34/64 / dtest.client_request_metrics_test.TestClientRequestMetrics.test_client_request_metrics Tests / dtest jdk17 34/64 / dtest.client_request_metrics_test.TestClientRequestMetrics.test_client_request_metrics - [~yukei] thank you a lot for the contribution! - Thank you [~dnk] and [~stefan.miklosovic] !!", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20979", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20979"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CQLSSTableWriter fails to serialize vector of date and time\nDescription: C* 5.x technically allows to create schema of vectors of variable types, e.g. non-numeric. {{CQLSSTableWriter}} fails to serialize vectors of dates and time. Stack trace: {code:java} org.apache.cassandra.exceptions.InvalidRequestException: Expected 4 byte long for date (0) at org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest(RequestValidations.java:358) at org.apache.cassandra.cql3.terms.MultiElements$Value.fromSerialized(MultiElements.java:73) at org.apache.cassandra.cql3.terms.Marker.bind(Marker.java:85) at org.apache.cassandra.cql3.terms.Term$NonTerminal.bindAndGet(Term.java:304) at org.apache.cassandra.cql3.terms.Constants$Setter.execute(Constants.java:482) at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:126) at org.apache.cassandra.io.sstable.CQLSSTableWriter.rawAddRow(CQLSSTableWriter.java:311) at org.apache.cassandra.io.sstable.CQLSSTableWriter.addRow(CQLSSTableWriter.java:212) at org.apache.cassandra.io.sstable.CQLSSTableWriter.addRow(CQLSSTableWriter.java:182) {code}\nComments: - bq. fails to serialize vectors of dates and time Does a vector of a time/date even make sense to have? If not it seems like we should just disallow this. - I think C* first supported vectors of numeric (fixed-length) types, but then it was extended to any sub-type. In drivers, we also implemented support for literally any vector sub-type (e.g. vector of blobs) - [JAVA-3143|https://datastax-oss.atlassian.net/browse/JAVA-3143]. FWIW those changes test also vectors of text: [CASSANDRA-18613|https://issues.apache.org/jira/browse/CASSANDRA-18613] (https://github.com/apache/cassandra/commit/ddbc52990f90473db729e96f22d2914e51a957a6). I agree that support for it is completely debatable, but currently it is fair to say that C* supports all vector subtypes. {code:java} cqlsh:test> create table vector_text_list(x int primary key, y vector<list<text>, 2>); cqlsh:test> insert into vector_text_list(x, y) values(1, [['hello', 'world'], ['a', 'b', 'c']]); cqlsh:test> {code}", "output": "Status: Triage Needed\nPriority: Low"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20978", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20978"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: JVMStabilityInspector does not call LoggingSupport#onShutdown() before killing the JVM\nDescription: `Killer#killCurrentJVM()` does not stop the `LoggerContext`, and this can hide error messages logged immediately prior to that. (Before we exit, we remove the drain-on-shutdown hook that otherwise would stop it.)\nComments: - The only possibly tricky thing here is that stopping the logging context might hang if the reason we're killing the JVM is an OOM. It might make sense to initially limit a fix to paths that run through {{{}inspectCommitLogError(){}}}, {{handleStartupFSError()}} and {{userFunctionTimeout()}} with a new parameter on \\{{killCurrentJVM()}}. - +1 (CI in progress...) - +1, pending CI", "output": "Status: Review In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20976", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20976"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: AssertionError when reading BTI sstable in 5.0.5 during token range query\nDescription: https://lists.apache.org/thread/p2fvjll2gcx15jyc5ft32z6950k2g880\nComments: - h3. Observations and Findings with respect to attached logs 20976.logs * The issue occurs with *token-based range queries* when the *BTI format* is enabled. * The use case involved a Spark job that was concurrently reading from and updating the same table \u2014 specifically, performing range reads while updating a field in that table. * Immediately following the *AssertionError* logs, there is an indication of a *memory leak* related to certain SSTables. * We can observe that the affected SSTable is being compacted around the same timeframe as the error occurrence. * Disabling *auto-compaction* resolves the issue even with BTI format enabled, which suggests a possible *race condition* during compaction. * Switching to the *Big format* does not reproduce the issue, further confirming that the problem is specific to the BTI format under compaction. - The problem is that {{PartionIndexEarly}} does not override {{sharedCopy()}} which makes the copy lose the in-memory tail. Patch coming soon. The lowest-impact workaround is to disable interval early open by setting {{sstable_preemptive_open_interval: 1PiB}} in {{{}cassandra.yaml{}}}: this disables early open on amount of data written, but still keeps it enabled for created files, which makes it almost as good as full early open with LCS and UCS. - Pull request: https://github.com/apache/cassandra/pull/4430 - CI runs: http://ci-cassandra.infra.datastax.com/job/cassandra-5.0/31/ http://ci-cassandra.infra.datastax.com/job/cassandra/48/ - +1 - Committed as [1db6f54925ddcb8a0ea41e6d711441898880c914|https://github.com/apache/cassandra/commit/1db6f54925ddcb8a0ea41e6d711441898880c914] (5.0) and [9cc3c085de548b3cd3537ca5c5b4f401348e6e65|https://github.com/apache/cassandra/commit/9cc3c085de548b3cd3537ca5c5b4f401348e6e65] (trunk). - Attached the two artefacts from each of the CI runs above. Downloading them first can be done with the command lines\u2026 {code} .build/run-ci -r \"https://github.com/blambov/cassandra.git\" -b \"CASSANDRA-20976\" -o 31 .build/run-ci -r \"https://github.com/blambov/cassandra.git\" -b \"CASSANDRA-20976-trunk\" -o 48 {code}", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20975", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20975"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-49: Hardware-accelerated compression\nDescription: CEP-49 proposes improving performance of compress/decompress operations with hardware-based accelerators. Based on feedback from the mailing list, creating this ticket to enable loading accelerated compression as a pluggable service provider, if jar file is available in the classpath.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20974", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20974"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Heap dump should not be generated on handled exceptions\nDescription: The {{dump_heap_on_uncaught_exception}} option was added on CASSANDRA-17795 to collect a heap dump when any uncaught exception is handled by the {{JVMStabilityInspector}}. However it's currently generated on any exception inspected by {{JVMStabilityInspector}} which I think is not the desired behavior.\nComments: - 5.0 CI: https://pre-ci.cassandra.apache.org/job/cassandra-5.0/27/ - +1 - Thanks for the review [~isaacreath] and [~smiklosovic] - addressed comment and created trunk patch which had some trivial conflicts trunk ci: [https://pre-ci.cassandra.apache.org/job/cassandra/121] will merge if it looks good. - Also for the record I removed in this patch the log statement {{\"Heap dump creation on uncaught exceptions is disabled.\"}} since it's not really needed, since this can be presumed when an uncaught exception is logged and no heap dump is created. - 5.0 once more https://pre-ci.cassandra.apache.org/job/cassandra-5.0/35/ I noticed that what Paulo run for 5.0 took just 10 minutes.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20973", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20973"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add Doug's new GPG key to KEYS\nDescription: The patch adds my gpg public key to the project's KEYS file found at https://dist.apache.org/repos/dist/release/cassandra/KEYS The GPG Public Key here has the fingerprint 2C94EBA59C0BA7E0EDAAE142BF79EF32B05FB5CA This is an ECC key which may not be useful (yet) for RPM/DEB signing, but is fine for things like the in-jvm-dtest-api project where we don't distribute a binary package like an RPM or DEB, just jars.\nComments: - bq. This is an ECC key which may not be useful (yet) for RPM/DEB signing We know the hard way that ECC keys will break Redhat installations, so do avoid building packages. Your key has been added in r79890. - I remember now that it is the presence of the key that causes a problem and I have reverted r79890. I think we have to unfortunately stick with non-ECC keys for a while.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20972", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20972"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: DISTINCT queries failing with range tombstones\nDescription: Queries like {{select distinct id from %s.tbl where token(id) > 0}} fail if there is an overlapping range tombstone: {code} ERROR 10:35:30 Exception in thread Thread[node1_ReadStage-1,5,SharedPool] java.lang.RuntimeException: java.lang.IllegalStateException: The UnfilteredRowIterator returned by the last call to next() was initialized: it must be closed before calling hasNext() or next() again. at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2612) at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829) Caused by: java.lang.IllegalStateException: The UnfilteredRowIterator returned by the last call to next() was initialized: it must be closed before calling hasNext() or next() again. at org.apache.cassandra.io.sstable.format.SSTableScanner$BaseKeyScanningIterator.computeNext(SSTableScanner.java:241) at org.apache.cassandra.io.sstable.format.SSTableScanner$BaseKeyScanningIterator.computeNext(SSTableScanner.java:228) at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) at org.apache.cassandra.io.sstable.format.SSTableScanner.hasNext(SSTableScanner.java:190) at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:90) at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:375) at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:187) at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:156) {code} Reason is that when {{LazilyInitializedUnfilteredRowIterator}} is closed the underlying iterator is nulled out, this allows the iterator to be re-initialized if anyone accesses a method that calls {{maybeInit}}, like {{isReverseOrder()}} which is called when adding an artificial range tombstone bound closer. This was caused by CASSANDRA-18398\nComments: - +1 - ci looks fine I think 5.0: https://pre-ci.cassandra.apache.org/job/cassandra-5.0/30/ trunk: https://pre-ci.cassandra.apache.org/job/cassandra/122/", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20971", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20971"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Reject untracked SSTables on startup and refresh\nDescription: Users expect that they can just throw an SSTable into the right data directory and start Cassandra: https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/operations/opsBackupSnapshotRestore.html But with mutation tracking, we\u2019d break invariants if we served a read from an SSTable that wasn\u2019t present in mutation tracking state management. There\u2019s a specific import path implemented in CASSANDRA-20383 that accounts for the new SSTables, and that\u2019s the path restores should use. But users that are expecting to copy a file into a data directory (or run nodetool refresh) should be notified that their actions will break invariants. Specifically, mutation tracking does a single data read and multiple summary reads, and expects that anything not in the summaries is reconciled. If we\u2019re adding an SSTable on a single instance and not tracking it in summaries, nothing would trigger the propagation of that SSTable to other replicas. In a world without mutation tracking, that SSTable would enter the unrepaired set, be included in reads at ALL, and eventually make it to other replicas with full or incremental repair.\nComments: No comments.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20970", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20970"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Accord witness awareness\nDescription: This assumes that MT already works with Accord (currently doesn't). Accord writes are probably largely unaffected. It might actually silently work today just because the mutation will automatically not be applied. Reads are affected because Accord picks the replica and it needs to pick a full replica. Likely the topology exposed to Accord needs to be witness aware so Accord can do the right thing.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20969", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20969"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness counter handling\nDescription: Counters in witness keyspaces need to be handled appropriately. [~aleksey] For IR based witnesses I think we didn't implement counter table support. One solution would be to just silently have counter tables not actually be replicated as witnesses and be all full replicas, but that might be infeasible because of assumptions made all over that witnesses are actually witnesses. Alternatively we could implement support for counters, but that requires having a design and migration strategy (which I currently don't have).\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20968", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20968"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Mutation tracking witness aware reconciliation\nDescription: Reconciliation propagates mutations to the mutation tracking log when it's missing and then applies the mutation. The existing witness code might automatically skip the applying the mutation so this all might \"just work\" but it's worth having jvm dtest that validates that it does. It partially depends on how MT applies mutations during reconciliation and whether it bypasses the code that currently implements the witness checks for skipping mutations.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20967", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20967"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: SSTable import support for witnesses\nDescription: Mutation tracking added support for SSTable import where the mutation tracking log has an entry for the import action for the sstables that are staged on disk. This allows read repair to be done for reads that include the imported sstables. On witnesses this record needs to be aware of witnesses and validate that the witness does not import out of range data. This validation should occur both for the initial sstable important and processes that are staging the sstables and then also when applying the record out of the MT log.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20966", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20966"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Rationalize and validate repair with witnesses\nDescription: IR based witnesses had an obvious integration that was done, but with MT based witnesses that is going away. Repair must continue to exist for MT based witnesses since MT still needs a repair mechanism for validation and to fix bugs that slip through the MT log and reconciliation process. What that will be is not finalized, but it needs to integrate with witnesses.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20965", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20965"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Validate nodetool cleanup with witnesses\nDescription: Nodetool cleanup with witnesses only cleans up repaired sstables. Mutation tracking \"repairs\" sstables now, but only things it managed and maybe there are some sharp edges to be found around topology changes and migration to mutation tracking. When migrating to mutation tracking there is likely to be some kind of repair run to ensure everything before mutation tracking started is successfully repaired. All of this needs to be rationalized together so that cleanup does the right things on witnesses and has tests that demonstrate that. There are tests of cleanup with witnesses, but they probably don't accurately reflect the new realities and things that will change.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20964", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20964"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Paxos simulation w/witnesses\nDescription: Running the Paxos simulation against the full set of cluster operations will give good signal as to how correct witnesses and mutation tracking are.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20963", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20963"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness bulk writer support\nDescription: When writing out sstables for a given node they should only be imported to a node if it is a full replica of the range. I believe that for the sstables that are imported we are targeting them being in the \"repaired\" state meaning all full copies have propagated so it isn't necessary for the mutation tracking log to contain mutations representing the contents of those tables. The mutation tracking log does contain an entry for the import event itself, but that is orthogonal to this ticket and [~aratnofsky] has implemented it.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20962", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20962"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness bulk reader support\nDescription: Bulk reader and possibly also sidecar need to be aware that they should only read from full replicas for a given range.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20961", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20961"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness range read support\nDescription: This is somewhat done, but there is a bug where the wrong {{Replica}} is used to determine whether the range being read from is witnesses on a particular node. There might be other remaining issues, but probably nothing that difficult.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20960", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20960"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add local script for checking project documentation.\nDescription: Add a simple script to evaluate if the anatora docs will build correctly. This is just to cut down on the overhead of doc contribution, not to replace the website build process.\nComments: - https://github.com/rustyrazorblade/cassandra/tree/CASSANDRA-20960 - Docs today are built (and tested) like: ``` cd cassandra-website /run.sh website build -g -u cassandra:https://github.com/thelastpickle/cassandra.git -b cassandra:mck/20678/5.0 ``` That approach means you have to have pushed your dev branch first, as the cassandra-website build process takes parameters for the fork+branch of in-tree docs you want to build but uses like inside a docker build, as you have also done. The ticket you need here is CASSANDRA-17260 that would make it possible to build just the in-tree docs. The idea is that in-tree needs its own site.yml and the how in-tree was built would not be different to how cassandra-website builds it. See (site.template.yaml)[https://github.com/apache/cassandra-website/blob/trunk/site-content/site.template.yaml] and (site_yaml_generator.py)[https://github.com/apache/cassandra-website/blob/trunk/site-content/bin/site_yaml_generator.py]. Currently the site.yml is generated on-the-fly by the cassandra-website build, after having copied in all the in-tree cassandra versions. See `generate_site_yaml ` at https://github.com/apache/cassandra-website/blob/trunk/site-content/docker-entrypoint.sh#L175 Implementing CASSANDRA-17260 would remove the need of this script. My only objection to temporarily having the script is if it still generates differences that don't get detected and published (because folk are only using the script to review and before merging). - Docs today are built (and tested) like: ``` cd cassandra-website /run.sh website build -g -u cassandra:https://github.com/thelastpickle/cassandra.git -b cassandra:mck/20678/5.0 ``` That approach means you have to have pushed your dev branch first, as the cassandra-website build process takes parameters for the fork+branch of in-tree docs you want to build but uses like inside a docker build, as you have also done. The ticket you need here is CASSANDRA-17260 that would make it possible to build just the in-tree docs. The idea is that in-tree needs its own site.yml Currently the site.yml is generated on-the-fly by the cassandra-website build, after having copied in all the in-tree cassandra versions. See `generate_site_yaml ` at https://github.com/apache/cassandra-website/blob/trunk/site-content/docker-entrypoint.sh#L175 fyi [~anthony] - I'm not exactly sure what the site.yaml is. If it's the Antora config, that's what I've included here. If you're merging some body of work in in a different ticket, that's cool, but I don't see any activity in the Jira you linked to. What do you want to do at this point? - a) double-check that the script is building the same way as cassandra-website docker-entrypoint.sh does (for each in-tree site) (otherwise it'll can be giving false qa) b) the site.yaml is the antora config. this info was fyi\u2013 how it \"should\" (and will eventually) be done, so you're aware it (and if you wanted to tackle that instead). - Thanks for the clarification. What about if we merge this as a stop gap between now and when CASSANDRA-17260 is complete? I added this because I was going to contribute some docs, and the current system was so overly complex (and undocumented in the C* repo) that I completely gave up. I found adding this script made it a lot easier for me to spot check my work. It built the docs in a couple seconds. I'll leave it to you to decide what should be done here, you've spent a bit more time on this and I'd rather not make things difficult if there's already a path to easy doc building. Feel free to merge this, close it, use some of it, or whatever you feel is best. I'm stretched pretty thin right now so unfortunately I'm a bit limited in what I have time to contribute. I trust your judgement and I don't want to waste your time either.", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20959", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20959"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45 Add support for counter tables/queries\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20958", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20958"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Add support for logged batches\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20957", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20957"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Add support for unlogged batches\nDescription: \nComments: - https://github.com/apache/cassandra/pull/4449", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20956", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20956"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Add support for table truncation\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20955", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20955"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Add support for dropping tables & keyspaces\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20954", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20954"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: (CEP-45) Fix SRP in range reads with PER PARTITION LIMIT set, simplify the read path\nDescription: There is a slight issue with SRP in the current mutation tracking range read implementation - if an augmenting mutation partially shadows a local LSM read, making a partition with PER PARTITION LIMIT set short, then SRP won't be invoked for that partition. While at it, we might simplify the read state machine class hierarchy and make obtaining follow-up reads for SRP a part of the simplified state machine rather than have a mixture of state machine transitions and callbacks/promises, giving it a more linear structure.\nComments: - I think {{MultiNodeTableWalkWithMutationTrackingTest}} reproduces this with {{builder.withSeed(3448512636059630802L).withExamples(1);}} {noformat} Caused by: java.lang.AssertionError: Unexpected results for query: SELECT * FROM ks1.tbl WHERE v0 >= [{0}, {0, 514}, {-1715, 3, 1215135}] PER PARTITION LIMIT 140 LIMIT 10 ALLOW FILTERING Caused by: java.lang.AssertionError: Missing rows: pk0 | ck0 | v0 | v1 '2025-01-28T20:50:22.330Z' | -60 | [{7948, 43651062, 65346426}, {0}, {7}] | {f0: {false: true, true: true}} '2025-01-28T20:50:22.330Z' | -34 | [{7948, 43651062, 65346426}, {0}, {7}] | {f0: {false: true, true: true}} '2025-01-28T20:50:22.330Z' | 21 | [{15}, {1176775}, {5416, 8072}] | null Expected: pk0 | ck0 | v0 | v1 '2022-12-30T09:50:17.410Z' | -112 | [{0, 51375431, 3145580081}, {-2630377, 24}, {-20506, -1, 2}] | null '1996-03-11T12:10:30.805Z' | -83 | [{111}, {-1588}, {-73, 0, 5105}] | {f0: {false: false}} '1996-03-11T12:10:30.805Z' | -77 | [{0, 87, 60579}, {319196}, {0, 13}] | {f0: {false: false, true: false}} '1996-03-11T12:10:30.805Z' | -28 | [{0}, {0, 514}, {-1715, 3, 1215135}] | null '2025-01-28T20:50:22.330Z' | -60 | [{7948, 43651062, 65346426}, {0}, {7}] | {f0: {false: true, true: true}} '2025-01-28T20:50:22.330Z' | -34 | [{7948, 43651062, 65346426}, {0}, {7}] | {f0: {false: true, true: true}} '2025-01-28T20:50:22.330Z' | 21 | [{15}, {1176775}, {5416, 8072}] | null {noformat}", "output": "Status: In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20953", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20953"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Witness SERIAL read/write path\nDescription: Needs to be updated to work with mutation tracking\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20952", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20952"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Apply cursor-based low allocation optimized compaction ideas to validation compaction\nDescription: A follow-up for CASSANDRA-20918 to apply the same ideas and re-use the building blocks for validation compaction as well to speedup repair. org.apache.cassandra.repair.ValidationManager#doValidation logic\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20951", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20951"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Documentation/Storage Engine - Markdown Render Misformat\nDescription: On the Architecture/Storage Engine in the documentation, the _ is rendered as italic between the \"Commit Log\" reference and \"Memtable\" reference. Documentation: https://cassandra.apache.org/doc/latest/cassandra/architecture/storage-engine.html !image-2025-10-03-15-00-18-013.png! The solution is to simply escape the underscore with a backslash.\nComments: - See PR: [https://github.com/apache/cassandra/pull/4409] - Render included.", "output": "Status: Patch Available\nPriority: Low"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20950", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20950"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: TCM - can't drop a keyspace with a table and recreate same structure\nDescription: Looks like if you create a keyspace and a table, then drop the keyspace, trying to re-create that same keyspace and table will completely lock up TCM. Steps to repro: {noformat} cqlsh> create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; cqlsh> create table test.t1 (id int primary key); cqlsh> drop KEYSPACE test; cqlsh> create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; cqlsh> create table test.t1 (id int primary key);{noformat} Error: {noformat} INFO [GlobalLogFollower] 2025-10-03T00:09:56,074 ColumnFamilyStore.java:513 - Initializing test.t1 ERROR [GlobalLogFollower] 2025-10-03T00:09:56,076 LocalLog.java:546 - Could not process the entry java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db.compression:type=CompressionDictionaryManager,keyspace=test,table=t1 at org.apache.cassandra.utils.MBeanWrapper$OnException.lambda$static$0(MBeanWrapper.java:365) at org.apache.cassandra.utils.MBeanWrapper$PlatformMBeanWrapper.registerMBean(MBeanWrapper.java:184) at org.apache.cassandra.utils.MBeanWrapper.registerMBean(MBeanWrapper.java:97) at org.apache.cassandra.utils.MBeanWrapper.registerMBean(MBeanWrapper.java:101) at org.apache.cassandra.db.compression.CompressionDictionaryManager.registerMBean(CompressionDictionaryManager.java:82) at org.apache.cassandra.db.compression.CompressionDictionaryManager.<init>(CompressionDictionaryManager.java:74) at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:582) at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:801) at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:778) at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:768) at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:380) at org.apache.cassandra.schema.DistributedSchema.createTable(DistributedSchema.java:396) at org.apache.cassandra.schema.DistributedSchema.lambda$initializeKeyspaceInstances$9(DistributedSchema.java:245) at java.base/java.lang.Iterable.forEach(Iterable.java:75) at org.apache.cassandra.schema.DistributedSchema.lambda$initializeKeyspaceInstances$13(DistributedSchema.java:245) at com.google.common.collect.ImmutableList.forEach(ImmutableList.java:422) at org.apache.cassandra.schema.DistributedSchema.initializeKeyspaceInstances(DistributedSchema.java:231) at org.apache.cassandra.tcm.listeners.SchemaListener.notifyInternal(SchemaListener.java:50) at org.apache.cassandra.tcm.listeners.SchemaListener.notifyPreCommit(SchemaListener.java:43) at org.apache.cassandra.tcm.log.LocalLog.notifyPreCommit(LocalLog.java:620) at org.apache.cassandra.tcm.log.LocalLog.processPendingInternal(LocalLog.java:521) at org.apache.cassandra.tcm.log.LocalLog$Async$AsyncRunnable.run(LocalLog.java:813) at org.apache.cassandra.concurrent.InfiniteLoopExecutor.loop(InfiniteLoopExecutor.java:119) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:840) Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db.compression:type=CompressionDictionaryManager,keyspace=test,table=t1 at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523) at org.apache.cassandra.utils.MBeanWrapper$PlatformMBeanWrapper.registerMBean(MBeanWrapper.java:180) ... 23 common frames omitted{noformat} This is fun too: {noformat} cqlsh> drop KEYSPACE test; cqlsh> create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; cqlsh> create table test.t2 (id int primary key); cqlsh> select * from test.t2; InvalidRequest: Error from server: code=2200 [Invalid query] message=\"table t2 does not exist\" {noformat} Exception thrown in C*: {noformat} INFO [GlobalLogFollower] 2025-10-03T00:14:24,536 ColumnFamilyStore.java:513 - Initializing test.t2 ERROR [GlobalLogFollower] 2025-10-03T00:14:24,538 LocalLog.java:546 - Could not process the entry java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db.compression:type=CompressionDictionaryManager,keyspace=test,table=t2 at org.apache.cassandra.utils.MBeanWrapper$OnException.lambda$static$0(MBeanWrapper.java:365) at org.apache.cassandra.utils.MBeanWrapper$PlatformMBeanWrapper.registerMBean(MBeanWrapper.java:184) at org.apache.cassandra.utils.MBeanWrapper.registerMBean(MBeanWrapper.java:97) at org.apache.cassandra.utils.MBeanWrapper.registerMBean(MBeanWrapper.java:101) at org.apache.cassandra.db.compression.CompressionDictionaryManager.registerMBean(CompressionDictionaryManager.java:82) at org.apache.cassandra.db.compression.CompressionDictionaryManager.<init>(CompressionDictionaryManager.java:74) at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:582) at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:801) at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:778) at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:768) at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:380) at org.apache.cassandra.schema.DistributedSchema.createTable(DistributedSchema.java:396) at org.apache.cassandra.schema.DistributedSchema.lambda$initializeKeyspaceInstances$9(DistributedSchema.java:245) at java.base/java.lang.Iterable.forEach(Iterable.java:75) at org.apache.cassandra.schema.DistributedSchema.lambda$initializeKeyspaceInstances$13(DistributedSchema.java:245) at com.google.common.collect.ImmutableList.forEach(ImmutableList.java:422) at org.apache.cassandra.schema.DistributedSchema.initializeKeyspaceInstances(DistributedSchema.java:231) at org.apache.cassandra.tcm.listeners.SchemaListener.notifyInternal(SchemaListener.java:50) at org.apache.cassandra.tcm.listeners.SchemaListener.notifyPreCommit(SchemaListener.java:43) at org.apache.cassandra.tcm.log.LocalLog.notifyPreCommit(LocalLog.java:620) at org.apache.cassandra.tcm.log.LocalLog.processPendingInternal(LocalLog.java:521) at org.apache.cassandra.tcm.log.LocalLog$Async$AsyncRunnable.run(LocalLog.java:813) at org.apache.cassandra.concurrent.InfiniteLoopExecutor.loop(InfiniteLoopExecutor.java:119) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:840) Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db.compression:type=CompressionDictionaryManager,keyspace=test,table=t2 at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523) at org.apache.cassandra.utils.MBeanWrapper$PlatformMBeanWrapper.registerMBean(MBeanWrapper.java:180){noformat} Trying to create any other tables ends up rethrowing the above exception for the table name that originally triggered the problem.\nComments: - Exactly which version are you seeing this with? I'm completely unable to reproduce it with trunk at {{61959e2}}, either in a single or multi-node cluster. From your stacktrace, the issue looks to be the CFS mbean not getting unregistered when the keyspace is dropped. Checking with jconsole locally and I can't find any evidence of that either. - I think the cqlsh tests are passing, and they do a drop and recreate: https://github.com/apache/cassandra-dtest/blob/trunk/cqlsh_tests/test_cqlsh.py#L916 - Looks like it is a bug in the patch (CASSANDRA-17021) that is under review. {code:java} Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db.compression:type=CompressionDictionaryManager,keyspace=test,table=t2 {code} The reason is that the CompressionDictionaryManager mbean is not unregistered when dropping keyspace. I will add the fix in the patch. We can close this JIRA.", "output": "Status: Resolved\nPriority: Urgent"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20949", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20949"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optionally verify checksums for SAI on-disk components in nodetool verify\nDescription: The *nodetool verify* command has never checked secondary indexes, but with SAI on-disk items being legitimate SSTable components, it might be useful. One place this might come up is when we have some kind of error in streaming. We verify the checksums of SAI components on the receiving end, but not on the sender, and it might be nice to know if there\u2019s a particular corrupted SSTable-attached index there, so we can remove it, bounce, and move along (instead of rebuilding the entire index on the sending node).\nComments: No comments.", "output": "Status: Review In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20948", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20948"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45 - Mutation Tracking: Review metrics\nDescription: Some of the metrics we have should be reviewed so they're providing the most useful information. Current metrics to consider are below, add to the description of the JIRA as others are found ReadRepairMetrics.trackedReconcile.mark() is called for every single read. This is technically correct, but is not be a useful metric. Let's consider marking it from ReadReconciliations.Coordinator#{acceptLocalSummary, acceptRemoteSummary} so we're only counting reconciliations that happen when a replica needs to be sent mutations during a read.\nComments: No comments.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20947", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20947"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45 - Mutation Tracking: De-duplicate outgoing mutations\nDescription: Outgoing mutation deduplication work was started as part of CASSANDRA-20729 (CEP-45: Implement unified reconcile logic), but wasn't completed. We should finish this so we're not sending the same mutation multiple times.\nComments: No comments.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20946", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20946"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Add replication_type and witness replicas to CassandraGenerators\nDescription: We're not currently testing mutation tracking or witness replicas in our property based testing. We should be testing both in property based tests where it makes sense.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20945", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20945"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add a message to the top of NEWS.txt about TCM and Upgrades.\nDescription: Add the following note to the top of NEWS.txt to help ensure people don't get surprised by the new steps needed when upgrading to a version that includes TCM. {noformat} **CRITICAL: UPGRADING** ---------------------- In order to support the new Cluster Metadata Service, added to Apache Cassandra in \"CEP-21: Transactional Cluster Metadata\", upgrading to this version of Cassandra involves operational steps above and beyond most previous upgrades. See the instructions below in the \"Upgrading\" section for more details on how to proceed.{noformat}\nComments: - [https://github.com/apache/cassandra/pull/4407] - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20944", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20944"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Disable mutation tracking by default in CassandraGenerators\nDescription: We currently enable mutation tracking explicitly where it makes sense for the AST tests (like in {{MultiNodeTableWalkMutationTrackingTest}}), but it doesn\u2019t make sense for it to even be an option for single node tests or other tests where legacy repair mechanisms are active.\nComments: - Have a +1 from [~bdeggleston] in the PR. CI is still a bit of a mess, but this patch hasn't caused any regressions for the dependencies of {{CassandraGenerators}}.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20943", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20943"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introducing comments and security labels for schema elements\nDescription: This proposal introduces the ability to add comments and security labels to Cassandra schema elements such as keyspaces, tables, columns, and UDTs. Users/Operators can embed human-readable explanations and business context directly into the schema, transforming Cassandra into a more self-documenting system. Comments improve comprehension, onboarding, troubleshooting, and long-term maintainability, while security labels provide a mechanism to mark elements with descriptors such as \"PII\" for sensitive data. Although security labels are purely descriptive in this proposal and do not alter Cassandra's authorization model, they establish a foundation for future integrations where external providers could enforce policies based on assigned labels, thereby enhancing both documentation and governance capabilities. https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-52%3A+Schema+Annotations+for+ApacheCassandra\nComments: - Just reviewed the functionality, and everything looks great. +1 and a great feature. Thanks for your work on this! - +1! Thank you for addressing my comments. Great work and test coverage. (Please post CI result) - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20942", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20942"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enhance traincompressiondictionary command with sampling and scope Options\nDescription: Extend the traincompressiondictionary nodetool command to include: * Support for multiple sampling strategies. * '--global' and '--local' options to define dictionary availability scope: ** Global (default): Store in system_distributed.compression_dictionaries and make available cluster-wide. ** Local: Restrict dictionary availability to the current node, when dictionary from local data is desired.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20941", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20941"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add Nodetool command to import / export/ list compression dictionary\nDescription: Introduce a new nodetool command to import a ZSTD compression dictionary. - Imported dictionary is automatically promoted as the current dictionary without going through evaluation. - Intended as an administrative tool; usage should be documented with warnings about its impact.\nComments: - +1 on the patch! Thank you for addressing my comments promptly. Please share the CI result. - This test started to fail (but not locally). I see it in CircleCI as well as in pre-ci. {code} org.awaitility.core.ConditionTimeoutException: Assertion condition defined as a org.apache.cassandra.Util Expected: <true> but: was <false> within 10 seconds. at org.awaitility.core.ConditionAwaiter.await(ConditionAwaiter.java:165) at org.awaitility.core.AssertionCondition.await(AssertionCondition.java:119) at org.awaitility.core.AssertionCondition.await(AssertionCondition.java:31) at org.awaitility.core.ConditionFactory.until(ConditionFactory.java:895) at org.awaitility.core.ConditionFactory.untilAsserted(ConditionFactory.java:679) at org.apache.cassandra.Util.spinUntilTrue(Util.java:793) at org.apache.cassandra.Util.spinUntilTrue(Util.java:778) at org.apache.cassandra.db.compression.CompressionDictionarySchedulerTest.testScheduleSSTableBasedTrainingWithSSTables(CompressionDictionarySchedulerTest.java:102) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Caused by: java.lang.AssertionError: Expected: <true> but: was <false> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.cassandra.Util.lambda$spinUntilTrue$3(Util.java:793) at org.awaitility.core.AssertionCondition.lambda$new$0(AssertionCondition.java:53) at org.awaitility.core.ConditionAwaiter$ConditionPoller.call(ConditionAwaiter.java:222) at org.awaitility.core.ConditionAwaiter$ConditionPoller.call(ConditionAwaiter.java:209) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:840) {code} However I do not think this patch is the cause of that. It also fails on other patches too. I do not know what's up because the last time we touched something related to compression was CASSANDRA-20938 (CQLSSTableWriter can produce SSTables backed by dict) and there it did not fail: https://pre-ci.cassandra.apache.org/job/cassandra/147/#showFailuresLink - bunch of failures but unrelated ones ... basically Accord and fuzz. https://app.circleci.com/pipelines/github/instaclustr/cassandra/6114/workflows/544be31d-ccf2-4efb-bb75-b19caf8ecb85", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20940", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20940"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Extend Nodetool tablestats for dictionary memory usage\nDescription: Add reporting in nodetool tablestats to display memory usage associated with cached ZSTD dictionaries.\nComments: - +1 Thank you for the patch! - https://pre-ci.cassandra.apache.org/job/cassandra/173/#showFailuresLink I have fixed {{ColumnFamilyMetricTest.testStartupRaceConditionOnMetricListeners}} locally and test passed, not going to run all ci just because of that ...", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20939", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20939"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add metrics for ZSTD dictionary compression\nDescription: Expose the following metrics for observability of ZSTD dictionary compression: - Training frequency - Training duration - Dictionary size - Unique active dictionaries count - Compression ratio\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20938", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20938"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support external ZSTD dictionary in CQLSSTableWriter\nDescription: Extend CQLSSTableWriter to accept an external ZSTD compression dictionary for writing SSTables. - Add a new option to pass the dictionary. - Dictionary is only applied if the table schema indicates ZSTD compression with dictionary enabled. - Document best practices: external tooling invoking CQLSSTableWriter should fetch the current dictionary from Cassandra to avoid excessive dictionary proliferation.\nComments: - Thank you for the patch! I am +1 on the patch. Could you please share the CI result? - https://pre-ci.cassandra.apache.org/job/cassandra/147/#showFailuresLink nothing out of \"ordinary\". Going to ship it.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20937", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20937"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Coordinate ZSTD dictionary auto-training on a single node\nDescription: Ensure that ZSTD dictionary auto-training is performed by only one node in the cluster at a time. - Select a single node to handle the end-to-end workflow: sampling data, training the dictionary, evaluating performance, and promoting the dictionary if applicable. - Prevent multiple nodes from duplicating effort or promoting conflicting dictionaries.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20936", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20936"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add dictionary evaluation and auto-promotion mechanism\nDescription: Develop a component to evaluate compression performance between dictionaries and automatically promote the better-performing one. - Compare compression ratios and effectiveness between new and current dictionaries. - If the new dictionary demonstrates better performance, promote it automatically. - Provide hooks for metrics to track evaluation outcomes and promotions.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20935", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20935"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add sampling strategies and reservoir for ZSTD dictionary training\nDescription: Introduce support for sampling strategies in ZSTD dictionary auto-training and persist sampled data in a reservoir for reuse. - Support: random sampling and exponential decay reservoir sampling. - Maintain a reservoir of sampled data for consistent and representative training. - Ensure integration between sampling and reservoir storage is seamless.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20934", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20934"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix several witness tests\nDescription: There were a few new failing tests merging to {{cep-45-mutation-tracking}}\nComments: - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20933", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20933"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky test - transport.AuthMessageSizeLimitTest sendTooBigAuthMultiFrameMessage\nDescription: AuthMessageSizeLimitTest fails from time to time. https://ci-cassandra.apache.org/job/Cassandra-trunk/2287/testReport/junit/org.apache.cassandra.transport/AuthMessageSizeLimitTest/Tests___test_jdk17_3_20___sendTooBigAuthMultiFrameMessage__jdk17_x86_64/\nComments: - The failure can be reproduced more reliably by adding a delay into org.apache.cassandra.transport.SimpleClient.SimpleFlusher#writeLargeMessage logic between sending of subsequent frames of a large auth message. so we have one of two cases: * (normal case) client is fast enough to send both frames into an open TCP socket before a server logic starts to process - in this case we see the expected Protocol error on a client side * (flaky case) the following exception is thrown: {code} java.lang.RuntimeException: timeout at org.apache.cassandra.transport.SimpleClient.execute(SimpleClient.java:298) at org.apache.cassandra.transport.SimpleClient.execute(SimpleClient.java:287) at org.apache.cassandra.transport.AuthMessageSizeLimitTest.lambda$sendTooBigAuthMultiFrameMessage$1(AuthMessageSizeLimitTest.java:82) at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63) at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892) at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366) at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210) at org.apache.cassandra.transport.AuthMessageSizeLimitTest.lambda$sendTooBigAuthMultiFrameMessage$3(AuthMessageSizeLimitTest.java:82) at org.apache.cassandra.transport.NativeProtocolLimitsTestBase.doTest(NativeProtocolLimitsTestBase.java:132) {code}", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20932", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20932"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Do not emit warning on duplicate keys in snakeyaml to preserve behavior pre-CASSANDRA-20928\nDescription: upgrade of snakeyaml 2.1 to 2.4 under CASSANDRA-20928 started to emit warnings into the logs when it detects that respective YAML file contains duplicate keys. This is happening while we run tests, as we are concatenating various yaml fragments into one yaml we use for tests (cassandra.yaml, cdc.yaml and similar). This will emit warnings to logs and tests will fail because they expect empty error output. The solution is to not emit warnings on duplicate keys.\nComments: - +1 - one can see it is fixed from here (1) (1) https://app.circleci.com/pipelines/github/instaclustr/cassandra/6058/workflows/f6f7a962-9973-4a77-9348-4e8f4364da64", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20931", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20931"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Lightweight `ant check` github action\nDescription: Proposing the following patch: https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/gha-lint It's a one-liner `check-code.sh` and is light and quick to run at ~6 minutes. We are supposed to `ant check` (what check-code.sh does) before merging but it's an easy thing to forget to do. A lightweight gh action can help prevent many breaking merges since many of us push our dev branches to gh. Example result run here: https://github.com/thelastpickle/cassandra/actions/runs/17880689988/job/50847932781 dev@ ML thread: https://lists.apache.org/thread/pdsll118zlf31r0y23t0gr7k3y40kx3f\nComments: - +1 - CI: http://ci-cassandra.infra.datastax.com/job/cassandra/43/ - [^ci_summary_thelastpickle_mck-gha-lint_43.html] and [^results_details_thelastpickle_mck-gha-lint_43.tar.xz] - Committed as https://github.com/apache/cassandra/commit/8aad9d6c05265c08126b43ee7196f96f84df88e3", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20930", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20930"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-46: Initial non-serial single partition read/write path\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20929", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20929"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Protocol v5 spec is wrong - wrt compressed frame format\nDescription: From [https://github.com/apache/cassandra/blob/e2d2e3b31ba47dfc9bbdbfdbcace3932fd3d32e7/doc/native_protocol_v5.spec#L137-L140] : The variant with LZ4 compression uses an 8 byte header. The first 4 bytes contain an unsigned integer (little endian) containing the compressed and uncompressed lengths of the payload and the self contained flag. But [https://github.com/apache/cassandra/blob/e2d2e3b31ba47dfc9bbdbfdbcace3932fd3d32e7/doc/native_protocol_v5.spec#L144-L146] says: 1. Compressed length (17 bits) 2. Uncompressed length (17 bits) 3. isSelfContained flag (1 bit) 17 + 17 + 1 are already 35 bits - which is more than 'first 4 bytes'\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20928", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20928"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update snakeyaml to 2.4\nDescription: We can bump to 2.4. Jackson dataformat-yaml also depends on snakeyaml 2.4 so exclusion is not necessary. OWASP checks 12.1.6 also depends on snakeyaml 2.4. We are hence aligned on all fronts.\nComments: - https://pre-ci.cassandra.apache.org/job/cassandra/105/#showFailuresLink - +1 - it looks like we've got some new test test failures after the version bump.. ([https://ci-cassandra.apache.org/job/Cassandra-trunk/2288/]) [https://ci-cassandra.apache.org/job/Cassandra-trunk/2288/testReport/junit/org.apache.cas[]\u2026]t_cdc_jdk11_16_20___testWriteAndCompact_cdc_jdk11_x86_64/ with some strange duplicate keys errors {code:java} Failed to clean stderr completely. Raw (length=432): Sep 27, 2025 2:46:03 AM org.yaml.snakeyaml.internal.Logger warn WARNING: duplicate keys found : cdc_enabled Sep 27, 2025 2:46:03 AM org.yaml.snakeyaml.internal.Logger warn WARNING: duplicate keys found : cdc_enabled Sep 27, 2025 2:46:03 AM org.yaml.snakeyaml.internal.Logger warn WARNING: duplicate keys found : cdc_enabled Sep 27, 2025 2:46:03 AM org.yaml.snakeyaml.internal.Logger warn WARNING: duplicate keys found : cdc_enabled {code} - why did not (1) catch this? (1) https://pre-ci.cassandra.apache.org/job/cassandra/105/#showFailuresLink - We will resolve this under CASSANDRA-20932", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20927", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20927"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix ShardManager#coveringRange to include provide partition position\nDescription: Range is left exclusive. When constructing Range, it should decrement left position instead of incrementing right position. Also ShardManager#rangeSpanned for (min, min] should be 1 instead of 1 token span. Impact is trivial: it can cause is a range to be measured to span 0 range instead of 2^-64 (if it only contains one partition that falls on a local ownership boundary), but this is then anyway corrected to give it a sensible density. The min-min case only relates to tests. downstream patch: https://github.com/datastax/cassandra/commit/dd5da490b5705a61b49bc6bfadf6a47a973c9804\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20926", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20926"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Add config to disable mutation tracking\nDescription: Mutation tracking needs to be disabled by default. The config option exists in the feature branch but doesn't actually do anything. This will actually disable mutation tracking if it's not explicitly enabled\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20925", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20925"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update netty to 4.1.125\nDescription: \nComments: - [https://pre-ci.cassandra.apache.org/job/cassandra/102/#showFailuresLink] [~brandon.williams] my plan is to just do this in trunk and then I will be upgrading dependency check jar and update / remove deprecations for all branches in CASSANDRA-20924 - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20924", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20924"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update dependency-check library to version 12.1.6\nDescription: The current version (12.1.0) fail on unauthorized exceptions to OSS index which 12.1.6 skips.\nComments: - After bumping I see this on trunk at least: trunk - we use netty 4.1.119 and cassandra-driver-core-3.12.1-shaded.jar seems valid, high [4.1.91, 4.1.118) [CVE-2025-24970|https://nvd.nist.gov/vuln/detail/CVE-2025-24970] - cassandra-driver-core-3.12.1-shaded.jar http related, not valid [x, 4.1.108) [CVE-2024-29025|https://nvd.nist.gov/vuln/detail/CVE-2024-29025] - cassandra-driver-core-3.12.1-shaded.jar windows, not applicable, [x, 4.1.115) [CVE-2024-47535|https://nvd.nist.gov/vuln/detail/CVE-2024-47535] - cassandra-driver-core-3.12.1-shaded.jar http, not applicable [CVE-2025-55163|https://nvd.nist.gov/vuln/detail/CVE-2025-55163] - netty-transport-4.1.119.Final.jar http, not applicable [CVE-2025-58056|https://nvd.nist.gov/vuln/detail/CVE-2025-58056] - netty-transport-4.1.119.Final.jar I am not sure we use this, says about Brotli, but patch also touches more (1) [CVE-2025-58057|https://nvd.nist.gov/vuln/detail/CVE-2025-58057] - netty-transport-4.1.119.Final.jar - this is fixed in 4.1.125 (1) [https://github.com/netty/netty/commit/9d804c54ce962408ae6418255a83a13924f7145d#diff-7f4d98f6301cd7edb79eacae3cbf3364253e4d4e739aec49828290f613459ea8] I see all of them in 4.0 plus CVE-2023-44487 (http) and CVE-2025-25193 (windows). - btw the only place we use cassandra-driver-core-3.12.1-shaded is in stress tool so all these CVEs in there are in practice quite harmless. Until stress has its own class path, the scanner will report that. - We can update to netty 4.1.125 to get rid of CVE-2025-58057, tracking it here (1). Then everything else can be suppressed as it is used in shaded driver which is used just in stress tool. (1) https://issues.apache.org/jira/browse/CASSANDRA-20925 - trunk [https://github.com/apache/cassandra/compare/trunk...smiklosovic:cassandra:CASSANDRA-20924-trunk?expand=1] 5.0 [https://github.com/apache/cassandra/compare/trunk...smiklosovic:cassandra:CASSANDRA-20924-5.0?expand=1] 4.1 [https://github.com/apache/cassandra/compare/trunk...smiklosovic:cassandra:CASSANDRA-20924-4.1?expand=1] 4.0 [https://github.com/apache/cassandra/compare/trunk...smiklosovic:cassandra:CASSANDRA-20924-4.0?expand=1] - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20923", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20923"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Segregate SAI's query metrics per query type\nDescription: SAI's tracks query metrics providing info about latency, partition reads, filtered rows, etc. These metrics are for all the SAI queries for a certain table. When we see problems in queries, for example a high latency, questions that can usually come next are what query is producing this high latency? Are there vector queries? Are queries for a single partition? In that case, what is the latency of those queries? How many rows are they selecting? To answer those questions, I think it can be useful to track metrics for specific types of query. Tracking is done in a {{TableQueryMetrics}} associated to the table. This PR only adds a few type of queries: filtering, top-k, single-partition and range queries. If we need new types in the future, this patch should make it easy to add separate metrics for them, possibly with an one-liner. The general query metrics remain untouched, this PR only adds new ones. More background info from downstream work in https://github.com/datastax/cassandra/pull/1969 and https://github.com/datastax/cassandra/pull/2015\nComments: - first downstream patch (for #1969) is https://github.com/datastax/cassandra/commit/a4e8dda0f53603ae67e0fab413f1271b96aa5fcd - second downstream patch (for #2015) is https://github.com/datastax/cassandra/commit/403ed3331e", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20922", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20922"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Explore Accord behaviour under synthetic geodistributed workloads\nDescription: Working with [~otrack] to explore the behaviour of Accord under synthetic workloads using docker and latency injection using tc/netem, with ycsb workloads. https://github.com/belliottsmith/cassandra-evaluation/tree/belliottsmith\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20921", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20921"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Introduce hardRemoved concept for marking nodes permanently offline\nDescription: If Accord experiences unrecoverable loss of a quorum, it will refuse to make progress. This patch introduces a mechanism for marking nodes as permanently offline to permit these minority quorums to make progress without the lost nodes. This may result in undetectable consistency violations, since we have exceeded the maximum failure tolerance of the quorum. Also fix: - Paxos should not update minHlc if AccordService is not setup - Remove `EndpointMapper` methods that require non-null, to ensure call-sites handle null case explicitly\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20920", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20920"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Journal Replay\nDescription: This patch introduces SegmentStateTracker, which tracks Mutation Journal allocations and listens to Memtable->SSTable flushes. * SegmentStateTracker logic is largely adapted from CommitLog (i.e. tracking dirty allocation upper bound, and clean/flushed min/max bounds, and checking their intersection to see if entire segment is flushed. * MutationJournal now tracks new allocations per segment (and, within a segment, per table), and listens to Memtable flushes, marking CommitLogPosition bounds reported by the flush as clean * Memtables now distinguish between commit log positions they track vs journal positions. It might be a good idea to mark CommitLogPosition with a corresponding flag to avoid accidentally passing a wrong position {{MutationJournal#replay}} is added, and serves a purpose similar to CommitLog replay, albeit lacks some of its functionality, such as replay filter, and DROP TABLE support for now (missing pieces documented inline). Only segments holding allocations that were not memtable->sstable flushed are considered for replay. Testing: * A fuzz test for SegmentStateTracker * A test that assumes some memtable behavior to exercise MutationJournal integration * Full integration / bounce test validating that data is being recovered by replay Important: a follow-up patch (in work by [~aleksey]) adds compaction and truncation of segments. Added tests will need to be expanded to support this. Remark: [trunk/accord commit I reviewed today|https://github.com/apache/cassandra/pull/4384/files#diff-a6dbe3c4eab186aaa33f04d7826c2239bfe99b3b392b026d18b8f31584652303R939] has changes StaticSegmentIterator to just SegmentIterator, which largely aligns with this patch, but might cause a rebase.\nComments: No comments.", "output": "Status: Resolved\nPriority: High"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20919", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20919"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Mutation Tracking: Journal Replay\nDescription: This patch introduces SegmentStateTracker, which tracks Mutation Journal allocations and listens to Memtable->SSTable flushes. SegmentStateTracker logic is largely adapted from CommitLog (i.e. tracking dirty allocation upper bound, and clean/flushed min/max bounds, and checking their intersection to see if entire segment is flushed. MutationJournal now tracks new allocations per segment (and, within a segment, per table), and listens to Memtable flushes, marking CommitLogPosition bounds reported by the flush as clean. MutationJournal#replay is added, and serves a purpose similar to CommitLog replay, albeit lacks some of its functionality, such as replay filter, for now (missing pieces documented inline). Only segments holding allocations that were not memtable->sstable flushed are considered for replay.\nComments: No comments.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20918", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20918"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add cursor-based low allocation optimized compaction implementation\nDescription: Compaction does a ton of allocation and burns a lot of CPU in the process; we can move away from allocation with some fairly simple and straightforward reusable objects and infrastructure that make use of that, reducing allocation and thus CPU usage during compaction. Heap allocation on all test-cases holds steady at 20MB while regular compaction grows up past 5+GB. This patch introduces a collection of reusable objects: * ReusableLivenessInfo * ReusableDecoratedKey * ReusableLongToken And new compaction structures that make use of those objects: * CompactionCursor * CursorCompactionPipeline * SSTableCursorReader * SSTableCursorWriter There's quite a bit of test code added, benchmarks, etc on the linked branch. ~13k added, 405 lines deleted ~8.3k lines delta are non-test code ~5k lines delta are test code Attaching a screenshot of the \"messiest\" benchmark case with mixed size rows and full merge; across various data and compaction mixes the highlight is that compaction as implemented here is roughly 3-5x faster in most scenarios and uses 20mb on heap vs. multiple GB.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20917", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20917"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Throw RTE instead of FSError when RTE is thrown from FileUtis.write in TOCComponent.\nDescription: This was overlooked in CASSANDRA-20494.\nComments: - +1 - https://pre-ci.cassandra.apache.org/job/cassandra-5.0/24/", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20916", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20916"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-51: Support Include Semantics for cassandra.yaml\nDescription: https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-51%3A+Support+Include+Semantics+for+cassandra.yaml\nComments: No comments.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20915", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20915"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fixed incorrect error message constant for keyspace name length validation\nDescription: The validateKeyspaceName method incorrectly referenced TABLE_NAME_LENGTH (222) instead of NAME_LENGTH (48) in the error message when keyspace names exceeded the maximum allowed length. For example, {code:java} org.apache.cassandra.exceptions.ConfigurationException: Keyspace name must not be more than 222 characters long (got 51 characters for \"compression_dict_manager_test_keyspace_without_dict\") at org.apache.cassandra.schema.KeyspaceMetadata.validateKeyspaceName(KeyspaceMetadata.java:80) at org.apache.cassandra.schema.KeyspaceMetadata.validate(KeyspaceMetadata.java:403) at org.apache.cassandra.schema.SchemaTestUtil.announceNewKeyspace(SchemaTestUtil.java:36) at org.apache.cassandra.SchemaLoader.createKeyspace(SchemaLoader.java:252) {code}\nComments: - PR: https://github.com/apache/cassandra/pull/4389 - +1 - https://pre-ci.cassandra.apache.org/job/cassandra/101/#showFailuresLink This is such a cosmetic feature that I do not anticipate it would fail anything else on lower branches so manual testing on 4.0, 4.1, 5.0 is just OK. - \"a\".repeat() is available from Java 11. We need to patch this in 4.0+ which needs to compile with Java 8. I have rewritten this construct with a loop and StringBuilder appending \"a\" enough times to violate the condition and trigger exception. - just a note for future: org.apache.commons.lang3.StringUtils has such API as well", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20914", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20914"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Document allowed length for table name\nDescription: CASSANDRA-20389 implemented validation of table name length. Thus it's good to be specific in the documentation.\nComments: - PR against 4.0: https://github.com/apache/cassandra/pull/4448 - I plan to create PRs for other branches (4.1, 5.0, trunk) after the first round of review to reduce overhead of cherry picking the patch with changes. - I created PRs for all affected branches and started CI builds. Note there is a difference between PRs for 4.0/4.1 and for 5.0/trunk as the assertion method is changed accordantly with the local use. 4.0: PR: https://github.com/apache/cassandra/pull/4448 CircleCI pre-commit j8: https://app.circleci.com/pipelines/github/k-rus/cassandra/151/workflows/40d4a69e-fed0-4e5c-b69d-3c0ccc0f261f CircleCI pre-commit j11: https://app.circleci.com/pipelines/github/k-rus/cassandra/151/workflows/16578e02-b86c-44c6-a0d3-623b20779f60 4.1: PR: https://github.com/apache/cassandra/pull/4450 CircleCI pre-commit j8: https://app.circleci.com/pipelines/github/k-rus/cassandra/152/workflows/de0086b9-9265-476a-b058-d46ef81a7ac1 CircleCI pre-commit j11: https://app.circleci.com/pipelines/github/k-rus/cassandra/152/workflows/597ef400-9287-4606-b4cc-1c71e9ca5822 5.1: PR: https://github.com/apache/cassandra/pull/4451 Pre-commit build: http://ci-cassandra.infra.datastax.com/job/cassandra-5.0/33/ trunk: PR: https://github.com/apache/cassandra/pull/4452 Pre-commit build: http://ci-cassandra.infra.datastax.com/job/cassandra/50/ - Compilations in all CI builds have succeeded. I've checked and deem that there are no related test failures in any of CI builds. - +1 - +1 {code:java} SchemaConstants.java:{code} {code:java} TABLE_NAME_LENGTH = FILENAME_LENGTH (255) - TABLE_UUID_AS_HEX_LENGTH (32) - TABLE_DIRECTORY_NAME_SEPARATOR.length() (1);{code} - I will merge tomorrow / in Friday. - [~smiklosovic] thank you for taking the merge task for this patch. I rebased the branches on their latest states (4.0 didn't require any rebase) and there were no conflicts. Let me know if I need to do anything else.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20913", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20913"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add DDL Guardrail enabling administrators to disallow creation/modification of key spaces with durable_writes = false\nDescription: This change implements a guardrail to disallow creation or alteration of key spaces to set the property durable_writes = false; durable_writes = true or unset should be the only allowable options for this property when enabled.\nComments: - [~anaik] is this something you plan to add yourself? - Implementation-wise, I think it would be good to model this as already done in case of table properties guardrail (1). A keyspace has also properties as a table has. So if we did not want to have \"durable_writes\" specified, then they would be marked as \"disallowed\", then default, true, would be used for durable_writes. I do not think there should be a guardrail taking care of one specific parameter, we should treat whole family of properties on a keyspace instead. (1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/guardrails/Guardrails.java#L135-L144 - [~smiklosovic] As long as we're sure {{DEFAULT_DURABLE_WRITES}} will remain {{true}} in perpetuity, a {{Values}} guardrail would work. The nice thing about that is this would make it possible to guardrail things like CEP-45 (mutation tracking) without making further code changes. - Yeah I think it is pretty safe bet to say that we will be defaulting to durable writes to be true. - [~anaik] thank you for your patch but I think it would be worth it to rework it as suggested. What you have done is basically what {{Values}} guardrail type is doing. - Sounds good Stefan, thank you! - Thanks! looks way better, I have reviewed on the PR and added few comments. - +1, [~maedhroz] your turn. - Approved w/ nits. I can commit once we get a clean CI run... - we need to fix tests https://pre-ci.cassandra.apache.org/job/cassandra/103/ - [~anaik] there is test failure you have introduced in the very test results you have uploaded. The failing test class is GuardrailTablePropertiesTest {code} junit.framework.AssertionFailedError: Expecting message to be: \"Invalid value for table_properties_warned: '[invalid]' do not parse as valid table properties\" but was: \"Invalid values for table_properties_warned: '[invalid]' do not parse as valid table properties\" Throwable that failed the check: java.lang.IllegalArgumentException: Invalid values for table_properties_warned: '[invalid]' do not parse as valid table properties at org.apache.cassandra.config.GuardrailsOptions.validateTableProperties(GuardrailsOptions.java:1474) at org.apache.cassandra.config.GuardrailsOptions.setTablePropertiesWarned(GuardrailsOptions.java:340) at org.apache.cassandra.db.guardrails.Guardrails.setTablePropertiesWarned(Guardrails.java:793) at org.apache.cassandra.db.guardrails.GuardrailTester.lambda$assertInvalidProperty$0(GuardrailTester.java:180) at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63) at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892) at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366) at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210) at org.apache.cassandra.db.guardrails.GuardrailTester.assertInvalidProperty(GuardrailTester.java:180) at org.apache.cassandra.db.guardrails.GuardrailTablePropertiesTest.assertInvalidProperty(GuardrailTableProper {code} - also this comment: https://github.com/apache/cassandra/pull/4393/files#r2383992357 cc [~maedhroz] - PTAL at this PR - [https://github.com/apache/cassandra/pull/4406] Cherry-picked changes from [b806612|https://github.com/apache/cassandra/commit/b8066128c7dec413cfe5d155d19cf2c77f3af595] - added last comments, feel free to merge this without my assistance after Caleb's final approval / judgement. - None of the failures in the latest attached CI run look like they have anything to do with the patch, and the tests that were failing on an earlier run on pre-ci are green now. I think this is ready to commit... - Committed as https://github.com/apache/cassandra/commit/61959e215c91d837c9dd67e65976456d74f93700 Congrats [~anaik] on your first contribution! - Thank you Caleb for the support. Thank you Stefan for reviewing the PR! Appreciate the feedback.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20912", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20912"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add badges to README.asc for license, ci, downloads, docker, slack, socials\nDescription: Example: https://github.com/thelastpickle/cassandra/blob/mck/read-me-shields/README.asc PR: https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/read-me-shields\nComments: - Great addition to the README. Surprised this wasn't there before. +1 - Committed https://github.com/apache/cassandra/commit/70bcaec54d492658cb331b70869215ad64feb63d", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20911", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20911"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Pattern Tracing\nDescription: Improve tracing: - Introduce pattern tracing: specify an arbitrary number of patterns filtering by txn kind, domain, keys, phase, new/failure; sample matching operations to enable per-transaction filtering - Support multiple reservoirs for both patterns and txnId level: SAMPLE, RING, LEAKY (with additional uniform chance filter that may be applied to any)\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20910", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20910"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Instances from a 2nd ring join another ring when running on the same nodes\nDescription: Hi, We experienced an issue today whereby instances from a 2nd ring join another ring when running on the same nodes following a rolling restart which took place following an OS patch and node reboot (both on Cassandra 4.1.2). The cluster names and storage ports are different and this type of activity normally runs without issue. Any ideas as to what could have happened? Could this be a bug? The seeds use the same IP addresses but no storage port is configured in the seeds parameter, should we add the storage port to prevent this from happening again? Any thoughts? Messages like the following could be seen on ring 1. INFO [GossipStage:1] 2025-09-18 04:11:49,040 Gossiper.java:1434 - Node /XX.XX.XX.190:7002 is now part of the cluster INFO [GossipStage:1] 2025-09-18 04:11:49,043 TokenMetadata.java:539 - Updating topology for /XX.XX.XX.190:7002 INFO [Messaging-EventLoop-3-8] 2025-09-18 04:11:49,044 OutboundConnection.java:1153 - /XX.XX.XX.61:7000(/XX.XX.XX.61:41920)->/XX.XX.XX.190:7002-URGENT_MESSAGES-7af53583 successfully connected, version = 12, framing = CRC, encryption = unencrypted INFO [GossipStage:1] 2025-09-18 04:11:49,044 TokenMetadata.java:539 - Updating topology for /XX.XX.XX.190:7002 INFO [GossipStage:1] 2025-09-18 04:11:49,044 Gossiper.java:1434 - Node /XX.XX.XX.214:7002 is now part of the cluster INFO [Messaging-EventLoop-3-3] 2025-09-18 04:11:49,046 OutboundConnection.java:1153 - /XX.XX.XX.61:7000(/XX.XX.XX.61:62628)->/XX.XX.XX.214:7002-URGENT_MESSAGES-0515b24a successfully connected, version = 12, framing = CRC, encryption = unencrypted INFO [GossipStage:1] 2025-09-18 04:11:49,046 TokenMetadata.java:539 - Updating topology for /XX.XX.XX.214:7002 INFO [GossipStage:1] 2025-09-18 04:11:49,046 TokenMetadata.java:539 - Updating topology for /XX.XX.XX.214:7002 INFO [GossipStage:1] 2025-09-18 04:11:49,047 Gossiper.java:1434 - Node /XX.XX.XX.247:7002 is now part of the cluster INFO [Messaging-EventLoop-3-4] 2025-09-18 04:11:49,048 InboundConnectionInitiator.java:529 - /XX.XX.XX.190:7002(/XX.XX.XX.190:60180)->/XX.XX.XX.61:7000-URGENT_MESSAGES-edfb2d8f messaging connection established, version = 12, framing = LZ4, encryption = unencrypted Messages like the following in ring 2: WARN [GossipStage:1] 2025-09-18 04:11:49,304 GossipDigestSynVerbHandler.java:58 - ClusterName mismatch from /XX.XX.XX.247:7000 ring1!=ring2 WARN [GossipStage:1] 2025-09-18 04:11:49,819 GossipDigestSynVerbHandler.java:58 - ClusterName mismatch from /XX.XX.XX.108:7000 ring1!=ring2 WARN [GossipStage:1] 2025-09-18 04:11:51,598 GossipDigestSynVerbHandler.java:58 - ClusterName mismatch from /XX.XX.XX.190:7000 ring1!=ring2 WARN [GossipStage:1] 2025-09-18 04:11:52,361 GossipDigestSynVerbHandler.java:58 - ClusterName mismatch from /XX.XX.XX.111:7000 ring1!=ring2 WARN [GossipStage:1] 2025-09-18 04:11:53,489 GossipDigestSynVerbHandler.java:58 - ClusterName mismatch from /XX.XX.XX.84:7000 ring1!=ring2 WARN [GossipStage:1] 2025-09-18 04:11:58,322 GossipDigestSynVerbHandler.java:58 - ClusterName mismatch from /XX.XX.XX.247:7000 ring1!=ring2 Instances from ring2 were listed in nodetool describecluster as unreachable under schema versions. They were also listed as DN under nodetool status. The nodetool removenode command was used to remove the instances successfully. Regards, Chris.\nComments: - You should probably provide as much detail about your setup as you can. On the surface it seems like ring1 should've been rejecting ring2 with a cluster name mismatch, the same way it happened in reverse. - Thanks [~brandon.williams] - are there any specific details that you would need to investigate? Are there any known bugs where rings aren't rejected following a cluster name mismatch in 4.1.2+? Thanks, Chris. - Nothing specific, yet at least. I don't know of any bugs around cluster name mismatch. - Hi [~brandon.williams] do you think there would be any value in adding the storage port to the seeds parameter to prevent this from happening again? or should we need to do this? Note that I'm trying to reproduce this in a lab environment with no success yet. Please let me know if there is anything we can try in our lab? Would a jstack dump be useful if we can reproduce this? Thanks, Chris. - Hi [~brandon.williams] - not sure if this helps but we noticed the following Prometheus ERROR: {code:java} INFO [prometheus-netty-pool-0] 2025-09-18 04:11:39,508 FBUtilities.java:166 - InetAddress.getLocalHost() was used to resolve listen_address to xxxx/yyyy, double check this is correct. Please check your node's config and set the listen_address in cassandra.yaml accordingly if applicable. {code} This error immediately follows below message which indicates that the correct cassandra.yaml was loaded: {code:java} INFO [main] 2025-09-18 04:11:39,273 YamlConfigurationLoader.java:104 - Configuration location: file:/opt/xxxx/apache-cassandra-4.1.2/conf/cassandra.yaml {code} Thanks, Chris. - [~brandon.williams] - any thoughts re: my last update? Also fyi, we have specified the storage port in the seeds configuration in the second cluster. So would be interested to hear from you as to whether this measure will help prevent a reoccurrence or not. Thanks, Chris. - bq. not sure if this helps but we noticed the following Prometheus ERROR: The thing I notice here is the thread pool name `prometheus-netty-pool-0` since C* doesn't integrate prometheus, where is this coming from? Maybe specifying the seed port helps keep things on the right track, but we still don't understand why the cluster name mismatch didn't protect there, so it's hard to say whether this will prevent anything. I think explicit is better than implicit though, so it's a good change to have regardless. - Hi [~brandon.williams] We're using Cassandra exporter so assume that explains the thread pool name `prometheus-netty-pool-0. We've implemented the explicit seed port and will update this ticket if we experience a re-occurrence. Has any progress been made re: why the cluster name mismatch didn't protect the cluster? Thanks, Chris.", "output": "Status: Triage Needed\nPriority: Urgent"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20909", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20909"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Potential inefficiency in first flush with TrieMemtable + UnifiedCompaction\nDescription: Hello, I\u2019m a Cassandra newbie, so please correct me if I\u2019ve misunderstood something. While evaluating Cassandra for adoption, I\u2019ve been testing different modes. In the process of exploring features, I considered using TrieMemtable together with the UnifiedCompaction strategy (level). My thought was that combining the two could allow SSTables created from memtables to utilize token ranges much more efficiently, so I\u2019ve been investigating whether this is feasible. During this testing, I noticed that the first SSTable created is always generated as a single file. Because only one SSTable is produced, it ends up covering the entire token range, which I believe is highly inefficient. After digging into the source, I found that the cause seems to be in {{{}org.apache.cassandra.db.compaction.unified.Controller#getFlushSizeBytes{}}}: {{double envFlushSize = cfs.metric.flushSizeOnDisk.get();}} For the very first flush, this metric returns {{{}0{}}}, which results in the minimum value of {{1}} being used. This explains why the first SSTable covers the entire token range. {code:java} INFO [NativePoolCleaner] 2025-09-19 09:47:10,318 AbstractAllocatorMemtable.java:302 - Flushing largest CFS(Keyspace='tx_bench_stcs', ColumnFamily='tx2') to free up room. Used total: 0.14/0.25, live: 0.14/0.25, flushing: 0.00/0.00, this: 0.14/0.25 INFO [NativePoolCleaner] 2025-09-19 09:47:10,319 ColumnFamilyStore.java:1052 - Enqueuing flush of tx_bench_stcs.tx2, Reason: MEMTABLE_LIMIT, Usage: 285.357MiB (14%) on-heap, 2.000GiB (25%) off-heap INFO [MemtableFlushWriter:3] 2025-09-19 09:47:10,627 UnifiedCompactionStrategy.java:275 - Flush density: 0.0, calculated shards: 1, flushSizeBytes: 0.0, override: 0 INFO [MemtableFlushWriter:3] 2025-09-19 09:47:10,629 ShardedMultiWriter.java:96 ShardedMultiWriter ---/bin/./../data/data/tx_bench_stcs/tx2-bfd93e0094f111f0bebb039c0b9bf5db/nb-1-big 1 INFO [MemtableFlushWriter:3] 2025-09-19 09:47:10,645 Flushing.java:221 - Created flush writer: ShardedMultiWriter for tx_bench_stcs.tx2 INFO [PerDiskMemtableFlushWriter_0:3] 2025-09-19 09:47:10,646 Flushing.java:153 - Writing Memtable-tx2@1210444503(1.978GiB serialized bytes, 379732 ops, 285.364MiB (14%) on-heap, 2.000GiB (25%) off-heap), flushed range = [min(-9223372036854775808), max(9223372036854775807)) INFO [PerDiskMemtableFlushWriter_0:3] 2025-09-19 09:47:19,056 Flushing.java:179 - Completed flushing -------/data/data/tx_bench_stcs/tx2-bfd93e0094f111f0bebb039c0b9bf5db/nb-1-big-Data.db (1.879GiB) for commitlog position CommitLogPosition(segmentId=1758242550278, position=2579081) {code} I also experimented with using {{flushSizeOverride}} to generate arbitrary shard values, but since this parameter does not adjust dynamically, I feel this is not a good long-term solution. Once again, I may be misunderstanding this behavior, so please correct me if my interpretation is wrong. Thank you.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20908", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20908"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Rebootstrap and unsafe bootstrap\nDescription: To support recovering a node that has lost some of its local transaction log, introduce rebootstrap and unsafe bootstrap modes, where Accord ensures no responses are produced for transactions the node cannot be certain it had not previously answered.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20907", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20907"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CCM not using correct version when validating jdk on install-dir clusters\nDescription: When creating a cluster and validating the jdk, check the cassandra version. {{common.py::assert_jdk_valid_for_cassandra_version}} should perform the jdk version check based on only cassandra versions\nComments: - https://github.com/apache/cassandra-ccm/pull/795 - Committed https://github.com/apache/cassandra-ccm/commit/3ef48de49e428e27653f5639f492a99851e2282d", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20906", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20906"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Split AsyncChain and AsyncResult; normalise AsyncResult with C* Future\nDescription: \nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20905", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20905"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Strict Serializability Verifier may incorrectly handle blind writes\nDescription: Blind writes that are witnessed before the operation that writes them is ACK'd may be incorrectly marked as a history violation. This is due to a simple bug of using binarySearch on an unsorted array (whoops).\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20904", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20904"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: cassandra-stress does not work with TLSv1.2 and TLSv1.3\nDescription: A customer has reported that cassandra-stress does not work on 4.x branch with TLS. As we are still officially supporting this branch, we should fix it. It was empirically tested it works when Cassandra Java Driver 3.12.1 is used, also it works when 3.11.5 is used instead of 3.11.0 which is present in 4.x. We should at least bump it to 3.11.5 to have the least amount of friction possible while addressing the issue.\nComments: - 4.0 https://app.circleci.com/pipelines/github/instaclustr/cassandra/6032/workflows/7a9f57f2-126c-448c-aec4-d4e5cace0bcc - [CASSANDRA-20904-4.1|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20904-4.1] {noformat} java11_pre-commit_tests \u2713 j11_build 2m 31s \u2713 j11_cqlsh_dtests_py38_vnode 6m 34s \u2713 j11_cqlshlib_cython_tests 8m 18s \u2713 j11_cqlshlib_tests 6m 56s \u2713 j11_dtests_vnode 41m 20s \u2713 j11_jvm_dtests 15m 45s \u2713 j11_jvm_dtests_vnode 12m 55s \u2713 j11_unit_tests 9m 17s \u2715 j11_cqlsh_dtests_py3 5m 38s cql_tracing_test.TestCqlTracing test_tracing_simple \u2715 j11_cqlsh_dtests_py311 5m 48s cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_cqlsh_dtests_py311_vnode 6m 1s cql_tracing_test.TestCqlTracing test_tracing_simple \u2715 j11_cqlsh_dtests_py38 5m 59s cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_cqlsh_dtests_py3_vnode 6m 52s cql_tracing_test.TestCqlTracing test_tracing_default_impl \u2715 j11_dtests 58m 42s rebuild_test.TestRebuild test_simple_rebuild refresh_test.TestRefresh test_refresh_deadlock_startup {noformat} [java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/6035/workflows/39bfc8ef-912c-4747-bb90-6ddf34d7dfcf] - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20903", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20903"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Dont support vectors of counters when creating tables\nDescription: It is possible to create a table containing vectors of counters eg CREATE TABLE test_counter (id int PRIMARY KEY, vec vector<counter, 3>); This makes no sense and should error at the DB level.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20902", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20902"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-54 - ZSTD Dictionary compression\nDescription: Epic for gathering all work related to CEP-54.\nComments: No comments.", "output": "Status: In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20901", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20901"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Typo on log message when a query was aborted due to too many tombstones\nDescription: I just noticed that there is a typo in the log message when a query gets aborted due to too many tombstones. Its in https://github.com/apache/cassandra/blob/d7a46b52ef2634ef48acee566cce0a6cccd7f244/src/java/org/apache/cassandra/db/filter/TombstoneOverwhelmingException.java#L31 Currently we have {{[...] and partion key was [...]}} , which should be {{[...] and partition key was [...]}}. Affects (at least) Cassandra 5 and 4.1.\nComments: - There are quite a few 'partion' typos: {noformat} $ git grep -i partion | wc -l 18 {noformat} - PR: [https://github.com/apache/cassandra/pull/4401]", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20900", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20900"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remote Virtual Tables\nDescription: Introduce RemoteToLocalVirtualKeyspace and RemoteToLocalVirtualTable, that wrap any pre-existing nominated virtual tables/keyspaces, allowing them to be queried from any other node.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20899", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20899"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Lazy Virtual Tables\nDescription: Virtual tables construct their result eagerly by default, with no protection against time or heap overruns, which has proven to be a problem for some Accord debug virtual tables. This patch introduces AbstractLazyVirtualTable that is just as easy to use (requires minimal porting), but: - aborts the query if it is taking too long - eagerly filters rows by any search criteria, and discards any columns we are not selecting - eagerly applies limits to the records it collects, so that we never keep more than some multiple of the final result set in memory - supports lazy materialisation of rows (at caller's discretion), so we can avoid formatting inputs until they have been excluded by the above filters - can exploit in-order visitation to tightly bound this extra work\nComments: - Great patch. I think it is worth to take a look at LogMessagesTable which can hold system logs. It is currently {code:java} super(TableMetadata.builder(keyspace, TABLE_NAME) .comment(TABLE_COMMENT) .kind(TableMetadata.Kind.VIRTUAL) .partitioner(new LocalPartitioner(TimestampType.instance)) .addPartitionKeyColumn(TIMESTAMP_COLUMN_NAME, TimestampType.instance) .addClusteringColumn(ORDER_IN_MILLISECOND_COLUMN_NAME, Int32Type.instance) .addRegularColumn(LOGGER_COLUMN_NAME, UTF8Type.instance) .addRegularColumn(LEVEL_COLUMN_NAME, UTF8Type.instance) .addRegularColumn(MESSAGE_COLUMN_NAME, UTF8Type.instance).build(), size); {code} then when somebody does {code:java} select message from system_views.system_logs {code} for now it discards everything but messages column. That table can handle 100k rows max. If we take into consideration LOGGER_COLUMN_NAME which holds name of a logger, not a lot of times we need to see the logger itself, we just want to see timestamp and message. But for now we would still go through the logic of populating that column while it is completely unnecessary. I will try to find some time to optimize this when this lands.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20898", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20898"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: AST fuzz testing support for range reads on keyspaces w/ mutation tracking\nDescription: We've roughly stabilized single-partition reads in CASSANDRA-20830, but there are still several outstanding failure modes for range reads, and at least one scenario (i.e. CASSANDRA-19007) has not yet been tested. It's time to dig into to root causes of those failures, starting with the set of minimal repros already codified in {{MutationTrackingRangeReadTest}}.\nComments: - I may narrow the scope of this to exclude range read bugs related to CASSANDRA-20954, which are being hit by the fuzz test ATM - Narrowing this further to just pushing up support for range reads in {{MultiNodeTableWalkWithMutationTrackingTest}} and {{MutationTrackingRangeReadTest}} that can serve as a backstop for the correctness of CASSANDRA-20954.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20897", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20897"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-55 Generated user names\nDescription: https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-55+Generated+role+names\nComments: - [https://github.com/apache/cassandra/pull/4379/files] - +1 - https://pre-ci.cassandra.apache.org/job/cassandra/134/#showFailuresLink - multiplexers https://app.circleci.com/pipelines/github/instaclustr/cassandra/6090/workflows/9c74b412-1ecc-4d44-ae1e-2e1d536d6624", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20896", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20896"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord: Topology compaction omits image records if minEpoch is ahead of last image\nDescription: Topology compaction omits image records if minEpoch is ahead of last image Also Fix: - RegisteredCallback should remove itself from callback map when cancelled - Do not throw CancellationException when processing requests that have been aborted, as may be caused by a successful meaningful reply that can be overridden - system_accord_debug.{executors,coordinations} fail with ClassCastException - CommandStore.updateMinHlc eats up CPU as called much too often - AccordCache not notifying flushed on shutdown Also Improve: - Improve violation information - Support skipping Deps - Violation information reported - Sort CommandStore shards by id\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20895", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20895"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Simplify upgrade to 5.x by requiring fewer rolling restarts\nDescription: The setting {{storage_compatibility_mode}} was introduced by CASSANDRA-14227 to allow a phased transition from cassandra 4.x storage format to cassandra 5.x storage format. The [upgrade instructions|https://github.com/apache/cassandra/blob/5d83175952dbda3cfca594764841fb3c584fd1ce/conf/cassandra.yaml#L2279-L2281] suggests 3 rolling restarts to upgrade a cluster from 4.X to 5.X: 1. Upgrade cluster to Cassandra 5 with {{storage_compatibility_mode=CASSANDRA_4}} 2. Rolling restart the cluster with {{storage_compatibility_mode=UPGRADING}} 3. Rolling restart the cluster with {{storage_compatibility_mode=NONE}} However this puts additional burden on operators, since it requires 3 restarts to fully upgrade a cluster to 5.x: CASSANDRA_4 -> UPGRADING -> NONE, 2 more than previous upgrade procedures where a single restart was required. Nodes started in *UPGRADING* compatibility mode will be started with Cassandra 5.0 messaging format and sstables, except that they will not accept writes with TTL higher than 2038 which is the limit before 5.x from CASSANDRA-14092 ([relevant code|https://github.com/apache/cassandra/blob/965a39166cf22f020065578407ca6c585b4a5132/src/java/org/apache/cassandra/db/rows/Cell.java#L96-L99]). The idea is to prevent sending writes with TTLs expiring after 2038 to nodes still in the old version. Once all nodes have been restarted with {{{}storage_compatibility_mode=UPGRADING{}}}, the nodes will start accepting TTLS expiring above 2038. At this stage, if a node is accidentally started in version 4.x or {{storage_compatibility_mode=CASSANDRA_4}}, the node toggles back to accepting only TTLs up to 2038. In order to avoid this, the [upgrade instructions|https://github.com/apache/cassandra/blob/5d83175952dbda3cfca594764841fb3c584fd1ce/conf/cassandra.yaml#L2279-L2281] suggest restarting the node with {{storage_compatibility_mode=NONE}}. Since *UPGRADING* mode is equivalent to *NONE* once all nodes are upgraded, we could relax the requirement of an additional rolling restart by making the *UPGRADING* state stable once all nodes have been upgraded. That is, even if nodes are restarted in earlier versions, the node would remain accepting TTLs expiring after 2038 similar to the *NONE* mode. After this we could potentially simplify the upgrade instructions to: - Upgrade to 5.x with {{storage_compatibility_mode=CASSANDRA_4}} - Restart with {{storage_compatibility_mode=UPGRADING}} - Upgrade completed, no additional rolling restart needed Alternatively if this turns out to be invasive we can update the instructions to make the third rolling restart optional.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20894", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20894"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Unqualified prepared statements are duplicated in cache\nDescription: The compatibility logic added by CASSANDRA-15252 and CASSANDRA-17248 duplicates prepared statements in the cache to support both the new and old formats while upgrading from versions earlier than 3.0.26, 3.11.12 and 4.0.2. I think we should remove this logic in trunk and perhaps make it optional in 5.0 to avoid using double the amount of space to cache unqualified prepared statements.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20893", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20893"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Test failure: JmxVirtualTableMetricsTest.testJmxEqualVirtualTableByMetricType\nDescription: Recent CI runs have failures for * JmxVirtualTableMetricsTest.testJmxEqualVirtualTableByMetricType * dtest-latest.client_request_metrics_test/TestClientRequestMetrics {code:java} AssertionError: org.apache.cassandra.metrics:type=ClientRequest,scope=Write-ANY,name=LatencyMean assert 0.0 is None {code} [https://ci-cassandra.apache.org/job/Cassandra-trunk/2265/testReport/junit/org.apache.cassandra.metrics/JmxVirtualTableMetricsTest/Tests___test_jdk11_7_20___testJmxEqualVirtualTableByMetricType__jdk11_x86_64/] [https://ci-cassandra.apache.org/job/Cassandra-trunk/2267/testReport/junit/dtest-latest.client_request_metrics_test/TestClientRequestMetrics/Tests___dtest_latest_jdk11_34_64___test_client_request_metrics/] [http://ci-cassandra.infra.datastax.com/job/cassandra/36/testReport/junit/dtest-latest.client_request_metrics_test/TestClientRequestMetrics/Tests___dtest_latest_jdk11_34_64___test_client_request_metrics/]\nComments: - [~benedict] can it be caused by this recent commit: [https://github.com/apache/cassandra/commit/a813a51f7c4bc7379af3621e7386e6a6ca11884f] ? - Yes, I believe it was. I believe it has already been addressed by CASSANDRA-20886.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20892", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20892"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Don't allow JMX operations for denylist when denylist is disabled\nDescription: While following the documentation ([2]), I executed the following commands {code:java} bean org.apache.cassandra.db:type=StorageProxy run denylistKey foo buz 3 true run loadPartitionDenylist run isKeyDenylisted foo buz 3 false {code} but encountered some confusing behavior: 1. The function _*isKeyDenylisted*_ always returned false, which was unexpected. 2. However, I observed from the logs that *_loadPartitionDenylist_* executed successfully: {code:java} Loaded partition denylist cache in 24ms{code} 3. Additionally, running the query {*}_SELECT * FROM foo.buz WHERE key=3_{*}; did not throw an exception as anticipated. After further investigation, I discovered that the feature needs to be explicitly enabled by setting *_partition_denylist_enabled: true_* in the _*cassandra.yaml*_ file. Unfortunately, this requirement is not mentioned in the documentation ([1], [2]). Reference: [1] [https://cassandra.apache.org/doc/latest/cassandra/managing/operating/denylisting_partitions.html] [2] [https://cassandra.apache.org/_/blog/Apache-Cassandra-4.1-Denylisting-Partitions.html]\nComments: - hi [~maoling], what branches are behaving like this? - Actually, as an improvement, we can do that just for trunk, makes perfect sense.", "output": "Status: Changes Suggested\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20891", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20891"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45: Support host replacement with tracked keyspaces\nDescription: Host replacement is the simplest topology change to make since there is token movement. Contrary to the initial design, topology changes shouldn't need the bulk transfer mechanism. SSTables streamed in will either be fully reconciled, or contain an sstable component describing which ids they contain. We can populate the new node's view of mutations it has witnessed from this (and the writes it receives during it's time in a pending state). Fully reconciled should be discovered via offsets broadcasting, and this should put the node in the right state to start handling reads once streaming completes. Main changes are: * Streaming path ** Need to add sstable offsets to mutation tracking service as replicated. * Read path ** Pull mutation rejection *** Since we'll be advertising offsets that we only have in sstable form, we will have to reject some mutation pull requests. This will need to be gracefully handled by read coordinator ** Durability update *** Node needs to know which mutations it has in sstables and/or are fully reconciled even without log entries across restarts\nComments: - https://github.com/apache/cassandra/pull/4396 - +1 from Abe on github", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20890", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20890"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45 - Mutation Tracking: Review metrics\nDescription: Some of the metrics we have should be reviewed so they're providing the most useful information. Current metrics to consider are below, add to the description of the JIRA as others are found ReadRepairMetrics.trackedReconcile.mark() is called for every single read. This is technically correct, but is not be a useful metric. Let's consider marking it from ReadReconciliations.Coordinator#{acceptLocalSummary, acceptRemoteSummary} so we're only counting reconciliations that happen when a replica needs to be sent mutations during a read.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20889", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20889"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CEP-45 - Mutation Tracking: De-duplicate outgoing mutations\nDescription: Outgoing mutation deduplication work was started as part of CASSANDRA-20729 (CEP-45: Implement unified reconcile logic), but wasn't completed. We should finish this so we're not sending the same mutation multiple times.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20888", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20888"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Minor improvements and hardening for IndexHints\nDescription: There are some follow-up items for CASSANDRA-18112, mostly related to the IndexHints class: 1.) We can serialize vints rather than shorts in the IndexSetSerializer 2.) The limit on the number of included/excluded indexes is way too high. It should be something like the guardrail on indexes per table, not Short.MAX_VALUE 3.) notExcluded() can return an Iterable and avoid set creation in some cases 4.) MessagingService#endpointsWithConnectionsOnVersionBelow() should be able to short-circuit faster in the inner loop when any messaging type detects a bad node\nComments: - All CI failures are also failing on trunk either in Butler or my CI infrastructure. - Have approval from [~dcapwell] in GH...moving to commit...", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20887", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20887"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make legacy index rebuilds safe on Gossip -> TCM upgrades\nDescription: On 5.0 -> trunk (6.0) upgrades, it is possible to start a legacy 2i build before schema has been committed, and this can lead to... {noformat} ERROR 2025-08-18 02:07:04,935 [SecondaryIndexManagement:1] org.apache.cassandra.utils.JVMStabilityInspector:70 - Exception in thread Thread[#92,SecondaryIndexManagement:1,5,SecondaryIndexManagement] java.lang.RuntimeException: java.lang.AssertionError: Unknown keyspace <redacted> at org.apache.cassandra.utils.Throwables.unchecked(Throwables.java:308) at org.apache.cassandra.utils.Throwables.cleaned(Throwables.java:327) at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:561) at org.apache.cassandra.index.internal.CassandraIndex.buildBlocking(CassandraIndex.java:711) at org.apache.cassandra.index.internal.CassandraIndex.lambda$getBuildIndexTask$6(CassandraIndex.java:681) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:1583) Caused by: java.lang.AssertionError: Unknown keyspace <redacted> at org.apache.cassandra.db.Keyspace.open(Keyspace.java:149) at org.apache.cassandra.db.Keyspace.openAndGetStore(Keyspace.java:167) at org.apache.cassandra.db.ReadExecutionController.forCommand(ReadExecutionController.java:154) at org.apache.cassandra.db.ReadCommand.executionController(ReadCommand.java:535) at org.apache.cassandra.index.SecondaryIndexManager.indexPartition(SecondaryIndexManager.java:1066) at org.apache.cassandra.index.internal.CollatedViewIndexBuilder.build(CollatedViewIndexBuilder.java:82) at org.apache.cassandra.db.compaction.CompactionManager$13.run(CompactionManager.java:2033) at org.apache.cassandra.concurrent.FutureTask$3.call(FutureTask.java:141) ... 6 common frames omitted {noformat}\nComments: - It's possible to simply rebuild the index after the upgrade completes, but it should never happen in the first place. Note that SAI dos not have this problem, as it indexes SSTables, not entire tables via {{SIM#indexPartition()}}. - https://github.com/apache/cassandra/pull/4363 - I don't see any new failures in CI, but I'll do a full audit... - All CI failures are also failing on {{trunk}} either in Butler or my CI infrastructure. - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20886", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20886"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix pending topology notifications\nDescription: All pending topology notifications must be propagated to a newly published epoch, not only the pending notification for that epoch Also Fix: - JournalAndTableKeyIterator not merging in consistent order, which can lead to replaying topologies in non-ascending order - Invariant failure when reporting topologies if fetchTopologies on startup yields a gap - removeRedundantMissing could erroneously remove missing dependencies occurring after the new appliedBeforeIndex - Invariant failure for some propagate calls supplying 'wrong' addRoute - Disambiguate nextTxnId and nextTxnIdWithDefaultFlags Also Improve: - LocalLog should not use NoSuchElementException for control flow (instead use ConcurrentSkipListMap)\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20885", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20885"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optimize org.apache.cassandra.audit.AuditLogManager#batchSuccess\nDescription: When we have an audit enabled for DDL/LOGIN events (and it is disabled for DML) we have an overhead for BATCH operations (0.75% of CPU usage in a real business workload), when we build a BATCH audit event to check that it is not in scope of audit and throw it away !image-2025-09-04-13-52-19-490.png|width=600! {code:java} UUID batchId = UUID.randomUUID(); String queryString = String.format(\"BatchId:[%s] - BATCH of [%d] statements\", batchId, statements.size()); {code}\nComments: - we can add an API like: isFiltered(AuditLogEntryType auditLogEntryType) to check if we need BATCH events to audit [^audit_batch_perf.patch] - a sketch for the idea - +1 on https://github.com/apache/cassandra/pull/4364 we need to build this. - https://pre-ci.cassandra.apache.org/job/cassandra/87/#showFailuresLink I do not think anything is related to this change.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20884", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20884"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move JMX classes to the in-jvm-dtest API project and other small dtest-api improvements\nDescription: Several important JMX-related test classes should be moved over from the Cassandra codebase to the in-jvm-dtest library for two reasons: 1) They are duplicated in every Cassandra version from 4.0 onwards, and don't need to be. 2) More imporantly, using these classes outside of the Cassandra project (in sidecar and analytics) requires that those projects also have access to these classes, and we've worked around this with some fairly ugly hacks (copy/paste of code across even more projects) to date. Moving these will eliminate the need for the hacks.\nComments: - See Github PRs - +1 - +1 thanks for the patch - Added ci summaries for all 4 C* branches - confirmed test failures are unrelated to my changes and are, I believe, all documented already. - I've deleted and re-attached CI results for all four branches with the in-jvm dtest api jar's release build, and updated all of the branches to use it.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20883", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20883"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add support in the binary protocol to allow transactions to have multiple conditions\nDescription: Right now the binary format hard codes a single condition, so adding ELSE or IF ELSE or any other type of condition can\u2019t be done. Need to update the logic to be more flexible so we can add more conditional types in a backwards compatibility way.\nComments: - Benedict created a feedback branch here: https://github.com/belliottsmith/cassandra/tree/CASSANDRA-20883 Pulling in his feedback and going to do some final cleanup - Accord was approved and merged to trunk last night - +1 from Benedict in GH", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20882", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20882"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implement mutation tracking metadata persistence\nDescription: Persist shards, coordinator log info, and last host log id (for generating coordinator log ids) to system keyspace tables.\nComments: - GH PR: https://github.com/apache/cassandra/pull/4360", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20881", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20881"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: make LogbackMetrics work as a virtual table\nDescription: When opening the following appender in the _*logback.xml*_ {code:java} <appender name=\"LogbackMetrics\" class=\"com.codahale.metrics.logback.InstrumentedAppender\" />{code} then restart the cluster, we will encounter the following exceptions: {code:java} Caused by: java.lang.IllegalStateException: Unknown metric group: LogbackMetrics at org.apache.cassandra.metrics.CassandraMetricsRegistry.verifyUnknownMetric(CassandraMetricsRegistry.java:277) at org.apache.cassandra.metrics.CassandraMetricsRegistry.register(CassandraMetricsRegistry.java:411) at {code} This patch makes LogbackMetrics work again as a virtual table. The preview is: {code:java} cqlsh:system_views> select * from system_metrics.logback_metrics_group ; name | scope | type | value ---------------------------------------------------+-----------+-------+------- org.apache.cassandra.metrics.LogbackMetrics.all | undefined | meter | 1433 org.apache.cassandra.metrics.LogbackMetrics.debug | undefined | meter | 940 org.apache.cassandra.metrics.LogbackMetrics.error | undefined | meter | 0 org.apache.cassandra.metrics.LogbackMetrics.info | undefined | meter | 483 org.apache.cassandra.metrics.LogbackMetrics.trace | undefined | meter | 0 org.apache.cassandra.metrics.LogbackMetrics.warn | undefined | meter | 10 {code}\nComments: - Can you describe the usage scenario of this virtual table?", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20880", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20880"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: strict MV consistency backed by Paxos V2 implementation\nDescription: \nComments: - Initial PR: [https://github.com/apache/cassandra/pull/4358] [~bdeggleston] [~benedict] [~chovatia.jaydeep@gmail.com] could you please review this one? This is the change for the online hot path change for CEP-48. The work for backfill optimization is still in progress. Thanks. - Sorry [~curlylrt], I won't be able to review this.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20879", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20879"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add Swift Cassandra client to drivers list in documentation\nDescription: There is an actively maintained Swift Cassandra client: https://github.com/apple/swift-cassandra-client I am proposing including the Swift Cassandra client to the list of client drivers in Apache Cassandra docs.\nComments: - PR: https://github.com/apache/cassandra/pull/4357 - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20878", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20878"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improve Accord Observability\nDescription: Improve Observability: - Track all active Coordinations - Refactor Replica/Coordinator metrics and report Coordinator exhausted/preempted/timeout - DurabilityQueue metrics and visibility Also Fix: - WaitingState can get cause distributed stall when asked to wait for CanApply if not yet PreCommitted; track separate querying state and advance this to the next achievable state rather than the desired final state - Stalled coordinators should not prevent recovery - Edge case with fetch unable to make progress when pre-bootstrap and all peers have GC'd - Dependency initialisation for sync points across certain ownership changes - SyncPoint propagation may not include all of the epochs required on the receiving node for ranges they have lost but not closed, and receiving node does not validate them - Stable tracker accounting with LocalExecute - Do not prune non-durable APPLIED as must be reported in dependencies until durably applied (so as not to break recovery) - Ensure we cannot race with replies when initiating Coordination - ProgressLog does not guarantee to clear home or waiting states when erased or invalidated by compaction - WaitingState on non-home shard cannot guarantee progress once home shard is Erased - WaitingOnSync handles retired ranges incorrectly Also Improve: - Standardise failure accounting, use null to represent single reply timeouts - BurnTest record/replay to/from file\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20877", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20877"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: FINALIZED incremental local repair sessions are not cleaned up in case of a range movement\nDescription: * system.repairs table is local per each Cassandra node. * This table is cleaned up by a periodically running org.apache.cassandra.repair.consistent.LocalSessions#cleanup() job. * The job runs every cassandra.repair_cleanup_interval_seconds (with default = 10 minutes). * The job should delete repair sessions with FINALIZED state which are older than cassandra.repair_delete_timeout_seconds (with default value = 1 day). * Before deleting of a FINALIZED session org.apache.cassandra.repair.consistent.LocalSessions#isSuperseded check is executed for them to ensure if all ranges and tables covered by this session have since been re-repaired by a more recent session. If it is not superseded the session info delete from the table is skipped and a log message is printed: {code:java} Skipping delete of FINALIZED LocalSession {repairSessionId} because it has not been superseded by a more recent session\"{code} * isSuperseded logic allows to delete a repair session info only if all session ranges are covered by some newer session on the node. If we added a new node then a set of ranges is moved to it and for these ranges data are not repaired anymore on the old nodes, so isSuperseded always return false for the last session executed before the node adding. If we have a big cluster with a lot of nodes added while an incremental repair is executed regularly then we get a lot of non-removable old records in system.repairs table it may slow down startup for Cassandra nodes especially if a large number of tokens is used on the cluster historically. A similar issue is with a table removal, the logic consider the last session which was executed for a removed table as non-superseded and keeps it forever.\nComments: - a possible way to fix the issue can be to add the following kinds of ignoring into isSuperseded logic: # ignore non-local ranges # ignore non-existing tables - Patch for 4.0: https://github.com/apache/cassandra/pull/4365 - [~chovatia.jaydeep@gmail.com] does this look OK to you? Trying to get some reviews from \"repair people\". Thanks! - Sure, I will review it by early next week. - 4.0 branch lgtm with small comment in gh, could you provide the rest of the branches and test runs? - 4.0 test results: [https://ci-cassandra.apache.org/job/Cassandra-devbranch-before-5/2696/] Failed tests: dtest-novnode.commitlog_test.TestCommitLog.test_mv_lock_contention_during_replay dtest.global_row_key_cache_test.TestGlobalRowKeyCache.test_functional org.apache.cassandra.net.ProxyHandlerConnectionsTest.testExpireSome-cdc dtest-novnode.cqlsh_tests.test_cqlsh.TestCqlsh.test_unicode_invalid_request_error dtest-novnode.cqlsh_tests.test_cqlsh.TestCqlsh.test_unicode_invalid_request_error dtest.cqlsh_tests.test_cqlsh.TestCqlsh.test_unicode_invalid_request_error dtest.cqlsh_tests.test_cqlsh.TestCqlsh.test_unicode_invalid_request_error org.apache.cassandra.distributed.upgrade.DropCompactStorageTest.testDropCompactStorage none are related to the changes. 4.1/5.0/trunk - MR and tests are in progress. - Overall, it looks fine to me. Added a couple of comments to the PR. - Thank you! - ||Branch||MR||Test run|| |4.0|[https://github.com/apache/cassandra/pull/4365]|done, shared in a previous comment| |4.1|[https://github.com/apache/cassandra/pull/4421] |TBD| |5.0|[https://github.com/apache/cassandra/pull/4422] |TBD| |trunk|[https://github.com/apache/cassandra/pull/4424] |done| - trunk test results: * [^CASSANDRA-20877-trunk_ci_summary.htm] * [^CASSANDRA-20877-trunk_results_details .tar.xz] Failed tests: dtest-latest.client_request_metrics_test.TestClientRequestMetrics test_client_request_metrics dtest-latest.client_request_metrics_test.TestClientRequestMetrics test_client_request_metrics dtest.client_request_metrics_test.TestClientRequestMetrics test_client_request_metrics dtest.client_request_metrics_test.TestClientRequestMetrics test_client_request_metrics db.compaction.CompactionInfoTest testCompactionInfoToStringFormat-latest_jdk17_x86_64 distributed.test.accord.AccordProgressLogTest testFetchTimeWindow-_jdk11_x86_64 index.accord.RouteIndexTest test-cassandra.testtag_IS_UNDEFINED simulator.test.ShortPaxosSimulationTest selfReconcileTest-cassandra.testtag_IS_UNDEFINED not related to the current change - I've added an additional small change to the new test (IncrementalRepairCleanupAfterNodeAddingTest) to avoid flakiness: sessions are switched to FINALIZED state in an async way, after a repair is complete. ||Branch||MR||Test run|| |4.0|[https://github.com/apache/cassandra/pull/4365]|[https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-before-5/2699/] Failed tests are not related to the current change| |4.1|[https://github.com/apache/cassandra/pull/4421] |[https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-before-5/2700/] Failed tests are not related to the current change| |5.0|[https://github.com/apache/cassandra/pull/4422] |[https://pre-ci.cassandra.apache.org/job/cassandra-5.0/39/] [^CASSANDRA-20877-5.0_ci_summary.htm][^CASSANDRA-20877-5.0_results_details.tar.xz] Failed tests are not related to the current change| |trunk|[https://github.com/apache/cassandra/pull/4424] |[https://pre-ci.cassandra.apache.org/job/cassandra/170/] [^CASSANDRA-20877-trunk_results_details.tar.xz] [^CASSANDRA-20877-trunk_ci_summary-1.htm]CASSANDRA-20877-trunk_results_details.tar.xz{^}!/jira/images/icons/link_attachment_7.gif|width=7,height=7,align=absmiddle!{^} Failed tests are not related to the current change| - [~marcuse] [~chovatia.jaydeep@gmail.com] are you ok with the current changes? - It looks good to me.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20876", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20876"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Offline nodetool commands should not print network options in help\nDescription: There is no reason to print \"network related options\" like port, password etc. for offline nodetool commands like \"history\" and similar.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20875", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20875"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: fix resolution of Cassandra main branch upon job determination in .build/run-ci\nDescription: We are currently resolving the job name by looking into \"build.xml\" and parsing the version from there. This works until we use \"-b\" option on \".build/run-ci\" script which says which branch we want to use. Basically, we could be on \"trunk\" while specifying a branch via \"-b\" which is in fact a branch from cassandra-5.0. In that case we would run CI against \"cassandra\" / trunk job instead of \"cassandra-5.0\" job.\nComments: - https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/20875/5.0 - I ve done {code} ~/dev/cassandra/cassandra-instaclustr/cassandra [trunk L|\u2691 171] 14:58 $ .build/run-ci -b mck/20875/5.0 -r https://github.com/thelastpickle/cassandra.git -p packaging Enter Jenkins password: Build number: 80 Jenkins UI at http://pre-ci.cassandra.apache.org/job/cassandra/80/pipeline-overview/ Waiting for build to complete\u2026 00:59 | {code} while I was on my \"trunk\" branch ( ~/dev/cassandra/cassandra-instaclustr/cassandra [trunk L ...) +1 - Committed https://github.com/apache/cassandra/commit/8ccdbb88ed87ec48b1033e07d8698271d58960a9", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20874", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20874"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: .build/run-ci does not work for trunk\nDescription: {{jenkins-deployment.yaml}} contains the definition of two jobs: {{cassandra}} and {{cassandra-5.0}}. This does not work when {{.build/run-ci}} determines job name: {code} def determine_job_name(cassandra_dir: Path) -> str: \"\"\" Determines the default Jenkins job name based on the Cassandra version. Separate jobs are required because Jenkinsfiles are baked into the job configuration. ref: .jenkins/k8s/jenkins-deployment.yaml JCasC.configScripts.test-job TODO: add new version each release branching \"\"\" with open(cassandra_dir / \"build.xml\", \"r\", encoding=\"utf-8\") as build_file: for line in build_file: if 'property' in line and 'name=\"base.version\"' in line: version = line.split('value=\"')[1].split('\"')[0] if version.startswith(\"5.0.\"): return \"cassandra-5.0\" return \"trunk\" {code} When on cassandra-5.0 branch, that is ok, but for trunk it is {{trunk}}, not {{cassandra}} so it fails. Ways to fix it: 1) fix jenkins deployment and probably redeploy, not good, a lot of work and unnecessary 2) fix {{.build/run-ci}} to return {{cassandra}} instead of {{trunk}} when on trunk.\nComments: - (2) it's a ninja fix. thanks for spotting this [~smiklosovic] (terrible testing of it from me) - but a real problem here is it is only looking at the local build.xml this isn't accurate when {{`-b`}} is specified", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20873", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20873"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Hanging streaming during repair\nDescription: Repair is not running but there are active streaming sessions. Logs will be posted as comments. * Node 1 (aaa.bbb.ccc.ddd) {noformat} $ nodetool netstats Mode: NORMAL Repair 8bcef5b0-78b7-11f0-bfc8-7392e70ddefe /www.xxx.yyy.zzz Receiving 105 files, 136257 bytes total. Already received 110 files (104.76%), 137014 bytes total (100.56%) $ nodetool repair_admin no sessions {noformat} * Node 2 (www.xxx.yyy.zzz) {noformat} $ nodetool netstats Mode: NORMAL Repair 8bcef5b0-78b7-11f0-bfc8-7392e70ddefe /aaa.bbb.ccc.ddd Sending 105 files, 136257 bytes total. Already sent 110 files (104.76%), 137014 bytes total (100.56%) $ nodetool repair_admin no sessions{noformat} * Node 3 (ccc.ddd.eee.fff) ** No streaming or repair running From the logs, we can gather the following streaming session states: Node1 view of the streaming session: * Node2 state: STREAMING * Node3 state: WAIT_COMPLETE Node2 view of the streaming session: * Node1 state: WAIT_COMPLETE Node3 no view of the streaming session\nComments: - *Node1 log while draining* * *node1 closing active connection with node2* {noformat} DEBUG [Messaging-EventLoop-3-1] 2025-08-28 18:49:40,117 StreamingMultiplexedChannel.java:514 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Closing stream connection channels on /www.xxx.yyy.zzz:1234 ERROR [Stream-Deserializer-/www.xxx.yyy.zzz:1234-46e95837] 2025-08-28 18:49:40,117 StreamSession.java:717 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Socket closed before session completion, peer www.xxx.yyy.zzz:1234 is probably down. java.nio.channels.ClosedChannelException: null at org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119) at org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:178) at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:49) at org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:58) {noformat} * *node1 closing streaming session with node 2* {noformat} DEBUG [NonPeriodicTasks:1] 2025-08-28 18:49:40,120 StreamSession.java:596 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Changing session state from STREAMING to FAILED INFO [NonPeriodicTasks:1] 2025-08-28 18:49:40,120 StreamResultFuture.java:201 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Session with /www.xxx.yyy.zzz:1234 is failed WARN [NonPeriodicTasks:1] 2025-08-28 18:49:40,120 StreamResultFuture.java:250 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Stream failed: Session peer /www.xxx.yyy.zzz:1234 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING DEBUG [NonPeriodicTasks:1] 2025-08-28 18:49:40,121 StreamSession.java:564 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Will close attached inbound {} and outbound {} channels {noformat} *node1 closing streaming with node 3 (even though node3 does not see this stream)* {noformat} DEBUG [NonPeriodicTasks:1] 2025-08-28 18:49:40,121 StreamSession.java:596 - [Stream #37f2d540-62b1-11f0-afef-1340d58ed089] Changing session state from WAIT_COMPLETE to FAILED INFO [NonPeriodicTasks:1] 2025-08-28 18:49:40,121 StreamResultFuture.java:201 - [Stream #37f2d540-62b1-11f0-afef-1340d58ed089] Session with /eee.fff.ggg.hhh:1234 is failed WARN [NonPeriodicTasks:1] 2025-08-28 18:49:40,121 StreamResultFuture.java:250 - [Stream #37f2d540-62b1-11f0-afef-1340d58ed089] Stream failed: Session peer /eee.fff.ggg.hhh:1234 Failed because there was an java.nio.channels.ClosedChannelException with state=WAIT_COMPLETE DEBUG [NonPeriodicTasks:1] 2025-08-28 18:49:40,121 StreamSession.java:564 - [Stream #37f2d540-62b1-11f0-afef-1340d58ed089] Will close attached inbound {eb5fe05c=org.apache.cassandra.streaming.async.NettyStreamingChannel@2cdae6ad} and outbound {ff4eaf92=org.apache.cassandra.streaming.async.NettyStreamingChannel@b66a5c0, 184ef185=org.apache.cassandra.streaming.async.NettyStreamingChannel@30b56f1, e16e60cd=org.apache.cassandra.streaming.async.NettyStreamingChannel@105f3d91, eb5fe05c=org.apache.cassandra.streaming.async.NettyStreamingChannel@2cdae6ad, 978d403e=org.apache.cassandra.streaming.async.NettyStreamingChannel@6bcb5eb9} channels {noformat} *node2 logs during node1 drain* {noformat} DEBUG [Messaging-EventLoop-3-6] 2025-08-28 18:49:40,119 StreamingMultiplexedChannel.java:514 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Closing stream connection channels on /aaa.bbb.ccc.ddd:1234 ERROR [Stream-Deserializer-/aaa.bbb.ccc.ddd:1234-d87ba681] 2025-08-28 18:49:40,119 StreamSession.java:717 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Socket closed before session completion, peer aaa.bbb.ccc.ddd:1234 is probably down. ERROR [Stream-Deserializer-/aaa.bbb.ccc.ddd:1234-23d3c0ac] 2025-08-28 18:49:40,119 StreamSession.java:717 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Socket closed before session completion, peer aaa.bbb.ccc.ddd:1234 is probably down. ERROR [Stream-Deserializer-/aaa.bbb.ccc.ddd:1234-678f744d] 2025-08-28 18:49:40,119 StreamSession.java:717 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Socket closed before session completion, peer aaa.bbb.ccc.ddd:1234 is probably down. ERROR [Stream-Deserializer-/aaa.bbb.ccc.ddd:1234-60cfa387] 2025-08-28 18:49:40,120 StreamSession.java:717 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Socket closed before session completion, peer aaa.bbb.ccc.ddd:1234 is probably down. DEBUG [NonPeriodicTasks:1] 2025-08-28 18:49:40,123 StreamSession.java:596 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Changing session state from WAIT_COMPLETE to FAILED INFO [NonPeriodicTasks:1] 2025-08-28 18:49:40,123 StreamResultFuture.java:201 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Session with /aaa.bbb.ccc.ddd:1234 is failed WARN [NonPeriodicTasks:1] 2025-08-28 18:49:40,123 StreamResultFuture.java:250 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Stream failed: DEBUG [NonPeriodicTasks:1] 2025-08-28 18:49:40,125 StreamSession.java:564 - [Stream #8bcef5b0-78b7-11f0-bfc8-7392e70ddefe] Will close attached inbound {} and outbound {} channels {noformat}", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20872", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20872"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make sure new TCM distributed keyspace is added to system_schema on upgrade\nDescription: The new system_cluster_metadata keyspace is added to system_schema when running {{nodetool cms initialize}} - it should be added directly when the upgraded node starts\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20871", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20871"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: ArrayIndexOutOfBoundsException with repaired data tracking and counters\nDescription: {code} INFO [node1_ReadStage-1] 2025-08-29T07:30:04,087 SubstituteLogger.java:222 - ERROR 09:30:04,085 Exception in thread Thread[node1_ReadStage-1,5,SharedPool] java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 0 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:3132) at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:168) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:150) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829) Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 0 at org.apache.cassandra.utils.ByteArrayUtil.getShort(ByteArrayUtil.java:112) at org.apache.cassandra.db.marshal.ByteArrayAccessor.getShort(ByteArrayAccessor.java:190) at org.apache.cassandra.db.marshal.ByteArrayAccessor.getShort(ByteArrayAccessor.java:41) at org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:175) at org.apache.cassandra.db.context.CounterContext.hasLegacyShards(CounterContext.java:597) at org.apache.cassandra.db.Digest$1.updateWithCounterContext(Digest.java:77) at org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:150) at org.apache.cassandra.db.rows.ColumnData.digest(ColumnData.java:278) {code}\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20870", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20870"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Expose StorageService.dropPreparedStatements via JMX\nDescription: Looks like this method was added to StorageService but it is not available via StorageServiceMBean. This ticket is to make that.\nComments: - on 5.0 this was just exposing the method StorageService.dropPreparedStatements on StorageServiceMBean on trunk StorageService.dropPreparedStatements was missing, so this was added which is essentially a proxy to {{QueryProcessor.instance.clearPreparedStatements(memoryOnly)}} - +1 - +1 looks reasonable to expose to me.", "output": "Status: Resolved\nPriority: Low"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20869", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20869"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix issue when reconfiguring cms with paxos repair disabled\nDescription: We currently throw exception if {{nodetool cms reconfigure ...}} is run when paxos repair is disabled\nComments: No comments.", "output": "Status: Patch Available\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20868", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20868"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Increase memory for javadoc building\nDescription: I was about to stage 4.1.10 and I failed to do so as I was not able to pass \"ant publish\". It will eventually try to build javadocs and this step behaves completely randomly, sometimes it passes, sometimes it does not. What I suspect is happening is that, based on empirical testing I conducted, more memory we assign to javadoc generation, more successful it is / probability of that failing is lower. For now, it is 256m, I will increase it to 1024m. Tricky part about releasing is that changes have to be there so I need to commit this before deploying. If it does not work, we will just skip javadoc completely.\nComments: No comments.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20867", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20867"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: EKS docs for setting up a ci-cassandra jenkins helm clone\nDescription: EKS docs to put in .jenkins/k8s/ that describe how pre-ci.cassandra.apache.org was setup, and how to repeat in any EKS.\nComments: - https://github.com/apache/cassandra/pull/4347", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20866", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20866"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add equals/hashCode to RangeTermTree$Term to make sure it plays nicely with IntervalNode construction\nDescription: I've been seeing local test failures in {{{}StorageAttachedIndexDDLTest#shouldCreateIndexFilesAfterMultipleConcurrentIndexCreation(){}}}, and it looks like the root cause of this is that {{RangeTermTree$Term}} doesn't implement {{{}equals(){}}}, so the invariant checks in {{IntervalTree$IntervalNode}} aren't reliable. {noformat} Caused by: java.lang.IllegalStateException: null at com.google.common.base.Preconditions.checkState(Preconditions.java:496) at org.apache.cassandra.utils.IntervalTree$IntervalNode.<init>(IntervalTree.java:693) at org.apache.cassandra.utils.IntervalTree.<init>(IntervalTree.java:97) at org.apache.cassandra.utils.IntervalTree.build(IntervalTree.java:186) at org.apache.cassandra.index.sai.view.RangeTermTree$Builder.build(RangeTermTree.java:97) at org.apache.cassandra.index.sai.view.View.<init>(View.java:59) at org.apache.cassandra.index.sai.view.IndexViewManager.update(IndexViewManager.java:107) at org.apache.cassandra.index.sai.StorageAttachedIndex.onSSTableChanged(StorageAttachedIndex.java:625) at org.apache.cassandra.index.sai.StorageAttachedIndexGroup.onSSTableChanged(StorageAttachedIndexGroup.java:335) at org.apache.cassandra.index.sai.StorageAttachedIndexBuilder.completeSSTable(StorageAttachedIndexBuilder.java:313) at org.apache.cassandra.index.sai.StorageAttachedIndexBuilder.indexSSTable(StorageAttachedIndexBuilder.java:195) at org.apache.cassandra.index.sai.StorageAttachedIndexBuilder.build(StorageAttachedIndexBuilder.java:118) at org.apache.cassandra.db.compaction.CompactionManager$13.run(CompactionManager.java:2035) {noformat}\nComments: - As expected, nothing new in the CI run, but posted it for posterity.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20865", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20865"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: WARNING: cqlsh was built against 4.1.4, but this server is 4.1.8. All features may not work!\nDescription: There's an issue where we're getting a WARNING when logging into cqlsh: {{WARNING: cqlsh was built against 4.1.4, but this server is 4.1.8. All features may not work!}} We verified that the cqlsh version is from 4.1.8 and that is the only cqlsh installed. We used an rpm installation of 4.1.8 with yum. We don\u2019t get the warning in Oracle Linux 7, but do get it in Oracle Linux 8. I did notice that the difference is: OL7 -> python2 OL8 -> pytthon3 {code:java} {{Output:[root ~]# cqlsh -u cassandra $HOSTNAME}} {{Password: WARNING: cqlsh was built against 4.1.4, but this server is 4.1.8. All features may not work!}} {{Connected to sesssrv_null at }} {{[cqlsh 6.1.0 | Cassandra 4.1.8 | CQL spec 3.4.6 | Native protocol v5] Use HELP for help.}} {{cassandra@cqlsh> cassandra@cqlsh> SELECT release_version FROM system.local;}} {{ release_version ----------------- 4.1.8 (1 rows)}} {{[SKIPPED] cassandra-tools-4.1.8-1.noarch.rpm: Already downloaded}} {{Running transaction check Transaction check succeeded. Running transaction test}} {{Transaction test succeeded.}} {{Running transaction Preparing : }} {{ 1/1 Installing : cassandra-tools-4.1.8-1.noarch }} {{ 1/1 Verifying : cassandra-tools-4.1.8-1.noarch }} {{ 1/1 Installed: cassandra-tools-4.1.8-1.noarch}} {{Complete!}} {{}} {{[root ~]# rpm -qa |grep cassandra cassandra-tools-4.1.8-1.noarch}} {{cassandra-4.1.8-1.noarch}} {code} My assumption is that cqlsh for python2 was built in the 4.1.8 release, but the cqlsh for python3 was not.\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20864", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20864"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Expose Prepared Statement Cache Size Metric (in bytes)\nDescription: Currently, Cassandra exposes the count of prepared statements through a metric, PreparedStatementsCount. However, the maximum cache size is configured in terms of bytes through the configuration prepared_statements_cache_size, creating a mismatch between the exposed metrics and the configuration units. This discrepancy makes it difficult for operators to anticipate when the cache is approaching its configured limit. The scope of this ticket is to add a new metric, PreparedStatementsCacheSize, which will expose the size of the cache in bytes to improve the observability of the size of the prepared statement cache.\nComments: - +1, pending CI https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-5/404/ - +1, and the ci seems to be all right. Start to commit the pr. - Thank you both for the fast reviews!", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20863", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20863"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Missing authorizer and role_manager configs in system_views.settings\nDescription: Authorizer role_manager configs are missing in system_views.settings after the changes in CASSANDRA-19946 were merged. Example query on a 5.x cluster: cqlsh> select * from system_views.settings where name = 'authorizer'; name | value ------+------- cqlsh> select * from system_views.settings where name = 'role_manager'; name | value ------+------- (0 rows)\nComments: - In CASSANDRA-19946 all auth* related classes allow configuration parameters. This was done by changing the underlying mechanism that loads those implementations using {{{}ParameterizedClass{}}}. This impacted the way those settings were exposed in {{system_views.settings}} virtual table. For instance, while in Cassandra 5.0.1 we could check for the configured role_manager by running: {{select * from system_views.settings where name = 'role_manager';}} in Cassandra 5.0.2 and later we need to use \\{{select * from system_views.settings where name = 'role_manager.class_name'; }}. The same happens when checking for the used authorizer. In Cassandra 5.0.1, we could use {{select * from system_views.settings where name = 'authorizer';}} while in Cassandra 5.0.2 we need to use {{select * from system_views.settings where name = 'authorizer.class_name'; }}(notice the {{.class_name}} after the property name). Note that in Cassandra 5.0.1, to query for the authenticator one already had to use {{query select * from system_views.settings where name = 'authenticator.class_name';}} as querying for {{query select * from system_views.settings where name = 'authenticator';}} would yield no results. While we were careful to ensure {{cassandra.yaml}} configuration would be backwards compatible, allowing to configure either: {code:java} authorizer: AllowAllAuthorizer{code} or {code:java} authorizer: class_name: AllowAllAuthorizer {code} we failed to notice the impact of {{system_views.settings}} virtual table. This could be seen a regression between 5.0.1 and 5.0.2. [~skrlaha] would adapting the queries be a solution for you? [~stefan.miklosovic] as we worked together on this, do you think it makes sense / be possible to modify the {{system_views.settings}} to continue reporting {{{}authenticator{}}}, {{{}role_manager{}}}, and {{{}authorizer{}}}? - [~tiagomlalves] We will prefer if the changes are backwards compatible. Please let us know if it will be possible to create a fix so that the changes in CASSANDRA-19946 are backwards compatible. - [~tiagomlalves], I can pick this up and get a fix ready to make settings table backwards compatible. - [~shrutips] I've reviewed and approved the PR. I've tested locally the changes and it works as intended. Thank you for the contribution. - I am OK to add them back to 5.0 but I am not sure we should continue to have two ways in trunk, there should be just one approach, the newest one. The patch is for trunk. I think it should be for 5.0 and we should keep trunk untouched. Thoughts? What we can do in trunk is that we might make it conditional by a system property set by default to false (do not show old names) and a user can turn that property on so even legacy names would be shown in trunk (on top of the new ones). - Well, looking into this I think that will be more involved. Let's just ship what we have in the patch as is just to bring them back and we can contemplate about any improvements later. - {quote} I am OK to add them back to 5.0 but I am not sure we should continue to have two ways in trunk, there should be just one approach, the newest one. The patch is for trunk. I think it should be for 5.0 and we should keep trunk untouched. {quote} I totally agree that this patch should go for 5.0. I forgot about asking about the deprecation policy for C* versions. Given trunk is the upcoming 6.0 version this warning should no longer be needed. Is that right? {quote}Well, looking into this I think that will be more involved. Let's just ship what we have in the patch as is just to bring them back and we can contemplate about any improvements later {quote} What do you mean? - Let's just apply this everywhere, 5.0 and trunk as well, and we will decide what to do with it in trunk in another ticket. There is not only this one added among backward compatible / deprecated and I am not sure we can just remove it all in trunk easily. - [~shrutips] could you please open PR against 5.0 and make sure it is aligned with 5.0? - Thank you for the review [~tiagomlalves]. [~smiklosovic], makes sense, I just changed it to 5.0. - +1 thanks for the fix. [~shrutips] can you open the PR against trunk as well and share CI results for both branches - I opened the PR against trunk here- https://github.com/apache/cassandra/pull/4469 I've attached the CI results for 5.0, will share the CI results against trunk when the job finishes. - +1 CI: http://pre-ci.cassandra.apache.org/job/cassandra/155/pipeline-overview/ - trunk https://pre-ci.cassandra.apache.org/job/cassandra/160/#showFailuresLink - I will ship this no worries.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20862", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20862"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Test failure: org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest on seed 0xca7dc33660ae3b8a\nDescription: https://ci-cassandra.apache.org/job/Cassandra-trunk/2250/testReport/junit/org.apache.cassandra.simulator.test/ShortAccordSimulationTest/Tests___simulator_dtest_jdk11___simulationTest__jdk11_x86_64/ {code} java.lang.RuntimeException: Simulation failed with exit code: 1 at org.apache.cassandra.simulator.SimulatorUtils.executeWithExceptionThrowing(SimulatorUtils.java:110) at org.apache.cassandra.simulator.paxos.AccordSimulationRunner.executeWithExceptionThrowing(AccordSimulationRunner.java:104) at org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest(ShortAccordSimulationTest.java:97) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Caused by: org.apache.cassandra.simulator.SimulationException: Failed on seed 0xca7dc33660ae3b8a Caused by: java.lang.AssertionError: History violations detected at org.apache.cassandra.simulator.paxos.PaxosSimulation.logAndThrow(PaxosSimulation.java:341) at org.apache.cassandra.simulator.paxos.PaxosSimulation.isDone(PaxosSimulation.java:304) at org.apache.cassandra.simulator.paxos.PaxosSimulation$2.hasNext(PaxosSimulation.java:275) at org.apache.cassandra.simulator.paxos.PaxosSimulation.run(PaxosSimulation.java:250) at org.apache.cassandra.simulator.paxos.AbstractPairOfSequencesPaxosSimulation.run(AbstractPairOfSequencesPaxosSimulation.java:301) at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation.run(PairOfSequencesAccordSimulation.java:64) at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:414) at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:63) at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:54) at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:394) at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:54) at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:359) at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:54) at picocli.CommandLine.executeUserObject(CommandLine.java:2045) at picocli.CommandLine.access$1500(CommandLine.java:148) at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2469) at picocli.CommandLine$RunLast.handle(CommandLine.java:2461) at picocli.CommandLine$RunLast.handle(CommandLine.java:2423) at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2277) at picocli.CommandLine$RunLast.execute(CommandLine.java:2425) at org.apache.cassandra.simulator.SimulatorUtils.lambda$prepareRunner$11(SimulatorUtils.java:92) at picocli.CommandLine.execute(CommandLine.java:2174) at org.apache.cassandra.simulator.SimulatorUtils.executeWithExceptionThrowing(SimulatorUtils.java:105) at org.apache.cassandra.simulator.paxos.AccordSimulationRunner.executeWithExceptionThrowing(AccordSimulationRunner.java:104) at org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest(ShortAccordSimulationTest.java:97) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Suppressed: org.apache.cassandra.simulator.paxos.HistoryViolation: Unknown write step on key 0 with value 341 is reachable from its happens-before relations at accord.verify.StrictSerializabilityVerifier$Step.receiveUnknownStepPredecessor(StrictSerializabilityVerifier.java:451) at accord.verify.StrictSerializabilityVerifier$Step.receiveKnowledgePhasedPredecessors(StrictSerializabilityVerifier.java:443) at accord.verify.StrictSerializabilityVerifier$Step.receiveUnknownStepPredecessor(StrictSerializabilityVerifier.java:459) at accord.verify.StrictSerializabilityVerifier$Step.receiveKnowledgePhasedPredecessors(StrictSerializabilityVerifier.java:438) at accord.verify.StrictSerializabilityVerifier$FutureWrites.receiveKnowledgePhasedPredecessors(StrictSerializabilityVerifier.java:536) at accord.verify.StrictSerializabilityVerifier$Register.propagateToDirectSuccessor(StrictSerializabilityVerifier.java:702) at accord.verify.StrictSerializabilityVerifier$Register.onChange(StrictSerializabilityVerifier.java:732) at accord.verify.StrictSerializabilityVerifier$UnknownStepHolder.run(StrictSerializabilityVerifier.java:214) at accord.verify.StrictSerializabilityVerifier$Register.onChange(StrictSerializabilityVerifier.java:738) at accord.verify.StrictSerializabilityVerifier$UnknownStepHolder.discoveredStepIndex(StrictSerializabilityVerifier.java:197) at accord.verify.StrictSerializabilityVerifier$FutureWrites.newSequence(StrictSerializabilityVerifier.java:510) at accord.verify.StrictSerializabilityVerifier$Register.updateSequence(StrictSerializabilityVerifier.java:635) at accord.verify.StrictSerializabilityVerifier.apply(StrictSerializabilityVerifier.java:870) at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.lambda$close$0(StrictSerializabilityValidator.java:66) at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.convertHistoryViolation(StrictSerializabilityValidator.java:89) at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.close(StrictSerializabilityValidator.java:66) at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.verify(PairOfSequencesAccordSimulation.java:232) at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:161) at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.accept(PairOfSequencesAccordSimulation.java:185) at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:109) at org.apache.cassandra.simulator.systems.SimulatedActionCallable$1.run(SimulatedActionCallable.java:47) at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:284) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829) Suppressed: java.lang.NullPointerException at org.apache.cassandra.simulator.paxos.PaxosTopologyChangeVerifier.afterInternal(PaxosTopologyChangeVerifier.java:67) at org.apache.cassandra.simulator.paxos.PaxosTopologyChangeVerifier.after(PaxosTopologyChangeVerifier.java:61) at org.apache.cassandra.simulator.cluster.OnClusterChangeTopology.accept(OnClusterChangeTopology.java:69) at org.apache.cassandra.simulator.cluster.OnClusterChangeTopology.accept(OnClusterChangeTopology.java:32) at org.apache.cassandra.simulator.ActionListener$3.transitivelyAfter(ActionListener.java:90) at org.apache.cassandra.simulator.utils.CompactLists.safeForEach(CompactLists.java:116) at org.apache.cassandra.simulator.Action.transitivelyFinished(Action.java:771) at org.apache.cassandra.simulator.Action.transitivelyFinished(Action.java:760) at org.apache.cassandra.simulator.Action.finishedSelf(Action.java:746) at org.apache.cassandra.simulator.Action.invalidate(Action.java:549) at org.apache.cassandra.simulator.Action.invalidate(Action.java:523) at org.apache.cassandra.simulator.utils.SafeCollections.safeForEach(SafeCollections.java:35) at org.apache.cassandra.simulator.ActionSchedule.close(ActionSchedule.java:454) at org.apache.cassandra.simulator.paxos.PaxosSimulation$2.close(PaxosSimulation.java:294) at org.apache.cassandra.simulator.paxos.PaxosSimulation.run(PaxosSimulation.java:247) {code}\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20861", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20861"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Test timeout: org.apache.cassandra.simulator.test.AccordHarrySimulationTest\nDescription: AccordHarrySimulationTest fails periodically on different CIs due to timeouts for a test execution or test split execution (1h). Examples for the failures: * https://ci-cassandra.apache.org/job/Cassandra-trunk/2239/testReport/junit/org.apache.cassandra.simulator.test/AccordHarrySimulationTest/Tests___simulator_dtest_jdk11___test_cassandra_testtag_IS_UNDEFINED/ * https://pre-ci.cassandra.apache.org/job/cassandra/68/pipeline-overview/?selected-node=534 It is paintful because it can impact an overall CI run execution - by causing a timeout for a test split execution, so the whole test CI run is aborted as a result. Example: https://pre-ci.cassandra.apache.org/job/cassandra/68/pipeline-overview/?selected-node=534\nComments: - I've noticed that it is possible to configure a time here: org.apache.cassandra.simulator.ClusterSimulation.Builder#secondsToSimulate(int) maybe it can be an option to make the execution time for the test more predictable..", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20860", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20860"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Failed to instantiate type org.apache.cassandra.simulator.logging.RunStartDefiner\nDescription: org.apache.cassandra.simulator.test.AccordHarrySimulationTest print errors like this: {code:java} 23:42:59 [junit-timeout] 22:42:53,609 |-ERROR in ch.qos.logback.core.model.processor.DefaultProcessor@2d140a7 - Failed to traverse model define ch.qos.logback.core.model.processor.ModelHandlerException: ch.qos.logback.core.util.DynamicClassLoadingException: Failed to instantiate type org.apache.cassandra.simulator.logging.RunStartDefiner 23:42:59 [junit-timeout] at ch.qos.logback.core.model.processor.ModelHandlerException: ch.qos.logback.core.util.DynamicClassLoadingException: Failed to instantiate type org.apache.cassandra.simulator.logging.RunStartDefiner 23:42:59 [junit-timeout] at at ch.qos.logback.core.model.processor.DefineModelHandler.handle(DefineModelHandler.java:93) 23:42:59 [junit-timeout] at at ch.qos.logback.core.model.processor.DefaultProcessor.mainTraverse(DefaultProcessor.java:203) 23:42:59 [junit-timeout] at at ch.qos.logback.core.model.processor.DefaultProcessor.mainTraverse(DefaultProcessor.java:211) 23:42:59 [junit-timeout] at at ch.qos.logback.core.model.processor.DefaultProcessor.process(DefaultProcessor.java:104) 23:42:59 [junit-timeout] at at ch.qos.logback.core.joran.GenericXMLConfigurator.processModel(GenericXMLConfigurator.java:222) 23:42:59 [junit-timeout] at at ch.qos.logback.core.joran.GenericXMLConfigurator.doConfigure(GenericXMLConfigurator.java:178) 23:42:59 [junit-timeout] at at ch.qos.logback.core.joran.GenericXMLConfigurator.doConfigure(GenericXMLConfigurator.java:123) 23:42:59 [junit-timeout] at at ch.qos.logback.core.joran.GenericXMLConfigurator.doConfigure(GenericXMLConfigurator.java:66) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.util.DefaultJoranConfigurator.configureByResource(DefaultJoranConfigurator.java:68) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.util.DefaultJoranConfigurator.configure(DefaultJoranConfigurator.java:35) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.util.ContextInitializer.invokeConfigure(ContextInitializer.java:142) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:101) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:67) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.spi.LogbackServiceProvider.initializeLoggerContext(LogbackServiceProvider.java:51) 23:42:59 [junit-timeout] at at ch.qos.logback.classic.spi.LogbackServiceProvider.initialize(LogbackServiceProvider.java:44) 23:42:59 [junit-timeout] at at org.slf4j.LoggerFactory.bind(LoggerFactory.java:201) 23:42:59 [junit-timeout] at at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:187) 23:42:59 [junit-timeout] at at org.slf4j.LoggerFactory.getProvider(LoggerFactory.java:511) 23:42:59 [junit-timeout] at at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:497) 23:42:59 [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:446) 23:42:59 [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) 23:42:59 [junit-timeout] at at org.apache.cassandra.simulator.systems.InterceptingGlobalMethods.<clinit>(InterceptingGlobalMethods.java:48) 23:42:59 [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:390) 23:42:59 [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:363) 23:42:59 [junit-timeout] at at org.apache.cassandra.simulator.test.AccordJournalSimulationTest.simpleRWTest(AccordJournalSimulationTest.java:60) 23:42:59 [junit-timeout] at at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 23:42:59 [junit-timeout] at at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 23:42:59 [junit-timeout] at at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 23:42:59 [junit-timeout] at at java.base/java.lang.reflect.Method.invoke(Method.java:566) 23:42:59 [junit-timeout] at at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 23:42:59 [junit-timeout] at at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 23:42:59 [junit-timeout] at at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 23:42:59 [junit-timeout] at at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 23:42:59 [junit-timeout] at at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 23:42:59 [junit-timeout] at at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 23:42:59 [junit-timeout] at at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 23:42:59 [junit-timeout] at at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 23:42:59 [junit-timeout] at at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38) 23:42:59 [junit-timeout] at at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:534) 23:42:59 [junit-timeout] at at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196) 23:42:59 [junit-timeout] at at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041) 23:42:59 [junit-timeout] Caused by: ch.qos.logback.core.util.DynamicClassLoadingException: Failed to instantiate type org.apache.cassandra.simulator.logging.RunStartDefiner 23:42:59 [junit-timeout] at at ch.qos.logback.core.util.OptionHelper.instantiateByClassNameAndParameter(OptionHelper.java:69) 23:42:59 [junit-timeout] at at ch.qos.logback.core.util.OptionHelper.instantiateByClassName(OptionHelper.java:44) 23:42:59 [junit-timeout] at at ch.qos.logback.core.util.OptionHelper.instantiateByClassName(OptionHelper.java:33) 23:42:59 [junit-timeout] at at ch.qos.logback.core.model.processor.DefineModelHandler.handle(DefineModelHandler.java:87) 23:42:59 [junit-timeout] at ... 46 common frames omitted 23:42:59 [junit-timeout] Caused by: java.lang.ExceptionInInitializerError 23:42:59 [junit-timeout] at at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 23:42:59 [junit-timeout] at at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 23:42:59 [junit-timeout] at at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 23:42:59 [junit-timeout] at at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) 23:42:59 [junit-timeout] at at ch.qos.logback.core.util.OptionHelper.instantiateByClassNameAndParameter(OptionHelper.java:61) 23:42:59 [junit-timeout] at ... 49 common frames omitted 23:42:59 [junit-timeout] Caused by: java.lang.IllegalStateException 23:42:59 [junit-timeout] at at accord.utils.Invariants.createIllegalState(Invariants.java:77) 23:42:59 [junit-timeout] at at accord.utils.Invariants.illegalState(Invariants.java:82) 23:42:59 [junit-timeout] at at accord.utils.Invariants.illegalState(Invariants.java:92) 23:42:59 [junit-timeout] at at accord.utils.Invariants.require(Invariants.java:224) 23:42:59 [junit-timeout] at at org.apache.cassandra.simulator.logging.RunStartDefiner.<clinit>(RunStartDefiner.java:29) 23:42:59 [junit-timeout] at ... 54 common frames omitted {code} If I got correctly it is caused by initializing a logger here: org.apache.cassandra.simulator.systems.InterceptingGlobalMethods {code} private static final Logger logger = LoggerFactory.getLogger(InterceptingGlobalMethods.class); {code} The logger is not actually used, so I suppose we can just remove the line. Based on the changes in a similar place in https://github.com/apache/cassandra/commit/e6cf2132ab33b2d6e68e79cb2e6d5c3a31103088#diff-d7c7bbcde5f2ce09a2113d707d824661d635ca7fb5dfb0c44cc9ce597e20ed1cR98 logger usage is not expected during an initialisation of simulation framework.\nComments: - Other places with the same issue: {code} [2025-08-25T19:37:10.865Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T19:37:10.865Z] [junit-timeout] at at org.apache.cassandra.simulator.test.HarrySimulatorTest.<clinit>(HarrySimulatorTest.java:187) {code} {code} [2025-08-25T19:59:42.642Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T19:59:42.642Z] [junit-timeout] at at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:117) [2025-08-25T19:59:42.642Z] [junit-timeout] at at org.apache.cassandra.distributed.impl.AbstractCluster.<clinit>(AbstractCluster.java:142) [2025-08-25T19:59:42.642Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:378) [2025-08-25T19:59:42.642Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:363) [2025-08-25T19:59:42.642Z] [junit-timeout] at at org.apache.cassandra.simulator.test.AccordJournalSimulationTest.simpleRWTest(AccordJournalSimulationTest.java:60) {code} {code} [2025-08-25T19:59:49.414Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T19:59:49.414Z] [junit-timeout] at at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:117) [2025-08-25T19:59:49.414Z] [junit-timeout] at at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:180) [2025-08-25T19:59:49.414Z] [junit-timeout] at at org.apache.cassandra.simulator.test.AccordJournalSimulationTest.lambda$simpleRWTest$81c80a4a$1(AccordJournalSimulationTest.java:63) {code} {code} [2025-08-25T20:02:38.124Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T20:02:38.124Z] [junit-timeout] at at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:117) [2025-08-25T20:02:38.124Z] [junit-timeout] at at org.apache.cassandra.distributed.impl.AbstractCluster.<clinit>(AbstractCluster.java:142) [2025-08-25T20:02:38.124Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:378) [2025-08-25T20:02:38.124Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:363) [2025-08-25T20:02:38.124Z] [junit-timeout] at at org.apache.cassandra.simulator.test.MonitorMethodTransformerTest.testSynchronizedMethod(MonitorMethodTransformerTest.java:61) {code} {code} [2025-08-25T20:02:44.893Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T20:02:44.893Z] [junit-timeout] at at org.apache.cassandra.simulator.systems.InterceptedWait$InterceptedConditionWait.<clinit>(InterceptedWait.java:118) [2025-08-25T20:02:44.893Z] [junit-timeout] at at org.apache.cassandra.simulator.systems.InterceptingMonitors.preMonitorEnter(InterceptingMonitors.java:675) [2025-08-25T20:02:44.893Z] [junit-timeout] at at org.apache.cassandra.simulator.systems.InterceptorOfSystemMethods$Global.preMonitorEnter(InterceptorOfSystemMethods.java:124) [2025-08-25T20:02:44.893Z] [junit-timeout] at at org.apache.cassandra.simulator.test.ClassWithSynchronizedMethods.synchronizedMethodWithParams(ClassWithSynchronizedMethods.java) [2025-08-25T20:02:44.893Z] [junit-timeout] at at org.apache.cassandra.simulator.test.MonitorMethodTransformerTest.lambda$testSynchronizedMethod$6ec7df81$1(MonitorMethodTransformerTest.java:56) {code} {code} [2025-08-25T20:02:47.831Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T20:02:47.831Z] [junit-timeout] at at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:117) [2025-08-25T20:02:47.831Z] [junit-timeout] at at org.apache.cassandra.distributed.impl.AbstractCluster.<clinit>(AbstractCluster.java:142) [2025-08-25T20:02:47.831Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:378) [2025-08-25T20:02:47.831Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SemaphoreTest.semaphoreAcquireUntilTest(SemaphoreTest.java:40) {code} {code} [2025-08-25T20:09:25.657Z] [junit-timeout] at at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472) [2025-08-25T20:09:25.657Z] [junit-timeout] at at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:117) [2025-08-25T20:09:25.657Z] [junit-timeout] at at org.apache.cassandra.distributed.impl.AbstractCluster.<clinit>(AbstractCluster.java:142) [2025-08-25T20:09:25.657Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:378) [2025-08-25T20:09:25.657Z] [junit-timeout] at at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:363) [2025-08-25T20:09:25.657Z] [junit-timeout] at at org.apache.cassandra.simulator.test.TrivialSimulationTest.identityHashMapTest(TrivialSimulationTest.java:40) {code} - Hm, we have at least the following tests where the assert is not true: HarrySimulatorTest AccordJournalSimulationTest MonitorMethodTransformerTest SemaphoreTest TrivialSimulationTest [~dcapwell] [~benedict] could you please help to clarify do we really need the require in RunStartDefiner? {code} public class RunStartDefiner extends PropertyDefinerBase { static { Invariants.require(CassandraRelevantProperties.SIMULATOR_STARTED.getString() != null); } {code} - I think the reason is that simulator logs the cluster logs based off the seed, so if we don't know the seed when setting up logging we have a logging bug. Does this actually fail the test, or just breaks the seed separation of logging? I thought the last time I touched simulator it was just spam and doesn't fail the test. In CI I don't think the seed log isolation really matters, feels more like local running to me. [~benedict] does it make sense to you to drop? - {quote}I think the reason is that simulator logs the cluster logs based off the seed, so if we don't know the seed when setting up logging we have a logging bug. {quote} Actually we have 2 property definers: * org.apache.cassandra.simulator.logging.RunStartDefiner - provides simulation start time (as currentTime in seconds), has the require check in static block * org.apache.cassandra.simulator.logging.SeedDefiner - provides seed, it does have a require check {quote}Does this actually fail the test, or just breaks the seed separation of logging? I thought the last time I touched simulator it was just spam and doesn't fail the test. {quote} I think it does not fail the tests but it pollutes logs and maybe (I've not verified) slow down their execution. The definers are used in test/conf/logback-simulator.xml {code:xml} <define name=\"run_start\" class=\"org.apache.cassandra.simulator.logging.RunStartDefiner\" scope=\"system\"/> <define name=\"run_seed\" class=\"org.apache.cassandra.simulator.logging.SeedDefiner\" scope=\"system\"/> {code} to construct log file paths: {code:java} <appender name=\"HISTORYLOG\" class=\"ch.qos.logback.core.FileAppender\"> <file>./build/test/logs/simulator/${run_start}-${run_seed}/history.log</file> {code} {code:java} <appender name=\"INSTANCEFILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>./build/test/logs/simulator/${run_start}-${run_seed}/cluster-${cluster_id}/${instance_id}/system.log</file> {code}", "output": "Status: In Progress\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20858", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20858"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add virtual table to expose last N uncaught exceptions\nDescription: There's a metric {{StorageMetrics.uncaughtExceptions}} that tracks uncaught exception count. Let's add a {{system_views.exceptions}} table with the following information: * ExceptionClass * Count * Last Message * Last StackTrace * Last Occurence Time This will give more visibility into uncaught exceptions allowing operators to easily find unexpected issues that can be hidden in logs.\nComments: - This is just too useful to not introduce everywhere we have system views tables at. - {quote}Let's add a system_views.exceptions table with the following information: - ExceptionClass - Count - Last Message - Last StackTrace - Last Occurence Time{quote} I don't see why not keep the last message/timestamp/stack trace for the last N exceptions. We could perhaps keep a global count per exception as static field that is not truncated. Perhaps we could have a schema like? not sure if something is missing {noformat} CREATE TABLE exceptions ( class text, count static int, time timestamp, message text, <-- not sure if this is needed, if the message is included in the stack trace stack_trace text, primary (class, time) ) {noformat} Also we would need a configuration of max stack traces to keep, something around 100 by default ? and this would be FIFO so new exceptions would truncate the oldest exceptions in memory so that only that amount is kept. - yeah, already working on it - [~paulo] [https://github.com/apache/cassandra/pull/4355] I do not know how to invoke uncaught exception so I do not know how to test it, heh. - As an option we can use Byteman for this purpose to inject an exception (if we want to emulate an end-to-end case), there are several tests which use this library to inject some faulty behaviour... Another simpler option is to invoke org.apache.cassandra.utils.JVMStabilityInspector#uncaughtException directly in the test. - [~paulo] do you want keep the track of exceptions which are of same class? If we get 100 FsErrors, you want to see them all? What I am currently doing is that I am \"merging them\" into one and just increasing the counter. That is how I understand what you suggested when this ticket was created but then you are also proposing the static column etc. I think that if we want to have it more unique while still deduplicating, we should do that based on where that exception was thrown (exact line number), that is possible to get from the stacktrace. So if you had IllegalStateException thrown at place A in the code and place B in the code, while it is still same exception type, they differ on the place in the code they were thrown at and they might be completely different, logically wise, from each other. With your suggestion of {{primary key (class, time)}} primary key, you would just see them ordered by time, but you would not easily recognize one from another if two of the same type are thrown at different places. What I currently have is that one would \"shadow\" another one, just with the counter increased. This is what I am proposing instead {code:java} CREATE TABLE exceptions ( exception_class text, thrown_at text, count int, last_occurence timestamp, last_message text last_stacktrace list<text>, primary (exception_class, thrown_at, last_message) ) {code} We do not need to have \"count\" static. We will just increase it as we gather same {{exception_class}} and {{thrown_at}}. You can do {code} select * from exceptions where exception_class = 'IllegalStateException' {code} {code} select * from exceptions where exception_class = 'IllegalStateException' and thrown_at = 'SomeClass.java:55' {code} {code} select * from exceptions where exception_class = 'IllegalStateException' and thrown_at = 'SomeClass.java:55' and last_message = 'Something went wrong'; {code} - > This is what I am proposing instead This seems reasonable and more useful than what I proposed. - +1 - https://pre-ci.cassandra.apache.org/job/cassandra/99/#showFailuresLink - great contribution thanks!", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20857", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20857"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add support for BEGIN TRANSACTION to allow mutations that touch multiple partitions\nDescription: UPDATE and DELETE are both able to update multiple partitions, but these queries would fail when using in BEGIN TRANSACTION as it assumed single partition only. To better support all CQL operations, need to support multiple partitions within a single mutation.\nComments: - +1 in GH", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20856", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20856"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: system_views.settings exposes encryption and TDE passwords in plaintext over CQL\nDescription: Selecting from the virtual table {{system_views.settings}} shows the property values cassandra.yaml faithfully, including the ones that contain passwords. Any user with {{SELECT}} on {{system_views.settings}} can read these secrets. {code:java} cqlsh:system_views> select * from settings where name = 'client_encryption_options.truststore_password'; name | value -----------------------------------------------+---------- client_encryption_options.truststore_password | changeit (1 rows) cqlsh:system_views> select * from settings where name = 'client_encryption_options.keystore_password'; name | value ---------------------------------------------+---------- client_encryption_options.keystore_password | changeit (1 rows) cqlsh:system_views> select * from settings where name = 'server_encryption_options.truststore_password'; name | value -----------------------------------------------+---------- server_encryption_options.truststore_password | changeit (1 rows) cqlsh:system_views> select * from settings where name = 'server_encryption_options.keystore_password'; name | value ---------------------------------------------+---------- server_encryption_options.keystore_password | changeit (1 rows) cqlsh:system_views> select * from system_views.settings where name = 'transparent_data_encryption_options.key_provider.parameters'; name | value -------------------------------------------------------------+-------------------------------------------------------------------------------------------------- transparent_data_encryption_options.key_provider.parameters | {keystore_password=cassandra, keystore=conf/.keystore, store_type=JCEKS, key_password=cassandra} {code} Passwords and secrets should be handled as a special case and not exposed in plain text in any of the virtual tables. Observed in 4.1.x and 5.0.x\nComments: - Good catch; we should completely obscure encryption related settings in the system_view.settings like we do in the logs: {noformat} .. client_encryption_options=<REDACTED> .. {noformat} https://github.com/apache/cassandra/blob/12dcb130f783e7cc16401f2134766aeab3268a38/src/java/org/apache/cassandra/config/Config.java#L1406-L1443 - what about using dynamic data masking on these columns ? only these users that have been grant the privileged can read the data ,otherwise masking data will be return . cc [~adelapena] - If a user with the CQL superuser privileges can undo the DDM, then DDM cannot be used as the solution. - it all depends on how you implement it . it can only be used for virtual table - are not only superusers allowed to see this table? - yes, you are right. and if you grant select permission on the virtual table, that means you give the user the permission to read the password. we do not support column level permission , see [here|https://issues.apache.org/jira/browse/CASSANDRA-12859] , If the permissions can be refined to the column level, I think this JIRA issue can be finally solved. - Well ... yes. If you give the permissions to them, they will indeed see it. The same way as a super-user sees them. Because ... you gave permissions to see it. I just fail to see under what circumstances you would give a non-super-user any visibility into this table? - there is CASSSIDECAR-272 which is going to read this so I think that it should be indeed redacted. - This does not seem to be problematic for 4.0, we are displaying these properties differently and passwords are just omitted https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/db/virtual/SettingsTable.java#L159-L189 - {quote}there is CASSSIDECAR-272 which is going to read this so I think that it should be indeed redacted. {quote} I think sidecar uses Cassandra auth under the hood ? that will have a new authenticated API endpoint differently from the v1 API. In any case it's probably not good practice to expose plain text credentials in virtual tables. I don't find very useful to expose a redacted setting, but I guess it is just to signalize the setting exist but cannot be read. - [~paulo] [~dcapwell] would be great to have some feedback here https://github.com/apache/cassandra/pull/4340 I tried to do something more robust so it is a no-brainer from now on. - Just to emphasise the redaction requirement from an enterprise perspective, the separation of concerns is an important factor here. Enterprises often have multiple operational personas. People who provision encryption passwords may not be the same as the CQL admin. Exposing plaintext secrets, even to CQL superusers, breaks this separation of concerns. A common compliance requirement (ISO 27001, SOC 2, PCI DSS) is that sensitive key material and passwords must not be retrievable like this. Also, this is worth a quick read - https://cwe.mitre.org/data/definitions/200.html - hi [~hayato.shimizu], yes, you are right. There is a comment of [~frankgh] (1) who asks if super-users should be allowed to see passwords and I think that the answer is, based on what you wrote as well, that \"no, they should not be able to see them\". (1) https://github.com/apache/cassandra/pull/4340/files#r2301604833 - What is redacted: 4.1 {code} client_encryption_options.keystore client_encryption_options.keystore_password client_encryption_options.truststore client_encryption_options.truststore_password server_encryption_options.keystore server_encryption_options.keystore_password server_encryption_options.truststore server_encryption_options.truststore_password {code} 5.0 - same as 4.1 5.1 / trunk {code} client_encryption_options.keystore client_encryption_options.keystore_password client_encryption_options.keystore_password_file client_encryption_options.truststore client_encryption_options.truststore_password client_encryption_options.truststore_password_file server_encryption_options.keystore server_encryption_options.keystore_password server_encryption_options.keystore_password_file server_encryption_options.truststore server_encryption_options.truststore_password server_encryption_options.truststore_password_file jmx_server_options.jmx_encryption_options.keystore jmx_server_options.jmx_encryption_options.keystore_password jmx_server_options.jmx_encryption_options.keystore_password_file jmx_server_options.jmx_encryption_options.truststore jmx_server_options.jmx_encryption_options.truststore_password jmx_server_options.jmx_encryption_options.truststore_password_file jmx_server_options.password_file {code} [~hayato.shimizu] [~frankgh] are you all ok with that? - What is problematic about redacting whole sections is that while testing it, there are existing tests in (1) (like I just linked), which are testing that we can set some configuration property. When I redact whole section, that test fails, because it will start to return \"REDACTED\". (1) https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/db/virtual/SettingsTableTest.java#L170 There is also this counter-argument for not redacting everything https://issues.apache.org/jira/browse/CASSANDRA-14573?focusedCommentId=16748128&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16748128 - [~maxwellguo] would you mind to review, please? - of course, i will take a look at it tomorrow. - [4.1 java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/6012/workflows/cf92b524-f735-48b2-876a-bb2b022e4b8f] [4.1 java8_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/6012/workflows/dadd3acc-7487-438b-8a55-4c87f54e7109] 5.0 https://pre-ci.cassandra.apache.org/job/cassandra-5.0/21/ trunk https://pre-ci.cassandra.apache.org/job/cassandra/78 - hey [~maxwellguo] , [~frankgh] has approved the patch and build are available, do you still want to review? - [~smiklosovic] I reviewed it 2 hours ago, and I remember I approved it :) update: I checked it again, it seems to be a network problem. I approved it again. - Resolving this, will return to 4.1.x fix version and I ll bump it to 4.1.11 when 4.1.10 which is currently being released is out and we have new version in JIRA.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20855", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20855"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: sstablemetadata throws IndexOutOfBoundsException if maxClustering is not a full clustering key\nDescription: In some cases we may get IndexOutOfBoundsException: {code} ./tools/bin/sstablemetadata /cassandra/data/keyspace/table-1707f6907c6e11f0b1fabf128b8d57a9/nb-5-big-Data.db SSTable: /cassandra/data/keyspace/table-1707f6907c6e11f0b1fabf128b8d57a9/nb-5-big Partitioner: org.apache.cassandra.dht.Murmur3Partitioner Bloom Filter FP chance: 0.1 Minimum timestamp: 1755556756403000 (08/18/2025 18:39:16) Maximum timestamp: 1755776375237088 (08/21/2025 07:39:35) SSTable min local deletion time: 1755774034 (08/21/2025 07:00:34) SSTable max local deletion time: 2147483647 (no tombstones) Compressor: org.apache.cassandra.io.compress.LZ4Compressor Compression ratio: 0.3084167695600824 TTL min: 0 TTL max: 5263998 (60 days 22 hours 13 minutes 18 seconds) First token: -9218140013640741760 (1:5129229) Last token: 9191231194481845151 (1:971548616326) Exception in thread \"main\" java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:659) at java.util.ArrayList.get(ArrayList.java:435) at org.apache.cassandra.tools.SSTableMetadataViewer.printSStableMetadata(SSTableMetadataViewer.java:369) at org.apache.cassandra.tools.SSTableMetadataViewer.main(SSTableMetadataViewer.java:548) {code} In this logic we have an implicit assumption that number of values in maxClusteringValues is always the same as for minClusteringValues but it is not true. For example, when we have a range tombstone: {code} List<AbstractType<?>> clusteringTypes = header.getClusteringTypes(); List<ByteBuffer> minClusteringValues = stats.minClusteringValues; List<ByteBuffer> maxClusteringValues = stats.maxClusteringValues; String[] minValues = new String[clusteringTypes.size()]; String[] maxValues = new String[clusteringTypes.size()]; for (int i = 0; i < clusteringTypes.size(); i++) { minValues[i] = clusteringTypes.get(i).getString(minClusteringValues.get(i)); maxValues[i] = clusteringTypes.get(i).getString(maxClusteringValues.get(i)); // issue is here <========== } field(\"minClusteringValues\", Arrays.toString(minValues)); field(\"maxClusteringValues\", Arrays.toString(maxValues)); {code}\nComments: - Need to check but I suppose it is only 4.x issue because since 5.0 we have another logic in this area, introduced in CASSANDRA-18134 - Checked for 5.0 - it is not affected - There is SSTableMetadataViewerTest we might add a test into which tests this. For the sake of the test it would be probably way more comfortable to create a tiny SSTable and commit it to test resources and then read it by that test instead of trying to construct that SSTable in a test directly (even though that is possible to do as well). - yep, it is the main reason why the status is still in progress, a test is remaining (and the most time consuming :) ) part to add. - I've added a test for the use case, a test SSTable is committed (to make the test faster) as well as the logic used to generate it - nice! we need CI though. One detail - could you please name \"data/sstable_metadata/test_table\" like \"data/sstable_metadata/test_table-CASSANDRA-20855\" instead? Or something similar. It is a little bit tricky to know very quickly what test that SSTable belongs to, without looking into git blame or similar. For example here (1) is a directory called \"CASSANDRA-15131\" so I guess we could use \"test/data/CASSANDRA-20855\" https://github.com/apache/cassandra/tree/cassandra-4.0/test/data - MRs: * 4.1: [https://github.com/apache/cassandra/pull/4344] * 4.0: [https://github.com/apache/cassandra/pull/4329] - [CASSANDRA-20855-4.1|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20855-4.1] {noformat} java11_pre-commit_tests \u2713 j11_build 2m 46s \u2713 j11_cqlsh_dtests_py3 5m 43s \u2713 j11_cqlsh_dtests_py311 6m 15s \u2713 j11_cqlsh_dtests_py3_vnode 6m 6s \u2713 j11_cqlshlib_cython_tests 9m 30s \u2713 j11_cqlshlib_tests 8m 25s \u2713 j11_dtests_vnode 40m 32s \u2713 j11_jvm_dtests 15m 8s \u2713 j11_unit_tests 9m 38s \u2713 j11_unit_tests_repeat 1m 44s j11_dtests 20m 51s \u2715 j11_cqlsh_dtests_py311_vnode 6m 29s cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_cqlsh_dtests_py38 5m 55s cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_cqlsh_dtests_py38_vnode 6m 10s cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_jvm_dtests_vnode 14m 7s junit.framework.TestSuite org.apache.cassandra.distributed.test.CASMultiDCTest {noformat} [java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5991/workflows/f8bcdc54-8eff-4d52-9e14-d3402fb345cc] - [CASSANDRA-20855-4.0|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20855-4.0] {noformat} java8_pre-commit_tests \u2713 j8_build 2m 35s \u2713 j8_cqlsh-dtests-py2-no-vnodes 5m 35s \u2713 j8_cqlsh-dtests-py2-with-vnodes 7m 2s \u2713 j8_cqlsh_dtests_py3 5m 18s \u2713 j8_cqlsh_dtests_py38 7m 16s \u2713 j8_cqlsh_dtests_py38_vnode 6m 31s \u2713 j8_cqlsh_dtests_py3_vnode 5m 34s \u2713 j8_cqlshlib_tests 9m 21s \u2713 j8_dtests_vnode 42m 5s \u2713 j8_jvm_dtests 13m 50s \u2713 j8_unit_tests 8m 57s \u2713 j8_unit_tests_repeat 2m 34s \u2713 j8_utests_system_keyspace_directory 8m 29s \u2713 j8_utests_system_keyspace_directory_repeat 2m 49s \u2713 j11_unit_tests_repeat 2m 20s \u2713 j11_unit_tests 8m 4s \u2713 j11_dtests_vnode 44m 20s \u2713 j11_cqlsh_dtests_py3_vnode 5m 57s \u2713 j11_cqlsh_dtests_py38_vnode 5m 53s \u2713 j11_cqlsh_dtests_py38 6m 8s \u2713 j11_cqlsh_dtests_py311_vnode 5m 49s \u2713 j11_cqlsh_dtests_py311 5m 40s \u2713 j11_cqlsh_dtests_py3 5m 34s \u2713 j11_cqlsh-dtests-py2-with-vnodes 5m 43s \u2713 j11_cqlsh-dtests-py2-no-vnodes 5m 21s \u2715 j8_cqlsh_dtests_py311 1m 38s \u2715 j8_cqlsh_dtests_py311_vnode 1m 32s \u2715 j8_dtests 54m 40s refresh_test.TestRefresh test_refresh_deadlock_startup \u2715 j11_dtests 54m 56s refresh_test.TestRefresh test_refresh_deadlock_startup {noformat} [java8_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5992/workflows/57e2634f-4b26-48f8-b95a-c04536e704c3] - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20854", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20854"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support low-overhead async profiling via JMX/Nodetool with the async-profiler library\nDescription: The [async-profiler|https://github.com/async-profiler/async-profiler] library is commonly used by the community to run low-overhead profiling on Cassandra processes due to its effectiveness and versatility. Following the discussion on [this thread,|https://lists.apache.org/thread/98y59qp42zwwrtbcvnzvlh254c5vyhoq] this library would be a great tool to integrate directly into Cassandra and expose via JMX accompanied by a convenient `nodetool profile` command.\nComments: - Opened PR for initial review - [https://github.com/apache/cassandra/pull/4255] Tagging [~drohrer], [~paulo], and [~yifanc] for visibility - Looking good so far, added some comments in the PR. - I see this ticket has been inactive for a while, and think that Yaman is not able to work on it anymore. I wanted to move it forward, and have create a PR using the already present code changes and applying the feedback on the original PR. Take a look here: https://github.com/apache/cassandra/pull/4487 - [^CASSANDRA-20854-ci_summary.html] Adding the CI - Thanks, added bunch of comments. - I highly encourage you to incorporate this (1). I have spent quite some time to refactor it so it is \"up to standards\", hardened everything and so on. We need this functionality a lot so I decided to help you with this patch a bit if you do not mind. (1) https://github.com/bbotella/cassandra/pull/4", "output": "Status: Changes Suggested\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20853", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20853"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Backport <condition property=\"arch_x86\"> to pre-5.0 branches where we actually have JDK8\nDescription: The below was added to build.xml for people on arm. The comment points to JDK8, which was also dropped in 5.0, so maybe we don't even need it. - https://github.com/apache/cassandra/blob/cassandra-5.0/build.xml#L222-L229 Someone hit that issue today with JDK8 and cassandra-4.0 on a non-intel Mac. Considering that cassandra-4.0 and cassandra-4.1 are still maintained, I do not see a reason not to backport this non-controversial change.\nComments: - It seems like we will have to modify our scripts to navigate around the jvm-server.options file... I guess it is not worth it on an old released version. Even if we do, I realized we also need a JNA bump to make these C* versions start on Mac, and we already opted out of that. Closing the ticket as Won't do.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20852", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20852"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improving CONTRIBUTING.md for new contributors\nDescription: Fleshing out CONTRIBUTING.md a bit to (A) Make it more welcoming to new contributors, (B) Provide pointers to key website documentation on development, testing, code style, etc.\nComments: - PR for initial edits: https://github.com/apache/cassandra/pull/4327 - Revised PR per feedback: A little more wording on workflow being a recommendation, added step to fork the repo, provided info about related projects: https://github.com/apache/cassandra/pull/4327/files - +1 - mick has approved the pr, I am going to merge it. - [~maxwellguo] are you going to merge? - yes, I will merge in the next few hours. - committed as eeb227be2f8d27392fe56e7818d5284e6f24e885", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20851", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20851"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implement nodetool history command\nDescription: There is already history file with all previous commands in `~/.cassandra/nodetool.history` but there is no corresponding nodetool command which would display it. A user just has to go to that file and inspect it manually which is not comfortable.\nComments: - I have been trying to do something \"smart\" as reading from the end of the file etc. but performance wise it was actually worse when number of commands to print is cca equal to number of lines in history file. What I measured, it was not significantly faster on default output either (processing last 1000 lines). Just reading whole file to a list and printing just a subset of that is just enough. My history file has 28 MB, containing 400k lines and it just prints it below 2 seconds. file containing 1000 lines {code:java} real 0m0,706s user 0m1,749s sys 0m0,175s {code} file containing 400k lines {code:java} real 0m0,804s user 0m1,914s sys 0m0,196s {code} - +1 thank you for the patch! - CI [https://pre-ci.cassandra.apache.org/job/cassandra/79/#showFailuresLink] the failing test was fixed locally, no reason to run CI again and burn credits.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20850", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20850"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: List of PMC members on website is outdated\nDescription: The [PMC roster on the website|https://cassandra.apache.org/_/community.html#meet-the-community] hasn't been updated in 4 years and so is missing 11 names.\nComments: - should we just point to the automated asf page ? https://projects.apache.org/committee.html?cassandra - I'm not that strongly opinionated, but other projects (e.g., [Apache Spark|https://spark.apache.org/committers.html], [Apache HBase|https://hbase.apache.org/team.html]) maintain separate pages instead of pointing to the automated one so I've opened a PR updating it within the current framework. - +1 to point to ASF automated page since that is auto-updated and also include committers - or somehow fetch it from there automatically. I think it's a bit elitist to mention only PMC members in the community page. - Fine by me. Will update my PR to replace the static list with a button to the Committee page. - Updated PR and built locally: !image-2025-08-19-10-52-25-512.png! - Looking awesome! While you're at it, do you mind adding another image side-by-side with the current one? I've attached a more recent group photo to this ticket from Cassandra Summit 2023. Thanks! - [~paulo] You got it. Pushed new commit to the PR with local build showing the result: !Screenshot 2025-08-19 at 12.07.52 PM.png! - LGTM - live.", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20849", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20849"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: commons-lang vulnerability: CVE-2025-48924\nDescription: [https://nvd.nist.gov/vuln/detail/CVE-2025-48924] Uncontrolled Recursion vulnerability in Apache Commons Lang. This issue affects Apache Commons Lang: Starting with commons-lang:commons-lang 2.0 to 2.6, and, from org.apache.commons:commons-lang3 3.0 before 3.18.0.\nComments: - [CASSANDRA-20848-20849-4.0|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20848-20849-4.0] {noformat} java11_pre-commit_tests \u2713 j11_build 2m 23s \u2713 j11_cqlsh-dtests-py2-no-vnodes 5m 28s \u2713 j11_cqlsh-dtests-py2-with-vnodes 5m 16s \u2713 j11_cqlsh_dtests_py3 5m 44s \u2713 j11_cqlsh_dtests_py311 5m 30s \u2713 j11_cqlsh_dtests_py311_vnode 6m 1s \u2713 j11_cqlsh_dtests_py38 6m 0s \u2713 j11_cqlsh_dtests_py38_vnode 7m 12s \u2713 j11_cqlsh_dtests_py3_vnode 6m 25s \u2713 j11_cqlshlib_tests 6m 48s \u2713 j11_dtests_vnode 43m 50s \u2713 j11_jvm_dtests 14m 23s \u2713 j11_unit_tests 8m 23s \u2715 j11_dtests 56m 35s refresh_test.TestRefresh test_refresh_deadlock_startup {noformat} [java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5975/workflows/29a8ef9e-447b-4d20-a238-563cce79cef6] - all test results are included in CASSANDRA-20848", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20848", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20848"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: jackson-core vulnerability: CVE-2025-52999\nDescription: https://nvd.nist.gov/vuln/detail/CVE-2025-52999 jackson-core contains core low-level incremental (\"streaming\") parser and generator abstractions used by Jackson Data Processor. In versions prior to 2.15.0, if a user parses an input file and it has deeply nested data, Jackson could end up throwing a StackoverflowError if the depth is particularly large.\nComments: - [CASSANDRA-20848-20849-4.0|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20848-20849-4.0] {noformat} java11_pre-commit_tests \u2713 j11_build 2m 23s \u2713 j11_cqlsh-dtests-py2-no-vnodes 5m 28s \u2713 j11_cqlsh-dtests-py2-with-vnodes 5m 16s \u2713 j11_cqlsh_dtests_py3 5m 44s \u2713 j11_cqlsh_dtests_py311 5m 30s \u2713 j11_cqlsh_dtests_py311_vnode 6m 1s \u2713 j11_cqlsh_dtests_py38 6m 0s \u2713 j11_cqlsh_dtests_py38_vnode 7m 12s \u2713 j11_cqlsh_dtests_py3_vnode 6m 25s \u2713 j11_cqlshlib_tests 6m 48s \u2713 j11_dtests_vnode 43m 50s \u2713 j11_jvm_dtests 14m 23s \u2713 j11_unit_tests 8m 23s \u2715 j11_dtests 56m 35s refresh_test.TestRefresh test_refresh_deadlock_startup {noformat} [java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5975/workflows/29a8ef9e-447b-4d20-a238-563cce79cef6] for 4.1 and 5.0, there are failures of configuration compatibility test I am looking into. [CASSANDRA-20848-20849-5.0|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20848-20849-5.0] {noformat} java17_pre-commit_tests \u2713 j17_build 6m 5s \u2713 j17_cqlsh_dtests_py311 6m 0s \u2713 j17_cqlsh_dtests_py311_vnode 6m 20s \u2713 j17_cqlsh_dtests_py38 6m 0s \u2713 j17_cqlsh_dtests_py38_vnode 6m 22s \u2713 j17_cqlshlib_cython_tests 8m 1s \u2713 j17_cqlshlib_tests 6m 38s \u2713 j17_dtests_latest 41m 4s \u2713 j17_dtests_vnode 41m 35s \u2713 j17_jvm_dtests 19m 55s \u2713 j17_jvm_dtests_latest_vnode 18m 3s \u2715 j17_dtests 36m 36s refresh_test.TestRefresh test_refresh_deadlock_startup \u2715 j17_unit_tests 14m 40s org.apache.cassandra.config.ConfigCompatibilityTest diff_3_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_4_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_4_1 org.apache.cassandra.config.ConfigCompatibilityTest diff_5_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_3_11 \u2715 j17_utests_latest 16m 0s org.apache.cassandra.config.ConfigCompatibilityTest diff_3_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_4_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_4_1 org.apache.cassandra.config.ConfigCompatibilityTest diff_5_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_3_11 \u2715 j17_utests_oa 15m 13s org.apache.cassandra.config.ConfigCompatibilityTest diff_3_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_4_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_4_1 org.apache.cassandra.config.ConfigCompatibilityTest diff_5_0 org.apache.cassandra.config.ConfigCompatibilityTest diff_3_11 {noformat} [java17_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5976/workflows/3552ab1b-ba0e-4ac9-9d4c-0fa19f5a1725] trunk: [java17_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5978/workflows/0ee06eea-9f17-42f7-9f9f-7dcb703e2c6c] [java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5978/workflows/9136b0f9-6906-41fc-a05a-42588b8b0780] - The error is of this nature: {code} org.apache.cassandra.config.ConfigCompatibilityTest diff_5_0 java.lang.NoSuchMethodError: 'void org.yaml.snakeyaml.parser.ParserImpl.<init>(org.yaml.snakeyaml.reader.StreamReader, org.yaml.snakeyaml.LoaderOptions)' at com.fasterxml.jackson.dataformat.yaml.YAMLParser.<init>(YAMLParser.java:196) at com.fasterxml.jackson.dataformat.yaml.YAMLFactory._createParser(YAMLFactory.java:505) at com.fasterxml.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:393) at com.fasterxml.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:15) at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3752) at org.apache.cassandra.config.ConfigCompatibilityTest.load(ConfigCompatibilityTest.java:246) at org.apache.cassandra.config.ConfigCompatibilityTest.diff(ConfigCompatibilityTest.java:161) at org.apache.cassandra.config.ConfigCompatibilityTest.diff_5_0(ConfigCompatibilityTest.java:154) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) {code} There is misalignement of snakeyaml and jackson I introduced in the PR. Jackson 2.19.2 I bumped Jackson to depends on snakeyaml 2.4 but Cassandra use snakeyaml 1.26 in 4.0 and 4.1 and 5.0. We use snakeyaml 2.1 in trunk. The solution: for 4.0 -> all is OK, we do not use jackson-dataformat-yaml which brings snakeyaml for 4.1 -> we use jackson-dataformat-yaml of version 2.13.2 which already excludes snakeyaml. To be compatible with snakeyaml of Cassandra 4.1 (1.26), we can just dowgrade jackson-dataformat-yaml to 2.13.2 (or, keep it at what it is), and just bump the rest of jackson libraries to 2.19.2. jackon-dataformat-yaml is test-scoped dependency only anyway. for 5.0 -> same as for 4.1 for trunk -> we use snakeyaml 2.1 in prod. jackson-dataformat-yaml 2.19.2 is compatible snakeyaml 2.1 so tests work, but it still excludes snakeyaml from its dependencies to get what we use in prod. Here we can update jackson-dataformat-yaml to 2.19.2 while we still continue to exclude snakeyaml to depend on prod snakeyaml instead. - [CASSANDRA-20848-20849-4.1|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20848-20849-4.1] {noformat} java11_pre-commit_tests \u2713 j11_build 2m 47s \u2713 j11_cqlsh_dtests_py311 5m 40s \u2713 j11_cqlshlib_cython_tests 11m 20s \u2713 j11_cqlshlib_tests 7m 33s \u2713 j11_dtests_vnode 40m 12s \u2713 j11_jvm_dtests 14m 25s \u2713 j11_jvm_dtests_vnode 12m 5s \u2713 j11_unit_tests 9m 12s \u2715 j11_cqlsh_dtests_py3 6m 21s cql_tracing_test.TestCqlTracing test_tracing_default_impl cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_cqlsh_dtests_py311_vnode 6m 6s cql_tracing_test.TestCqlTracing test_tracing_default_impl \u2715 j11_cqlsh_dtests_py38 6m 2s cql_tracing_test.TestCqlTracing test_tracing_simple \u2715 j11_cqlsh_dtests_py38_vnode 5m 36s cql_tracing_test.TestCqlTracing test_tracing_default_impl cql_tracing_test.TestCqlTracing test_tracing_simple \u2715 j11_cqlsh_dtests_py3_vnode 5m 42s cql_tracing_test.TestCqlTracing test_tracing_default_impl cql_tracing_test.TestCqlTracing test_tracing_simple cql_tracing_test.TestCqlTracing test_tracing_unknown_impl \u2715 j11_dtests 58m 44s refresh_test.TestRefresh test_refresh_deadlock_startup java11_separate_tests java8_pre-commit_tests java8_separate_tests {noformat} [java11_pre-commit_tests|https://app.circleci.com/pipelines/github/instaclustr/cassandra/5979/workflows/3ff26147-f340-4a0e-9e7b-b66d0ca54961] not sure what's up with tracing, it does not fail locally, it is unrelated anyway. [CASSANDRA-20848-20849-5.0|https://github.com/instaclustr/cassandra/tree/CASSANDRA-20848-20849-5.0] {noformat} java17_pre-commit_tests \u2713 j17_build 8m 52s \u2713 j17_cqlsh_dtests_py311 6m 1s \u2713 j17_cqlsh_dtests_py311_vnode 6m 25s \u2713 j17_cqlsh_dtests_py38 5m 54s \u2713 j17_cqlsh_dtests_py38_vnode 6m 37s \u2713 j17_cqlshlib_cython_tests 7m 45s \u2713 j17_cqlshlib_tests 10m 10s \u2713 j17_dtests_vnode 41m 59s \u2713 j17_jvm_dtests 21m 3s \u2713 j17_jvm_dtests_latest_vnode 19m 5s \u2713 j17_unit_tests 14m 53s \u2713 j17_utests_latest 16m 28s \u2713 j17_utests_oa 17m 24s \u2715 j17_dtests 35m 55s refresh_test.TestRefresh test_refresh_deadlock_startup materialized_views_test.TestMaterializedViewsConsistency test_multi_partition_consistent_reads_after_write \u2715 j17_dtests_latest 46m 18s bootstrap_test.TestBootstrap test_read_from_bootstrapped_node java17_separate_tests java11_pre-commit_tests java11_separate_tests {noformat} - [PR 4.0|https://github.com/apache/cassandra/pull/4335] [PR 4.1|https://github.com/apache/cassandra/pull/4336] [PR 5.0|https://github.com/apache/cassandra/pull/4337] [PR trunk|https://github.com/apache/cassandra/pull/4338] - +1", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20847", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20847"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Accord fixes\nDescription: Accord fixes: - Fix erroneous call to registerWithDelay with absolute deadline - ExecuteSyncPoint should discount votes from unbootstrapped replicas - Bootstrap no longer needs to first ensure durability (now ExecuteSyncPoint discounts votes from unbootstrapped replicas) - Progress stall with home shard stopping before Stable is propagated to all shards - Only upgrade Durability.HasPhase if we have queried all shards - Incorrectly inferring Durability in CheckStatus.finish - Incorrectly inferring/propagating stable from fast unstable reply; clarify to prevent further mistakes - MaxDecidedRX should track separate hlc bounds for filtering non-RX dependencies - minGcBefore.hlc() can move backwards across epochs, even if each shard moves forwards; must track minHlc directly - TxnId.parse for RV Also improve: - Don't query recovery state if fast path durably decided - Support deferred partial dep deserialisation - Support both orientations of KeyDeps/RangeDeps - Support partialDeps as ByteBuffer - Don't inflate when reading from cache for CommandsForRanges - Improve Deps serialization and Journal skipping\nComments: No comments.", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20846", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20846"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Unreliable query plan\nDescription: I am getting this error sometimes - very unreliably for the same query / structure which is frightening: [https://github.com/apache/cassandra/blob/9d5cef7f8c6c749d3f2b71fc1f6fae1ee086c2ae/src/java/org/apache/cassandra/index/sai/plan/StorageAttachedIndexQueryPlan.java#L89] select count(1) as count from xxx where is_deleted=false and ckey_type='lll' and cobj='org' and ctype='ggg' and oid=5403ef72-6e59-11f0-9018-465a9786c25e and key in ('asdasd','asdasd') allow filtering ; oid is the PK, and key is an SAI indexed col The weird thing is this sometimes / not all the time triggers an error :(\nComments: - Thanks for the report! Do you have any other information around the cluster size, keyspace (i.e. replication factor), table schema, and consistency level of your reads? This particular error is only thrown when strict filtering isn't possible from the coordinator's perspective. (ex. Strict filtering would be trivially allowed on a single-node cluster or at CL LOCAL_ONE/ONE, for instance. See the JavaDoc just above the link you've posted.)", "output": "Status: Triage Needed\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20845", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20845"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Test failure: org.apache.cassandra.service.accord.EpochSyncTest.test\nDescription: Observed during a pre-commit test run: {code} Property error detected: Seed = 1099291986993665462 Examples = 50 Pure = true Error: expected:<[COMPLE]TED> but was:<[NOT_STAR]TED> Steps: 500 Values: State: Topology: 14 Joined -6685484876646754832 17 Joined -4547643122712696438 12 Joined -2477977647150120874 7 Joined 2090079937055291339 9 Joined 3057505775704015055 5 Joined 4442400764383878551 1 Joined 6616008556894027521 13 Joined 7986072535660820288 16 Joined 8692081832504375642 : org.apache.cassandra.service.accord.EpochSyncTest.Cluster {code}\nComments: - cc [~dcapwell] - another example: https://ci-cassandra.apache.org/job/Cassandra-trunk/2246/#showFailuresLink - cc [~ifesdjeen] - Alex is on holiday. I'm not 100% sure what happened with this, as I think he had fixed it somewhere. BUT, I am fairly sure this has been addressed by my latest patch(es). - Thank you for the clarification. The last failure which I've seen is [https://ci-cassandra.apache.org/job/Cassandra-trunk/2265/#showFailuresLink] 3 Sept 2025, commit: [https://github.com/apache/cassandra/commit/b907c1e973f40f4f97fb6e57c58ec6a106d486c7] After it two last runs have been aborted due to CI timeouts. - I do not see EpochSyncTest in the list of failed tests anymore for the last runs, so I think we can consider it as fixed :-)", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20844", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20844"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: BEGIN TRANSACTION crashes if a mutation touches multiple rows\nDescription: This is forked from CASSANDRA-20828 to isolate the serialization changes. CQL operations such as the following cause BEGIN TRANSACTION to fail as this logic assumed mutations map to a single partition/row {code} UPDATE tbl \u2026 WHERE pk = 0 AND ck IN (0, 1, 2, 3); {code} This is allowed logic in normal CQL and is just fails due to a bad assumption when creating the write Fragments.\nComments: - +1 in GH", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20843", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20843"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: replica_filtering_protection section of cassandra.yaml needs a name change in description\nDescription: In the Cassandra 5 cassandra.yaml, we have the following guardrail section: {code:java} # Filtering and secondary index queries at read consistency levels above ONE/LOCAL_ONE use a # mechanism called replica filtering protection to ensure that results from stale replicas do # not violate consistency. (See CASSANDRA-8272 and CASSANDRA-15907 for more details.) This # mechanism materializes replica results by partition on-heap at the coordinator. The more possibly # stale results returned by the replicas, the more rows materialized during the query. replica_filtering_protection: # These thresholds exist to limit the damage severely out-of-date replicas can cause during these # queries. They limit the number of rows from all replicas individual index and filtering queries # can materialize on-heap to return correct results at the desired read consistency level. # # \"cached_replica_rows_warn_threshold\" is the per-query threshold at which a warning will be logged. # \"cached_replica_rows_fail_threshold\" is the per-query threshold at which the query will fail. # # These thresholds may also be adjusted at runtime using the StorageService mbean. # # If the failure threshold is breached, it is likely that either the current page/fetch size # is too large or one or more replicas is severely out-of-sync and in need of repair. cached_rows_warn_threshold: 2000 cached_rows_fail_threshold: 32000 {code} This is confusing for some as we had a customer attempt to configure cached_replica_rows_warn_threshold and cached_replica_rows_fail_threshold in addition to the actual parameters. We should update the wording so the description matches the parameters.\nComments: - [~maedhroz] by the way, these are actually not _guardrails_. They do not integrate with guardrails framework. I wonder if this was done on purpose or not as it is used in DataResolver etc. which might be performance sensitive. The easiest job is to just change the comments and any traces of {{cached_replica_rows_...}} as OP requires and move on. - Another option is to rename the properties themselves to {{cached_replica_rows_...}} but that would be more involved as we would need to keep the old ones still functioning so that would lead to us using \"@Replaces\" . - I think this patch landed in 3.0 originally, where perhaps the new Guardrails framework didn't exist, and we didn't want to implemented it different ways in different branches? In any case, I would just change the comments and the lingering places in {{ReplicaFilteringProtectionTest}} and {{ReplicaFilteringProtection}} where they are used. - Assign to me, I will make _*cached_rows*_ as guardrail and correct related yaml/doc/comment/UT tonight - I do not think this ticket should serve as a way how to make it a guardrail. There should be a dedicated one. This is just for renaming. - Agreed. Let's just use this Jira to get the naming fixed in all branches.", "output": "Status: Open\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "CASSANDRA-20842", "url": "https://issues.apache.org/jira/browse/CASSANDRA-20842"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix version range check in MessagingService.getVersionOrdinal\nDescription: Thank you to [~maedhroz] and Claude :) for highlighting a minor issue in the recent changes for CASSANDRA-20816: {code:java} ## Issues Found ### 1. Incorrect bounds check condition if (result < 0 || result > maxVersion) This condition is wrong. The check should be against the array length, not `maxVersion`. Currently: - `maxVersion` is the actual version value (e.g., 15) - The check should be against `Version.values().length - 1` (the maximum valid ordinal) ### 2. Misleading variable name `maxVersion` actually contains the version value, not the maximum ordinal index. ## Corrected Implementation static final int minVersion = Version.values()[0].value; static final int maxVersionOrdinal = Version.values().length - 1; /** * This is an optimisation to speed up the translation of the serialization * version to the {@link Version} enum ordinal. * We expect values for new versions to be incremented sequentially * * @param version the serialization version * @return a {@link Version} ordinal value */ public static int getVersionOrdinal(int version) { int result = version - minVersion; if (result < 0 || result > maxVersionOrdinal) throw new IllegalStateException(\"Unknown serialization version: \" + version); return result; } ## Additional Issues ### 3. Typo in exception message \"Unkown\" should be \"Unknown\" ### 4. Test case issue The test `checkUnknownBigVersion()` uses `Byte.MAX_VALUE + 1` (128), but this assumes version values are byte-sized. A more robust test would use a value that's guaranteed to be out of range regardless of the actual version values. ## Recommended Test Fix @Test(expected = IllegalStateException.class) public void checkUnknownBigVersion() { // Use a value that's definitely larger than any reasonable version MessagingService.getVersionOrdinal(1000); } {code}\nComments: - * I think 1-2 statements are correct, good catch by Claude :). * Regarding #3 - it is inherited from the old logic but it makes sense to fix it as well * Regarding #4 checkUnknownBigVersion - here I relied on the fact that the version is 8bit (but probably I should multiple MAX_VALUE by 2 due to sign/unsigned difference..): {code:java} // 8 bits version, so don't waste versions public enum Version {code} and org.apache.cassandra.net.HandshakeProtocol.Initiate#maybeDecode: {code:java} int minMessagingVersion = getBits(flags, 16, 8); int maxMessagingVersion = getBits(flags, 24, 8); {code} 1000 suggested here by AI I do not think very good as well. Taking in account the discovered bug I think it would be better to adjust the test and use the first out of range value instead - MR: https://github.com/apache/cassandra/pull/4320 - CI test results: * [^ci_summary_20842-trunk.htm] * [^results_details_20842-trunk.tar.xz] Known issues or flaky tests: * distributed.test.SSTableLoaderEncryptionOptionsTest bulkLoaderSuccessfullyStreamsOverSslWithDeprecatedSslStoragePort-_jdk11_x86_64 * distributed.test.log.InProgressSequenceCoordinationTest bootstrapProgressTest-_jdk11_x86_64 * distributed.test.log.InProgressSequenceCoordinationTest decommissionProgressTest-_jdk11_x86_64 * distributed.test.log.InProgressSequenceCoordinationTest inProgressSequenceRetryTest-_jdk11_x86_64 * simulator.test.AccordHarrySimulationTest test-cassandra.testtag_IS_UNDEFINED * simulator.test.TrivialSimulationTest trivialTest-_jdk11_x86_64 - +1 - bq. 1000 suggested here by AI I do not think very good as well. Taking in account the discovered bug I think it would be better to adjust the test and use the first out of range value instead +1000 ;)", "output": "Status: Resolved\nPriority: Normal"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19911", "url": "https://issues.apache.org/jira/browse/KAFKA-19911"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky test ShareConsumerTest.testShareGroupMaxSizeConfigExceeded\nDescription: [https://develocity.apache.org/scans/tests?search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D] {code:java} org.opentest4j.AssertionFailedError: Condition not met within timeout 30000. The 4th consumer was not kicked out of the group ==> expected: <true> but was: <false>at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:445)at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:493)at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:442)at org.apache.kafka.clients.consumer.ShareConsumerTest.testShareGroupMaxSizeConfigExceeded(ShareConsumerTest.java:2370)at java.lang.reflect.Method.invoke(Method.java:569)at java.util.Optional.ifPresent(Optional.java:178)at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)at java.util.ArrayList.forEach(ArrayList.java:1511)at java.util.ArrayList.forEach(ArrayList.java:1511) {code} Fail logs: {code:java} [2025-11-19 10:02:14,812] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,829] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,830] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,834] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,844] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,845] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) Bootstrap metadata: BootstrapMetadata(records=[ApiMessageAndVersion(FeatureLevelRecord(name='metadata.version', featureLevel=29) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='eligible.leader.replicas.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='group.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='share.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='streams.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='transaction.version', featureLevel=2) at version 0)], metadataVersionLevel=29, source=format command) Formatting metadata directory /tmp/kafka-14829393993145388027/controller_3000 with metadata.version 4.2-IV1. Bootstrap metadata: BootstrapMetadata(records=[ApiMessageAndVersion(FeatureLevelRecord(name='metadata.version', featureLevel=29) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='eligible.leader.replicas.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='group.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='share.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='streams.version', featureLevel=1) at version 0), ApiMessageAndVersion(FeatureLevelRecord(name='transaction.version', featureLevel=2) at version 0)], metadataVersionLevel=29, source=format command) Formatting metadata directory /tmp/kafka-14829393993145388027/broker_0_data0 with metadata.version 4.2-IV1. [2025-11-19 10:02:14,870] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,871] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,875] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,887] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:14,924] WARN [QuorumController id=3000] Performing controller activation. The metadata log appears to be empty. Appending 6 bootstrap record(s) in metadata transaction at metadata.version 4.2-IV1 from bootstrap source 'testkit'. (org.apache.kafka.controller.QuorumController:106) [2025-11-19 10:02:14,951] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:15,010] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:15,024] WARN 'share' in group.coordinator.rebalance.protocols is deprecated. Share groups are controlled by the 'share.version' feature; this broker config will be ignored in a future release. (kafka.server.KafkaConfig:73) [2025-11-19 10:02:15,256] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator:48) [2025-11-19 10:02:20,615] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator:48) [2025-11-19 10:02:20,616] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator:48) [2025-11-19 10:02:20,617] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator:48) [2025-11-19 10:02:35,623] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator:48) [2025-11-19 10:02:35,632] ERROR [ShareConsumer clientId=consumer-group1-241, groupId=group1] ShareGroupHeartbeatRequest failed due to GROUP_MAX_SIZE_REACHED: The share group has reached its maximum capacity of 3 members. (org.apache.kafka.clients.consumer.internals.ShareHeartbeatRequestManager:405) [2025-11-19 10:02:35,632] ERROR [ShareConsumer clientId=consumer-group1-241, groupId=group1] Member parZioqNSU-v1HQJcS9PTg with epoch 0 transitioned to fatal state (org.apache.kafka.clients.consumer.internals.ShareMembershipManager:445) [2025-11-19 10:02:40,628] WARN [ShareConsumer clientId=consumer-group1-241, groupId=group1] Exception thrown in acknowledgement commit callback (org.apache.kafka.clients.consumer.internals.ShareConsumerImpl:618) org.apache.kafka.common.errors.GroupMaxSizeReachedException: The share group has reached its maximum capacity of 3 members. [2025-11-19 10:03:06,758] ERROR No share partition found for groupId group1 topicPartition aB3To2-gSTyuhHYJPFV0Ng:topic-0 while releasing acquired topic partitions (kafka.server.share.SharePartitionManager:380) [2025-11-19 10:03:06,760] ERROR No share partition found for groupId group1 topicPartition aB3To2-gSTyuhHYJPFV0Ng:topic-0 while releasing acquired topic partitions (kafka.server.share.SharePartitionManager:380) [2025-11-19 10:03:06,855] WARN [ShareConsumer clientId=consumer-group1-239, groupId=group1] Connection to node 0 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,859] WARN [ShareConsumer clientId=consumer-group1-238, groupId=group1] Connection to node 0 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,864] WARN [ShareConsumer clientId=consumer-group1-240, groupId=group1] Connection to node 0 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,868] WARN [ShareConsumer clientId=consumer-group1-241, groupId=group1] Connection to node 0 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,870] WARN [ShareConsumer clientId=consumer-group1-241, groupId=group1] Connection to node -1 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,870] WARN [ShareConsumer clientId=consumer-group1-241, groupId=group1] Bootstrap broker localhost:44511 (id: -1 rack: null isFenced: false) disconnected (org.apache.kafka.clients.NetworkClient:1255) [2025-11-19 10:03:06,961] WARN [ShareConsumer clientId=consumer-group1-238, groupId=group1] Connection to node 0 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,968] WARN [ShareConsumer clientId=consumer-group1-240, groupId=group1] Connection to node 0 (localhost/127.0.0.1:44511) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:899) [2025-11-19 10:03:06,977] WARN [NodeToControllerChannelManager id=3000 name=registration] Attempting to close NetworkClient that has already been closed. (org.apache.kafka.clients.NetworkClient:744) {code}\nComments: - I assume this is already fixed by KAFKA-19840. [~francisgodinho] can you reproduce the flaky on your local? - [~chia7712] Ah sorry, I didn't see that ticket. Haven't been able to reproduce it yet so gonna close this one out.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19910", "url": "https://issues.apache.org/jira/browse/KAFKA-19910"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support rack awareness in UniformHeterogeneousAssignmentBuilder\nDescription: \nComments: No comments.", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19909", "url": "https://issues.apache.org/jira/browse/KAFKA-19909"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support rack awareness in UniformHomogeneousAssignmentBuilder\nDescription: \nComments: - PR: [https://github.com/apache/kafka/pull/20000]", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19908", "url": "https://issues.apache.org/jira/browse/KAFKA-19908"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support rack awareness in RangeAssignor#assignHeterogeneousGroup\nDescription: \nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19907", "url": "https://issues.apache.org/jira/browse/KAFKA-19907"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support rack awareness in RangeAssignor#assignHomogeneousGroup\nDescription: \nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19906", "url": "https://issues.apache.org/jira/browse/KAFKA-19906"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: NullPointerException from Streams Metrics Infra on Topology Startup\nDescription: We saw this stacktrace when starting up a new instance. It appears to be very non-deterministic. ``` 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.TaskManager.transitRestoredTaskToRunning(TaskManager.java:980) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.TaskManager.handleRestoredTasksFromStateUpdater(TaskManager.java:1056) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.TaskManager.checkStateUpdater(TaskManager.java:923) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.StreamThread.checkStateUpdater(StreamThread.java:1433) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1239) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:926) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:886) [kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928Caused by: java.lang.NullPointerException: Cannot invoke \"Object.hashCode()\" because \"key\" is null 2025-11-21 12:56:11.928 at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1125) ~[?:?] 2025-11-21 12:56:11.928 at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1116) ~[?:?] 2025-11-21 12:56:11.928 at org.apache.kafka.common.metrics.Metrics.removeMetric(Metrics.java:550) ~[kafka-clients-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.removeMetric(StreamsMetricsImpl.java:341) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.internals.metrics.OpenIterators.remove(OpenIterators.java:68) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueIterator.close(MeteredKeyValueStore.java:467) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 at io.littlehorse.server.streams.store.LHKeyValueIterator.close(LHKeyValueIterator.java:33) ~[server-0.16.0-SNAPSHOT.jar:?] 2025-11-21 12:56:11.928 at io.littlehorse.server.streams.topology.core.processors.CommandProcessor.onPartitionClaimed(CommandProcessor.java:142) ~[server-0.16.0-SNAPSHOT.jar:?] 2025-11-21 12:56:11.928 at io.littlehorse.server.streams.topology.core.processors.CommandProcessor.init(CommandProcessor.java:85) ~[server-0.16.0-SNAPSHOT.jar:?] 2025-11-21 12:56:11.928 at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:123) ~[kafka-streams-4.1.1.jar:?] 2025-11-21 12:56:11.928 ... 10 more ``` We were running 4.1.1 with 2 streamthreads. This was when a new instance was joining the group.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19905", "url": "https://issues.apache.org/jira/browse/KAFKA-19905"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Tight reconnection loop during shutdown\nDescription: During shutdown, nodes 1 and 2 (brokers) are stuck in an infinite loop trying to connect to node 0 (the controller) every 50ms. The issue is time sensitive, but it can be reproduced easily shutting down all nodes at the same time. The problem is that even during shutdown, the NodeToControllerRequestThread continues to run. The RaftControllerNodeProvider still returns node 0 as the controller from cached Raft metadata, but node 0 has already terminated (NodeToControllerChannelManager:323). Looking at logs, the controller shut down at 12:31:38 while brokers were still in controlled shutdown. The sequence shows: 1. Node 1 and 2 request controlled shutdown 2. Controller grants the shutdown 3. Controller itself shuts down (RaftManager shutdown at 12:31:38) 4. Node 1 and 2 continue trying to heartbeat to the now-dead controller 5. They get stuck in this reconnection loop because the NodeToControllerRequestThread is still running and hasn't been shut down properly {code} [2025-11-21 12:31:38,515] INFO [NodeToControllerChannelManager id=2 name=heartbeat] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient) [2025-11-21 12:31:38,515] WARN [NodeToControllerChannelManager id=2 name=heartbeat] Connection to node 0 (localhost/127.0.0.1:9090) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient) [2025-11-21 12:31:38,515] INFO [broker-2-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:9090 (id: 0 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread) [2025-11-21 12:31:38,566] INFO [broker-2-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:9090 (id: 0 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread) [2025-11-21 12:31:38,566] INFO [NodeToControllerChannelManager id=2 name=heartbeat] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient) [2025-11-21 12:31:38,567] WARN [NodeToControllerChannelManager id=2 name=heartbeat] Connection to node 0 (localhost/127.0.0.1:9090) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient) [2025-11-21 12:31:38,567] INFO [broker-2-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:9090 (id: 0 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread) [2025-11-21 12:31:38,616] INFO [broker-2-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:9090 (id: 0 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread) {code}\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19904", "url": "https://issues.apache.org/jira/browse/KAFKA-19904"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add handling throttled records delivery for lower delivery limit\nDescription: \nComments: No comments.", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19903", "url": "https://issues.apache.org/jira/browse/KAFKA-19903"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add integration tests for handling throttled delivery records for Share Group\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19902", "url": "https://issues.apache.org/jira/browse/KAFKA-19902"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Consumer triggers OFFSET_OUT_OF_RANGE when committed offset uses stale epoch after leader change\nDescription: h2. Summary When a partition leader changes and the consumer commits offsets with a stale epoch, if the log segments containing that epoch are subsequently deleted due to retention policy, the consumer will encounter OFFSET_OUT_OF_RANGE error and reset to earliest (if auto.offset.reset=earliest), causing massive message reprocessing.The root cause is that SubscriptionState.allConsumed() uses position.offsetEpoch instead of position.currentLeader.epoch when constructing OffsetAndMetadata for commit, which can become stale when leader changes occur. If `auto.offset.reset=latest`, *the consequences will be more severe, resulting in message loss.* ---- h2. Environment Cluster Configuration: * Kafka Server Version: 3.9.0 * Kafka Client Version: 3.9.0 * Topic: 200 partitions, 7-day retention, no tiered storage * Consumer Group: 45 consumers (1 KafkaConsumer thread per machine) * No broker/controller restarts occurred * High throughput producer continuously writing messages Consumer Configuration: {code:java} auto.offset.reset=earliest enable.auto.commit=true {code} Consumer Code: * Registered ConsumerRebalanceListener * Calls kafkaConsumer.commitSync() in onPartitionsRevoked() method ---- h2. Problem Description In a scenario where the consumer group has no lag, consumers suddenly consumed a massive amount of messages, far exceeding the recent few minutes of producer writes. Investigation revealed that multiple partitions reset to the earliest offset and reprocessed up to 7 days of historical data. ---- h2. Observed Symptoms (Timeline) # Consumer group rebalance occurred (triggered by normal consumer group management) # Consumer logged OFFSET_OUT_OF_RANGE errors immediately after rebalance # Consumer reset to earliest offset due to auto.offset.reset=earliest configuration # Kafka broker logged NotLeaderOrFollowerException around the same timeframe, indicating partition leader changes # Consumer did not log any NOT_LEADER_OR_FOLLOWER errors (these are DEBUG level and not visible in production logs) The image below uses the partition asyncmq_local_us_us_marketplace-ssl-a_aws_us-east-1_2a7e053c-9d90-4efd-af2d-3a8bf9564715-153 as an example to trace the problem log chain. {*}See attached log screenshot at the bottom{*}. ---- h2. Root Cause Analysis h3. The Problem Chain 1. Leader change occurs (epoch changes from N to N+1) \u2193 2. Consumer continues processing old batches (epoch=N) \u2193 3. Consumer commits offset during/after rebalance \u251c\u2500 Committed offset: 1000 \u2514\u2500 Committed epoch: N (using position.offsetEpoch from old batch) \u2193 4. High throughput + retention policy causes old segments (epoch=N) to be deleted \u2193 5. Consumer restarts/rebalances and fetches committed offset \u251c\u2500 Tries to validate offset 1000 with epoch=N \u2514\u2500 Broker cannot find epoch=N ({*}segments deleted or tp leader change{*}) \u2193 6. Broker returns OFFSET_OUT_OF_RANGE \u2193 7. Consumer resets to earliest offset h3. Code Analysis The problematic code in SubscriptionState.allConsumed(): {code:java} // kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() { Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>(); assignment.forEach((topicPartition, partitionState) -> { if (partitionState.hasValidPosition()) allConsumed.put(topicPartition, new OffsetAndMetadata( partitionState.position.offset, partitionState.position.offsetEpoch, // Problem: uses offsetEpoch from consumed batch \"\")); }); return allConsumed; } {code} Why this is problematic:The FetchPosition class contains two different epoch values: * offsetEpoch: The epoch from the last consumed record's batch * currentLeader.epoch: The current partition leader's epoch from metadata When committing offsets, we should use currentLeader.epoch instead of offsetEpoch because: # offsetEpoch represents the epoch of already consumed data (historical) # currentLeader.epoch represents the current partition leader (up-to-date) h3. Scenarios Where These Epochs Diverge Scenario A: Leader changes while consumer is processing old batches * T1: Consumer fetches batch with epoch=5 * T2: Leader changes to epoch=6 * T3: Metadata updates with new leader epoch=6 * T4: Consumer commits offset * offsetEpoch = 5 (from batch being processed) * currentLeader.epoch = 6 (from updated metadata) * Problem: Commits epoch=5, which may soon be deleted Scenario B: Recovery from committed offset after leader change * Consumer commits offset with old epoch=N * Leader changes to epoch=N+1 * Old segments (epoch=N) are deleted by retention policy * Consumer rebalances and tries to restore from committed offset * offsetEpoch = N (from committed offset) * currentLeader.epoch = N+1 (from current metadata) * Problem: Validation fails because epoch=N no longer exists ---- h2. Steps to Reproduce This is a timing-sensitive edge case. The following conditions increase the likelihood: # Setup: * High-throughput topic (to trigger faster log rotation) * Relatively short retention period (e.g., 7 days) * Consumer group with rebalance listener calling commitSync() * enable.auto.commit=true (or any manual commit) # Trigger: * Trigger a partition leader change (broker restart, controller election, etc.) * Simultaneously or shortly after, trigger a consumer group rebalance * Wait for retention policy to delete old log segments # Expected Result: Consumer should resume from committed offset # Actual Result: Consumer encounters OFFSET_OUT_OF_RANGE and resets to earliest ---- h2. Impact * Data Reprocessing: Consumers may reprocess up to retention.ms worth of data * Service Degradation: Sudden spike in consumer throughput can overwhelm downstream systems * Resource Waste: Unnecessary CPU, memory, and network usage * Potential Duplicates: If using auto.offset.reset=earliest, duplicate message processing is guaranteed * If `auto.offset.reset=latest`, *the consequences will be more severe, resulting in message loss.* ---- h2. Proposed Fix h3. Root Cause Analysis The issue is more fundamental than a simple field selection problem. The core issue is that both epoch values in FetchPosition can be stale at commit time: # offsetEpoch: Contains the epoch from the last consumed record's batch. If a leader change occurs after consumption but before commit, this epoch becomes stale and may reference log segments that have been deleted. # currentLeader.epoch: Inherited from the previous position during normal consumption and only updated when: * NOT_LEADER_OR_FOLLOWER or FENCED_LEADER_EPOCH errors are detected * Position is restored from committed offsets (fetches from metadata) * Explicit validation is triggered via maybeValidatePositionForCurrentLeader() During normal, error-free consumption, currentLeader is never updated and can also become stale. h3. Problem with Current Code Location: org.apache.kafka.clients.consumer.internals.FetchCollector {code:java} if (nextInLineFetch.nextFetchOffset() > position.offset) { SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( nextInLineFetch.nextFetchOffset(), nextInLineFetch.lastEpoch(), // offsetEpoch: from consumed batch position.currentLeader); // \u274c currentLeader: inherited, NOT updated! log.trace(\"Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`\", position, nextPosition, tp, partRecords.size()); subscriptions.position(tp, nextPosition); positionAdvanced = true; } {code} The inherited currentLeader means it can be as stale as offsetEpoch in certain scenarios. ---- h3. Recommended Solution: Proactively Update currentLeader During Position Updates Option 1: Update currentLeader when advancing position (Primary Recommendation)Modify FetchCollector to fetch the latest leader information from metadata every time the position is updated: {code:java} if (nextInLineFetch.nextFetchOffset() > position.offset) { // Fetch the latest leader information from metadata Metadata.LeaderAndEpoch currentLeaderAndEpoch = metadata.currentLeader(tp); SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( nextInLineFetch.nextFetchOffset(), nextInLineFetch.lastEpoch(), currentLeaderAndEpoch); // \u2705 Use fresh leader info from metadata log.trace(\"Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`\", position, nextPosition, tp, partRecords.size()); subscriptions.position(tp, nextPosition); positionAdvanced = true; } {code} Advantages: * Ensures currentLeader is always up-to-date * Makes allConsumed() safe to use *currentLeader.epoch* for commits Modify SubscriptionState.allConsumed() to {color:#de350b}use currentLeader.epoch instead of offsetEpoch{color}: {code:java} public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() { Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>(); assignment.forEach((topicPartition, partitionState) -> { if (partitionState.hasValidPosition()) allConsumed.put(topicPartition, new OffsetAndMetadata( partitionState.position.offset, partitionState.position.currentLeader.epoch, // \u2705 Use current leader epoch \"\")); }); return allConsumed; } {code} * Minimal performance impact (metadata lookup is O(1) from local cache) * Aligns with the existing pattern in refreshCommittedOffsets() Potential Concerns: * Adds one metadata lookup per position update * If metadata is stale, currentLeader.epoch could still lag slightly, but this is the same risk as today ---- h3. Alternative Solutions Option 2: Fetch fresh leader info during commitModify allConsumed() to fetch the latest leader information at commit time: {code:java} // Note: This would require passing metadata reference to allConsumed() public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed(ConsumerMetadata metadata) { Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>(); assignment.forEach((topicPartition, partitionState) -> { if (partitionState.hasValidPosition()) { // Fetch the latest leader epoch from metadata at commit time Metadata.LeaderAndEpoch latestLeader = metadata.currentLeader(topicPartition); Optional<Integer> epochToCommit = latestLeader.epoch.isPresent() ? latestLeader.epoch : partitionState.position.offsetEpoch; // Fallback to offsetEpoch allConsumed.put(topicPartition, new OffsetAndMetadata( partitionState.position.offset, epochToCommit, \"\")); } }); return allConsumed; } {code} Advantages: * Only impacts commit path, not consumption hot path * Directly addresses the commit-time staleness issue Disadvantages: * Requires changing the signature of allConsumed() (API change) * May still have a race condition if leader changes between metadata fetch and commit * Metadata could be stale if update hasn't been processed yet ---- Option 3: Use the maximum epoch valueUse the larger of the two epoch values, assuming newer epochs have higher values: {code:java} public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() { Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>(); assignment.forEach((topicPartition, partitionState) -> { if (partitionState.hasValidPosition()) { Optional<Integer> epochToCommit; if (partitionState.position.offsetEpoch.isPresent() && partitionState.position.currentLeader.epoch.isPresent()) { // Use the maximum of the two epochs int maxEpoch = Math.max( partitionState.position.offsetEpoch.get(), partitionState.position.currentLeader.epoch.get()); epochToCommit = Optional.of(maxEpoch); } else { // Fallback to whichever is present epochToCommit = partitionState.position.currentLeader.epoch.isPresent() ? partitionState.position.currentLeader.epoch : partitionState.position.offsetEpoch; } allConsumed.put(topicPartition, new OffsetAndMetadata( partitionState.position.offset, epochToCommit, \"\")); } }); return allConsumed; } {code} Advantages: * No API changes required * Simple to implement * Provides better protection than using only one epoch Disadvantages: * Heuristic-based; assumes epochs are monotonically increasing * Could still use a stale epoch if both values are old * Doesn't solve the root cause of stale currentLeader ---- h3. Recommendation Primary recommendation: Implement Option 1 (Update currentLeader during position updates)This is the most robust solution because: # It ensures currentLeader is always fresh # It fixes the root cause rather than working around symptoms # It has minimal performance impact # It makes the codebase more consistent and maintainable Secondary recommendation: Implement Option 3 as a defense-in-depth measureEven with Option 1, using max(offsetEpoch, currentLeader.epoch) in allConsumed() provides additional safety against any edge cases where one epoch might be more up-to-date than the other.Combined approach (strongest protection): {code:java} // In FetchCollector.java if (nextInLineFetch.nextFetchOffset() > position.offset) { Metadata.LeaderAndEpoch currentLeaderAndEpoch = metadata.currentLeader(tp); SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( nextInLineFetch.nextFetchOffset(), nextInLineFetch.lastEpoch(), currentLeaderAndEpoch); // \u2705 Keep currentLeader fresh subscriptions.position(tp, nextPosition); positionAdvanced = true; } // In SubscriptionState.java public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() { Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>(); assignment.forEach((topicPartition, partitionState) -> { if (partitionState.hasValidPosition()) { // Use the maximum epoch as defense-in-depth Optional<Integer> epochToCommit; if (partitionState.position.offsetEpoch.isPresent() && partitionState.position.currentLeader.epoch.isPresent()) { epochToCommit = Optional.of(Math.max( partitionState.position.offsetEpoch.get(), partitionState.position.currentLeader.epoch.get())); } else { epochToCommit = partitionState.position.currentLeader.epoch.isPresent() ? partitionState.position.currentLeader.epoch : partitionState.position.offsetEpoch; } allConsumed.put(topicPartition, new OffsetAndMetadata( partitionState.position.offset, epochToCommit, \"\")); } }); return allConsumed; } {code} This combined approach provides: * Prevention: Keep currentLeader fresh during normal operation * Defense: Use the best available epoch value at commit time * Resilience: Minimize the window where a stale epoch can cause issues ---- h2. Additional Notes Why consumers don't log NOT_LEADER_OR_FOLLOWER errors:All consumer-side handling of NOT_LEADER_OR_FOLLOWER errors uses DEBUG level logging: {code:java} // FetchCollector.java line 325 log.debug(\"Error in fetch for partition {}: {}\", tp, error.exceptionName()); // AbstractFetch.java line 207 log.debug(\"For {}, received error {}, with leaderIdAndEpoch {}\", partition, partitionError, ...); // OffsetsForLeaderEpochUtils.java line 102 LOG.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\", ...); {code} This makes the issue difficult to diagnose in production environments. ---- h2. Workarounds (Until Fixed) # Increase retention period to reduce likelihood of epoch deletion # Monitor consumer lag to ensure it stays low # Reduce rebalance frequency (increase max.poll.interval.ms, session.timeout.ms) # Use cooperative rebalance strategy to minimize rebalance impact # Consider using auto.offset.reset=latest if reprocessing is more costly than data loss (application-dependent) ---- h2. Related Code References h3. 1. The problematic method: SubscriptionState.allConsumed() Location: org.apache.kafka.clients.consumer.internals.SubscriptionState {code:java} public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() { Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>(); assignment.forEach((topicPartition, partitionState) -> { if (partitionState.hasValidPosition()) allConsumed.put(topicPartition, new OffsetAndMetadata(partitionState.position.offset, partitionState.position.offsetEpoch, \"\")); // Uses offsetEpoch instead of currentLeader.epoch }); return allConsumed; } {code} h3. 2. How FetchPosition is updated during normal consumption Location: org.apache.kafka.clients.consumer.internals.FetchCollector {code:java} if (nextInLineFetch.nextFetchOffset() > position.offset) { SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( nextInLineFetch.nextFetchOffset(), nextInLineFetch.lastEpoch(), // offsetEpoch: from consumed batch position.currentLeader); // currentLeader: inherited from old position, NOT updated! log.trace(\"Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`\", position, nextPosition, tp, partRecords.size()); subscriptions.position(tp, nextPosition); positionAdvanced = true; } {code} Key Issue: The currentLeader field is inherited from the previous position and not automatically updated during normal consumption. It only gets updated when leader change errors are detected. h3. 3. How committed offsets are restored after rebalance Location: org.apache.kafka.clients.consumer.internals.ConsumerUtils.refreshCommittedOffsets() {code:java} public static void refreshCommittedOffsets(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata, final ConsumerMetadata metadata, final SubscriptionState subscriptions) { for (final Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsetsAndMetadata.entrySet()) { final TopicPartition tp = entry.getKey(); final OffsetAndMetadata offsetAndMetadata = entry.getValue(); if (offsetAndMetadata != null) { // first update the epoch if necessary entry.getValue().leaderEpoch().ifPresent(epoch -> metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch)); // it's possible that the partition is no longer assigned when the response is received, // so we need to ignore seeking if that's the case if (subscriptions.isAssigned(tp)) { final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.currentLeader(tp); final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(), // offsetEpoch from committed offset (may be old) leaderAndEpoch); // currentLeader from current metadata (may be new) subscriptions.seekUnvalidated(tp, position); log.info(\"Setting offset for partition {} to the committed offset {}\", tp, position); } } } } {code} The Divergence Point: When restoring from committed offsets, offsetEpoch comes from the stored offset (potentially old), while currentLeader comes from fresh metadata (potentially new after leader change). h3. 4. How OffsetsForLeaderEpoch validation request is constructed Location: org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochUtils.prepareRequest() {code:java} static AbstractRequest.Builder<OffsetsForLeaderEpochRequest> prepareRequest( Map<TopicPartition, SubscriptionState.FetchPosition> requestData) { OffsetForLeaderTopicCollection topics = new OffsetForLeaderTopicCollection(requestData.size()); requestData.forEach((topicPartition, fetchPosition) -> fetchPosition.offsetEpoch.ifPresent(fetchEpoch -> { OffsetForLeaderTopic topic = topics.find(topicPartition.topic()); if (topic == null) { topic = new OffsetForLeaderTopic().setTopic(topicPartition.topic()); topics.add(topic); } topic.partitions().add(new OffsetForLeaderPartition() .setPartition(topicPartition.partition()) .setLeaderEpoch(fetchEpoch) // Uses offsetEpoch for validation .setCurrentLeaderEpoch(fetchPosition.currentLeader.epoch .orElse(RecordBatch.NO_PARTITION_LEADER_EPOCH)) ); }) ); return OffsetsForLeaderEpochRequest.Builder.forConsumer(topics); } {code} The Validation Problem: The validation request uses fetchEpoch (which is offsetEpoch) to validate against the broker. If this epoch no longer exists in the broker's log, validation fails and triggers OFFSET_OUT_OF_RANGE. h3. 5. FetchPosition class definition Location: org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchPosition {code:java} /** * Represents the position of a partition subscription. * * This includes the offset and epoch from the last record in * the batch from a FetchResponse. It also includes the leader epoch at the time the batch was consumed. */ public static class FetchPosition { public final long offset; final Optional<Integer> offsetEpoch; // Epoch from last consumed record's batch final Metadata.LeaderAndEpoch currentLeader; // Current partition leader info from metadata FetchPosition(long offset) { this(offset, Optional.empty(), Metadata.LeaderAndEpoch.noLeaderOrEpoch()); } public FetchPosition(long offset, Optional<Integer> offsetEpoch, Metadata.LeaderAndEpoch currentLeader) { this.offset = offset; this.offsetEpoch = Objects.requireNonNull(offsetEpoch); this.currentLeader = Objects.requireNonNull(currentLeader); } @Override public String toString() { return \"FetchPosition{\" + \"offset=\" + offset + \", offsetEpoch=\" + offsetEpoch + \", currentLeader=\" + currentLeader + '}'; } }{code} Class Design: The class contains both offsetEpoch (historical data epoch) and currentLeader.epoch (current metadata epoch), but allConsumed() only uses the former when committing. h1. Attachements logs: h2. group rebalance log !image-2025-11-21-18-03-24-592.png! h2. consumer client log !image-2025-11-21-20-00-10-862.png! h2. broker server log !image-2025-11-21-18-11-24-316.png!\nComments: - Hi [~guozhang] [~showuon] [~hachikuji] Could you please help me look into this issue? Thank you very much!", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19901", "url": "https://issues.apache.org/jira/browse/KAFKA-19901"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: improve the error message when changing the configuration of dynamic voters\nDescription: The static voters accept dynamic configuration on individual controllers, whereas the dynamic voters do not. While KAFKA-18928 will address this, we should also explain this distinction in the documentation and relevant error messages\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19900", "url": "https://issues.apache.org/jira/browse/KAFKA-19900"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix the documentation for remove controller\nDescription: Fix the documentation for remove controller since KIP-996 is merged.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19899", "url": "https://issues.apache.org/jira/browse/KAFKA-19899"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bumping group epoch when member regex subscription changes from non empty to empty\nDescription: In `GroupMetadataManager#maybeUpdateRegularExpressions`, `updateRegularExpressionsResult` is set to REGEX_UPDATED if the updated member regex subscription is empty and the current member's is not empty, which [doesn't trigger a group epoch bump|https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java#L2376-L2386] later in `consumerGroupHeartbeat`. We should fix it by returning REGEX_UPDATED_AND_RESOLVED in this case.\nComments: - I'm investigating this. I verified the issue and will propose a fix soon. - Hi [~antonvasant], thanks for taking this. I've updated the description with more context. Feel free to ask more questions if you need clarification. - Thank you [~dongnuolyu], Just to clarify - when you say \"member regex subscription changes from non empty to empty\", do you mean: # The member's regex text itself changes from something like \"topic.*\" to empty/null (OR) # The number of members subscribed to a particular regex drops to zero (last member unsubscribes) (also case 1 can lead to case 2 right)", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19898", "url": "https://issues.apache.org/jira/browse/KAFKA-19898"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: OOM in Tls12SelectorTest#testGracefulClose\nDescription: {code:java} Gradle Test Run :clients:test > Gradle Test Executor 14 > Tls12SelectorTest > testGracefulClose() SKIPPED {code} {code:java} org.gradle.api.internal.tasks.testing.TestSuiteExecutionException: Could not complete execution for Gradle Test Executor 14. at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:66) at java.base@25.0.1/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104) at java.base@25.0.1/java.lang.reflect.Method.invoke(Method.java:565) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92) at jdk.proxy1/jdk.proxy1.$Proxy4.stop(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:194) at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:126) at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103) at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63) at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56) at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:122) at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:72) at app//worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69) at app//worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded at java.base/java.util.Arrays.copyOfRange(Arrays.java:3849) at java.base/java.lang.String.<init>(String.java:563) at java.base/java.lang.String.<init>(String.java:546) at java.base/java.util.jar.Manifest.parseName(Manifest.java:353) at java.base/java.util.jar.Manifest.read(Manifest.java:307) at java.base/java.util.jar.Manifest.<init>(Manifest.java:106) at java.base/java.util.jar.JarFile.getManifestFromReference(JarFile.java:417) at java.base/java.util.jar.JarFile.getManifest(JarFile.java:393) at java.base/jdk.internal.loader.URLClassPath$JarLoader$1.getManifest(URLClassPath.java:721) at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:762) at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:691) at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:620) at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:578) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490) at org.apache.kafka.test.TestSslUtils$CertificateBuilder.generate(TestSslUtils.java:428) at org.apache.kafka.test.TestSslUtils$SslConfigsBuilder.buildJks(TestSslUtils.java:672) at org.apache.kafka.test.TestSslUtils$SslConfigsBuilder.build(TestSslUtils.java:656) at org.apache.kafka.test.TestSslUtils.createSslConfig(TestSslUtils.java:245) at org.apache.kafka.test.TestSslUtils.createSslConfig(TestSslUtils.java:230) at org.apache.kafka.test.TestSslUtils.createSslConfig(TestSslUtils.java:224) at org.apache.kafka.common.network.SslSelectorTest.setUp(SslSelectorTest.java:72) at java.base/java.lang.invoke.LambdaForm$DMH/0x000000003b298400.invokeVirtual(LambdaForm$DMH) at java.base/java.lang.invoke.LambdaForm$MH/0x000000003b084800.invoke(LambdaForm$MH) at java.base/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(DirectMethodHandleAccessor.java:154) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104) at java.base/java.lang.reflect.Method.invoke(Method.java:565) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:787) at org.junit.platform.commons.support.ReflectionSupport.invokeMethod(ReflectionSupport.java:479) at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:161) {code}\nComments: - from: https://github.com/apache/kafka/pull/20917#issuecomment-3549551390 - possible root cause: github.com/apache/kafka/pull/20792/files#r2543321764 - Should be fixed with the linked PR (we can reopen if ever there is more behind it)", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19897", "url": "https://issues.apache.org/jira/browse/KAFKA-19897"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add support for Java 25 in 3.9 branch\nDescription: related discussion: https://github.com/apache/kafka-site/pull/748#issuecomment-3551441828\nComments: No comments.", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19896", "url": "https://issues.apache.org/jira/browse/KAFKA-19896"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Use MockTime::sleep instead of retrying in ShareConsumeRequestManagerTest.\nDescription: Currently, when there is a backoff wait period, we are retrying the acknowledgements until the backoff period completes and then these acks are sent. But as this is a unit test, we can use {color:#0747a6}time.sleep(){color} to forward the currentTime, which will allow the backoff period to be over. 4 tests in {color:#0747a6}ShareConsumeRequestManagerTest{color} use this currently.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19895", "url": "https://issues.apache.org/jira/browse/KAFKA-19895"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: __consumer_offsets Partitions Growing to TB Size and Not Cleaning Until Broker Restart\nDescription: Description: We encountered an abnormal situation with the Kafka system topic `__consumer_offsets` where two specific partition replicas grew to extremely large sizes and did not respond to cleanup policies. Details: 1. Two replicas under `__consumer_offsets` unexpectedly reached extremely large sizes: - Partition replica size: *{*}8.4 TB{*}* - Partition replica size: *{*}3.9 TB{*}* Other replicas of the same topic are only about *{*}100 MB{*}*, which is normal. 2. We attempted to force cleanup by applying: cleanup.policy = compact,delete retention.ms = <short value> This worked normally for all other partitions in the cluster. However, **the two abnormal partitions did not reduce in size at all**, even after hours of waiting. 3. After restarting the Kafka brokers, cleanup and compaction finally resumed, and the partitions returned to normal size. Questions: 1. What could cause only two `__consumer_offsets` partitions to grow to multi-TB sizes, while others remain at ~100 MB? 2. Why did modifying the cleanup policy not take effect on these abnormal partitions until a broker restart? 3. Could this be a known issue or bug in Kafka **2.7.1**, especially related to log cleanup or compaction? 4. Are there scenarios in which compaction for an offset partition can stall or freeze indefinitely? 5. What mitigation or preventive steps are recommended to avoid this problem in long-running clusters? Environment: - Kafka version: **2.7.1** - Deployment: Kubernetes - Cluster has been running for a long period without restart - Affected topic: `__consumer_offsets` - Affected partitions: 2 (replicas reached 8.4T and 3.9T)\nComments: No comments.", "output": "Status: Open\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19894", "url": "https://issues.apache.org/jira/browse/KAFKA-19894"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Reintroduce SaslPlainSslEndToEndAuthorizationTest\nDescription: PR [17424|https://github.com/apache/kafka/pull/17424/files#r1802513652] removed {{SaslPlainSslEndToEndAuthorizationTest}} along with {{AclAuthorizer}}. While there was a test within {{SaslPlainSslEndToEndAuthorizationTest}} which tested ZK ACLs, it *also* tested all the tests in its inheritance hierarchy. We should therefore re-introduce it as the suite lacks a test for {{SASL/PLAIN}} mechanism.\nComments: - 4.0: [https://github.com/apache/kafka/commit/f49a2fd083b8656779eaca8d8f7b60407efd5d21] 4.1: https://github.com/apache/kafka/commit/5428c70dec8472b2ed707385d87786c1087f6635 trunk: [https://github.com/apache/kafka/commit/26e45f62cddbd843b31e7c66fb423e6635de09d0]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19893", "url": "https://issues.apache.org/jira/browse/KAFKA-19893"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Reduce tiered storage redundancy with delayed upload\nDescription: Currently, Kafka uploads all non-active local log segments to remote storage even when they are still within the local retention period, resulting in redundant storage of the same data in both tiers. This wastes storage capacity and network bandwidth without providing immediate benefits, since reads during the retention window prioritize local data. However, some users rely on remote storage for real-time analytics and need the latest data to be available as soon as possible. Therefore, this optimization is offered as an optional configuration rather than the default behavior. !image-2025-11-18-19-19-27-579.png!\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19892", "url": "https://issues.apache.org/jira/browse/KAFKA-19892"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Acq lock timeout field in share ack response\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19891", "url": "https://issues.apache.org/jira/browse/KAFKA-19891"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: AsyncKafkaConsumer logs \"Unexpected exception\" for timeouts during close\nDescription: The new consumer (in share, consumer and streams groups) log \"unexpected exception\" at error log level when hitting a timeout during consumer close: [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L318] [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L489] This is confusing, since this is not really unexpected, and should probably be logged at warn level at most.\nComments: - Hey [~lucasbru] , thanks for raising this. I took a look and had a doubt, other than the timeout case, in cases of broker disconnection or any auth exception, we will enter this part of the code on close. So there it would be nice to log at the error level right? Maybe we can remove the word \"unexpected\" and have a better log line there? What do you think? - I'm not sure - if we timeout or disconnect during a close, does this really indicate an error that the user should be aware of? I agree that we could be more selective - maybe just remove \"unexpected\" and reduce to WARN for timeout exception... - Right Im not sure too if the user should be made aware of these errors. I can take this up and raise a fix for this? I see ClassicKafkaConsumer logs at WARN level for timeouts and handles error responses(including logging) even during close. Here AsyncConsumer ignores the responses and just logs the error, so maybe we can warn for timeout exception only? - Yes, in particular if the classic consumer logs at `WARN` level as well. Maybe [~lianetm] has an opinion here? - Hi all, I'd like to help with this issue. Since the correct log behavior is still being discussed, my plan is to start by investigating the exact scenarios where the \u201cUnexpected \u201d log is emitted in {{AsyncKafkaConsumer}} and the event loop. I will map out: # The state transitions that trigger this log # Which transitions are valid vs. truly unexpected After gathering that information, I\u2019ll summarize my findings here so the correct approach can be decided together. If nobody is actively working on this, could you please assign the issue to me? - https://github.com/apache/kafka/pull/20965", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19890", "url": "https://issues.apache.org/jira/browse/KAFKA-19890"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix start/stop command failure after wmic removal in newer Windows.\nDescription: The commands will failed in new windows version: windows 11 !image-2025-11-17-07-47-09-575.png! the background: https://techcommunity.microsoft.com/blog/windows-itpro-blog/wmi-command-line-wmic-utility-deprecation-next-steps/4039242 !image-2025-11-17-08-36-47-770.png! attach the PRs: [https://github.com/apache/kafka/pull/20896] [https://github.com/apache/kafka/pull/20257]\nComments: - [https://github.com/apache/kafka/pull/20896] [https://github.com/apache/kafka/pull/20257] attach the PRs", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19889", "url": "https://issues.apache.org/jira/browse/KAFKA-19889"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: max.connections.per.ip.overrides does not apply overrides to all IPs for multi-IP hostnames\nDescription: {{max.connections.per.ip}} can be overridden per host using the {{max.connections.per.ip.overrides}} configuration. The override value may be specified using either an IP address or a hostname. When a hostname is used, and that hostname resolves to multiple IP addresses (e.g., behind a load balancer or auto-scaling group), {*}only the first resolved IP address receives the override{*}, while all other resolved addresses fallback to the default limit. *Example:* {code:java} max.connections.per.ip=100 max.connections.per.ip.overrides=kafka.connect.test.com:500{code} hostname resolution: {code:java} kafka.connect.test.com => [127.0.0.1, 127.0.0.2, 127.0.0.3] {code} actual applied values: {code:java} 127.0.0.1 => 500 127.0.0.2 => 100 127.0.0.3 => 100 {code} *Expected behavior* All IPs resolved from the hostname should receive the override: {code:java} 127.0.0.1 => 500 127.0.0.2 => 500 127.0.0.3 => 500 {code} *Use Case* We configure a global connection limit of 100 and want to increase the limit for Kafka Connect workers behind a load-balanced and auto-scaling hostname. Using the hostname in {{max.connections.per.ip.overrides}} should apply the override to _all_ backend IPs. *Cause* Current code uses: {code:java} @volatile private var maxConnectionsPerIpOverrides = config.maxConnectionsPerIpOverrides.map { case (host, count) => (InetAddress.getByName(host), count) } {code} where _InetAddress.getByName(host)_ returns only the first resolved IP, as it internally uses: {code:java} public static InetAddress getByName(String host) throws UnknownHostException { return InetAddress.getAllByName(host)[0]; } {code} *Proposed Fix* Resolve all IPs using {{InetAddress.getAllByName(host)}} and apply the override value to each returned address.\nComments: - Fixed in https://github.com/apache/kafka/pull/20894", "output": "Status: Patch Available\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19888", "url": "https://issues.apache.org/jira/browse/KAFKA-19888"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Coordinator histogram negative values causing persistent write timeouts and consumer instability in Kafka 4.1.0\nDescription: h2. Summary In Kafka 4.1.0, a broker entered a corrupted state where the Coordinator began producing {*}negative histogram values{*}, causing persistent write timeouts to {{{}__consumer_offsets{}}}, continuous group coordinator instability, and repeated consumer disconnections. The issue did *not* self-heal after time synchronization stabilized and only cleared after restarting the affected broker. This suggests that the Coordinator histogram/timer subsystem becomes invalid after certain time irregularities and does not recover automatically. ---- h2. *Description* We observed repeated Coordinator failures on a single broker in a production Kafka 4.1.0 cluster. The internal histogram used by the Coordinator runtime began to record {*}negative values{*}, which should be impossible under correct operation. Once this occurs: * Coordinator write events begin to timeout repeatedly * Consumers are disconnected and reconnected frequently * Group coordination is unstable * Commits failing on __consumer_offsets * Lag increasing (\"fake\" lag), because of failures in commits * The broker remains in a broken state even after the system clock stabilizes * Only restarting the broker clears the condition * After the restart, consumer groups immediately select the same broker again as coordinator, and the problem eventually reappears This behavior suggests state corruption inside the Coordinator's timer/histogram logic, where the {{GroupCoordinator}} component is susceptible to a fatal {{ArrayIndexOutOfBoundsException}} when attempting to record a negative value in an internal histogram. This bug appears to be triggered by a non-monotonic clock adjustment (i.e., the system clock moving backward). When this exception occurs, the {{group-coordinator-event-processor}} thread fails, specifically when processing internal events like {{{*}HighWatermarkUpdate{*}.}} The failures cause of offset *commit failures* from {*}consumers{*}. ---- h2. *Actual Behavior* * Coordinator internal histograms observe *negative values* * The Coordinator *fails to write to {{__consumer_offsets}}* * Consumers experience recurring instability * Kafka does not self-recover even when system time becomes stable * Only a broker restart resolves the condition ---- h2. *Expected behavior* Kafka should: * Reject or guard against negative deltas in histogram/timer calculations * Prevent a corrupted Coordinator state from persisting * Avoid selecting a malfunctioning broker as group coordinator ---- h2. *Impact* * Critical production consumer instability * Continuous rebalances * Offset commit delays * ** \"Fake Lags\" * Persistent coordinator malfunction * Requires broker restarts to recover * High operational risk ---- h2. *LOGS* *Server-Side:* The {{GroupCoordinator}} broker log is flooded with the following exception, indicating the event processor thread is failing: *Kafka Log - 2025-11-12 at 14:56* {code:java} [2025-11-12 14:56:38,166] ERROR [group-coordinator-event-processor-0]: Failed to run event InternalEvent(name=HighWatermarkUpdate) due to: Histogram recorded value cannot be negative.. (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread) java.lang.ArrayIndexOutOfBoundsException: Histogram recorded value cannot be negative. [2025-11-12 14:56:38,167] ERROR [GroupCoordinator id=5] Execution of HighWatermarkUpdate failed due to Histogram recorded value cannot be negative.. (org.apache.kafka.coordinator.common.runtime.CoordinatorRuntime) java.lang.ArrayIndexOutOfBoundsException: Histogram recorded value cannot be negative. ... [2025-11-12 14:58:38,170] ERROR [group-coordinator-event-processor-2]: Failed to run event InternalEvent(name=FlushBatch) due to: Histogram recorded value cannot be negative.. (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread) java.lang.ArrayIndexOutOfBoundsException: Histogram recorded value cannot be negative. [2025-11-12 14:58:38,170] ERROR [GroupCoordinator id=5] Execution of FlushBatch failed due to Histogram recorded value cannot be negative.. (org.apache.kafka.coordinator.common.runtime.CoordinatorRuntime) java.lang.ArrayIndexOutOfBoundsException: Histogram recorded value cannot be negative.{code} *Kafka Log - 2025-11-13 09:15* {code:java} [2025-11-13 09:15:39,777] ERROR [group-coordinator-event-processor-0]: Failed to run event InternalEvent(name=HighWatermarkUpdate) due to: Histogram recorded value cannot be negative.. (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread) java.lang.ArrayIndexOutOfBoundsException: Histogram recorded value cannot be negative. at org.HdrHistogram.AbstractHistogram.countsArrayIndex(AbstractHistogram.java:2399) ~[HdrHistogram-2.2.2.jar:?] at org.HdrHistogram.AbstractHistogram.recordSingleValue(AbstractHistogram.java:559) ~[HdrHistogram-2.2.2.jar:?] at org.HdrHistogram.AbstractHistogram.recordValue(AbstractHistogram.java:467) ~[HdrHistogram-2.2.2.jar:?] at org.HdrHistogram.Recorder.recordValue(Recorder.java:136) ~[HdrHistogram-2.2.2.jar:?] at org.apache.kafka.coordinator.common.runtime.HdrHistogram.record(HdrHistogram.java:100) ~[kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.KafkaMetricHistogram.record(KafkaMetricHistogram.java:128) ~[kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.common.metrics.Sensor.recordInternal(Sensor.java:236) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.metrics.Sensor.record(Sensor.java:197) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.CoordinatorRuntimeMetricsImpl.recordEventQueueTime(CoordinatorRuntimeMetricsImpl.java:284) ~[kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread.handleEvents(MultiThreadedEventProcessor.java:146) [kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread.run(MultiThreadedEventProcessor.java:179) [kafka-coordinator-common-4.1.0.jar:?] [2025-11-13 09:15:39,777] ERROR [GroupCoordinator id=5] Execution of HighWatermarkUpdate failed due to Histogram recorded value cannot be negative.. (org.apache.kafka.coordinator.common.runtime.CoordinatorRuntime) java.lang.ArrayIndexOutOfBoundsException: Histogram recorded value cannot be negative. at org.HdrHistogram.AbstractHistogram.countsArrayIndex(AbstractHistogram.java:2399) ~[HdrHistogram-2.2.2.jar:?] at org.HdrHistogram.AbstractHistogram.recordSingleValue(AbstractHistogram.java:559) ~[HdrHistogram-2.2.2.jar:?] at org.HdrHistogram.AbstractHistogram.recordValue(AbstractHistogram.java:467) ~[HdrHistogram-2.2.2.jar:?] at org.HdrHistogram.Recorder.recordValue(Recorder.java:136) ~[HdrHistogram-2.2.2.jar:?] at org.apache.kafka.coordinator.common.runtime.HdrHistogram.record(HdrHistogram.java:100) ~[kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.KafkaMetricHistogram.record(KafkaMetricHistogram.java:128) ~[kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.common.metrics.Sensor.recordInternal(Sensor.java:236) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.metrics.Sensor.record(Sensor.java:197) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.CoordinatorRuntimeMetricsImpl.recordEventQueueTime(CoordinatorRuntimeMetricsImpl.java:284) ~[kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread.handleEvents(MultiThreadedEventProcessor.java:146) [kafka-coordinator-common-4.1.0.jar:?] at org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread.run(MultiThreadedEventProcessor.java:179) [kafka-coordinator-common-4.1.0.jar:?] {code} *Client-Side:* Consumers associated with this (failed) {{GroupCoordinator}} begin to fail their offset commits. The client logs show a continuous loop where the consumer: * Tries to commit and fails with {{This is not the correct coordinator.}} * Receives an error response {{{}NOT_COORDINATOR{}}}. * Disconnects and attempts rediscovery. * Discovers the _exact same_ (broken) broker as the coordinator and repeats the cycle. *Consumer Log - 2025-11-12 at 14:58* {code:java} 2025-11-13 14:58:46.730 WARN [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Offset commit failed on partition SEND_SMS_LOW_PRIORITY-31 at offset 2291423: The coordinator is not available. 2025-11-13 14:58:46.730 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Group coordinator X.X.X.X:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: error response COORDINATOR_NOT_AVAILABLE. isDisconnected: false. Rediscovery will be attempted. 2025-11-12 14:58:46.388 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Requesting disconnect from last known coordinator 10.0.01:9092 (id: 2147483642 rack: null) 2025-11-12 14:58:46.395 INFO [o.apache.kafka.clients.NetworkClient] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Client requested disconnect from node 2147483642 2025-11-12 14:58:46.396 INFO [o.apache.kafka.clients.NetworkClient] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Cancelled in-flight OFFSET_COMMIT request with correlation id 3436277 due to node 2147483642 being disconnected (elapsed time since creation: 12ms, elapsed time since send: 12ms, throttle time: 0ms, request timeout: 30000ms) 2025-11-12 14:58:46.677 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Discovered group coordinator X.X.X.X:9092 (id: 2147483642 rack: null) {code} *Consumer Log - 2025-11-13 at 09:15* {code:java} 2025-11-13 09:15:46.730 WARN [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Offset commit failed on partition SEND_SMS_LOW_PRIORITY-31 at offset 2291423: The coordinator is not available. 2025-11-13 09:15:46.730 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Group coordinator X.X.X.X:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: error response COORDINATOR_NOT_AVAILABLE. isDisconnected: false. Rediscovery will be attempted. 2025-11-13 09:15:46.730 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Requesting disconnect from last known coordinator X.X.X.X:9092 (id: 2147483642 rack: null) 2025-11-13 09:15:46.732 INFO [o.apache.kafka.clients.NetworkClient] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Client requested disconnect from node 2147483642 2025-11-13 09:15:46.949 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Discovered group coordinator X.X.X.X:9092 (id: 2147483642 rack: null) 2025-11-13 09:15:51.958 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Group coordinator X.X.X.X:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: error response COORDINATOR_NOT_AVAILABLE. isDisconnected: false. Rediscovery will be attempted. 2025-11-13 09:15:51.958 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Requesting disconnect from last known coordinator X.X.X.X:9092 (id: 2147483642 rack: null) 2025-11-13 09:15:51.958 INFO [o.apache.kafka.clients.NetworkClient] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Client requested disconnect from node 2147483642 2025-11-13 09:15:51.958 INFO [o.apache.kafka.clients.NetworkClient] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Cancelled in-flight OFFSET_COMMIT request with correlation id 3557487 due to node 2147483642 being disconnected (elapsed time since creation: 4908ms, elapsed time since send: 4908ms, throttle time: 0ms, request timeout: 30000ms) 2025-11-13 09:15:52.086 INFO [SendAggregateTemplateRoute] [] - Grouped 2 messages. 2025-11-13 09:15:52.132 INFO [o.a.k.c.c.i.ConsumerCoordinator] [] - [Consumer clientId=CLIENT_ID_CONSUMER_LOW_PRIORITY, groupId=CONSUMER_GROUP_LOW_PRIORITY] Discovered group coordinator X.X.X.X:9092 (id: 2147483642 rack: null) {code} ---- *Workaround (Temporary Fix):* # Restarting the affected broker to force a new {{GroupCoordinator}} election for the affected consumer groups. # ## After the broker restarts, consumers will once again discover it as Group Coordinator. ---- h2. *NTP* Brokers of Kafka Cluster and Clients Consumers are with NTP service active {code:java} timedatectl Local time: Fri 2025-11-14 11:07:49 -03 Universal time: Fri 2025-11-14 14:07:49 UTC RTC time: Fri 2025-11-14 14:07:49 Time zone: America/Sao_Paulo (-03, -0300) System clock synchronized: yes NTP service: active RTC in local TZ: no {code} ---- h2. *Request for Analysis / Questions* # Does Kafka 4.1.0 require strictly monotonic time for Coordinator histogram/timer logic? # Is histogram corruption (negative values) a known issue in 4.1.X? # Should Kafka validate histogram deltas to prevent negative values? # Is this fixed in newer Kafka releases? # Should CoordinatorRuntime reset its internal state when encountering time irregularities? # Is there a recommended workaround to avoid Coordinator entering this permanent error state?\nComments: - [~squah-confluent] Could you please take a look into this? - {quote}1. Does Kafka 4.1.0 require strictly monotonic time for Coordinator histogram/timer logic? {quote} Yes, but I believe this an implementation error and not by design. {quote}2. Is histogram corruption (negative values) a known issue in 4.1.X?{quote} No, this is the first bug report we've seen. It will impact Kafka versions >= 4.0. {quote}3. Should Kafka validate histogram deltas to prevent negative values? {quote} Yes {quote}4. Is this fixed in newer Kafka releases? {quote} Not yet. This issue has been marked as a blocker for the 4.2 release. 4.2 will contain the fix for it. {quote}5. Should CoordinatorRuntime reset its internal state when encountering time irregularities? {quote} The coordinator should have recovered as soon as the clock stopped going backwards and the internal queues were fully drained. The exception to this is when a __consumer_offsets leadership change happens at the same time and the load operation for a __consumer_offsets partition fails. Then that partition will be marked as failed and the coordinator will not retry loading. This will be visible in metrics under kafka.server:type=group-coordinator-metrics,name=partition-count,state=failed {quote}6. Is there a recommended workaround to avoid Coordinator entering this permanent error state? {quote} Not at the moment. The broker has to be restarted to clear the failure. - Hi [~squah-confluent] , [~dajac], how are you? During our analysis, we identified a potential time-synchronization conflict: the *VMware vCenter periodic time sync* (VMware Tools \"Synchronize time periodically\") might have been competing with the *NTP service running* on the affected virtual machines. As a workaround, we disabled the vCenter time synchronization. Since applying this workaround six days ago, we have not observed any recurrence of the \u201chistogram value negative\u201d exception. That said, we want to emphasize that we cannot confirm with certainty that this was the root cause. It may be coincidental, and the issue might not be directly related to this configuration. Our intention is to share our findings so the Kafka team has additional data points that might assist in evaluating the behavior. If this scenario can indeed impact Kafka\u2019s metrics subsystem, it may be valuable to consider: * improvements to make future Kafka versions more resilient to time shifts or sync conflicts, or * explicit documentation guidance highlighting how time synchronization conflicts can affect Kafka internals, if the team determines this is relevant. We\u2019re glad to provide any additional logs, metrics, or configuration details that could help. Thank you for your support. Best regards, Eliel Gomes", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19887", "url": "https://issues.apache.org/jira/browse/KAFKA-19887"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add separate option for topic regex pattern matching in command-line tools\nDescription: see [https://github.com/apache/kafka/pull/20865#discussion_r2522364991] Currently, the {{--topic}} option in kafka-topics.sh serves dual purposes - it accepts both literal topic names and regex patterns, which causes ambiguity and usability issues. This is particularly problematic for topic names containing dots (.) and other regex metacharacters. *Implementation Plan:* # 4.x: Introduce the new regex option and deprecate regex usage in {{--topic}} # 5.x: Remove regex functionality from {{--topic}} option entirely This improvement could also be extended to all other scripts that support regex patterns for consistency.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19886", "url": "https://issues.apache.org/jira/browse/KAFKA-19886"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Trogdor | Fix order of arguments in assertEquals() in unit tests\nDescription: This sub-task is intended to fix the order of arguments passed in assertions in test cases within the trogdor package. h4.\nComments: No comments.", "output": "Status: Open\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19885", "url": "https://issues.apache.org/jira/browse/KAFKA-19885"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Explicity define either `execution` or `dry-run`when altering the offsets\nDescription: see https://github.com/apache/kafka/pull/20867#discussion_r2520631578\nComments: No comments.", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19884", "url": "https://issues.apache.org/jira/browse/KAFKA-19884"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add tests to ensure Consumer.close() timeouts work as documented\nDescription: This task is to implement tests to show the timing behavior of {{Consumer.close()}} works as documented. See KAFKA-15402.\nComments: No comments.", "output": "Status: Patch Available\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19883", "url": "https://issues.apache.org/jira/browse/KAFKA-19883"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support Hierarchical Transactional Acknowledgments for Share Groups\nDescription: Add transactional acknowledgment support to Kafka Share Groups to enable +*exactly-once read semantics*+ in distributed stream processing engines like Apache Spark and Apache Flink. The exactly-once guarantee comes from transactional acknowledgments ensuring no record is permanently acknowledged until the checkpoint commits, *Current Behavior:* Share Groups support only immediate acknowledgment mode where records are permanently acknowledged as soon as consumer.acknowledge() is called. This creates data loss scenarios in distributed streaming frameworks: 1. Worker polls and acknowledges records to Kafka 2. Records are permanently removed from Kafka 3. Checkpoint/batch fails before writing to sink 4. Records are lost (acknowledged but never reached sink) This prevents exactly-once semantics in Spark/Flink with Share Groups. *Expected Behavior:* Share Groups should support transactional acknowledgment mode where: 1. Records are acknowledged within transactions 2. Acknowledgments are pending until transaction commits 3. Transaction rollback makes ALL records available for retry 4. Enables atomic coordination with framework checkpoints/batches *Proposed Solution:* Implement hierarchical two-phase commit transactions: {quote} BatchTransaction (coordinator/driver level) \u251c\u2500 TaskTransaction1 (worker/executor level) \u2502 \u251c\u2500 acknowledge(record1, ACCEPT) \u2502 \u251c\u2500 acknowledge(record2, ACCEPT) \u2502 \u2514\u2500 prepare() \u2190 Phase 1 \u2502 \u251c\u2500 TaskTransaction2 (worker/executor level) \u2502 \u251c\u2500 acknowledge(record3, ACCEPT) \u2502 \u2514\u2500 prepare() \u2190 Phase 1 \u2502 \u2514\u2500 commit() \u2190 Phase 2: Atomically commits all task transactions {quote} - Batch transactions (coordinator-level) - Task transactions (worker-level) - Records stay in Kafka until transaction commits - Rollback on failure \u2192 no data loss Example Recovery after failure: ``` Example: Recovery After Failure Checkpoint CP1 Processing: Time 10:00 - Consumer 1 polls, gets records [A, B, C] (offsets 1000, 1001, 1002) Time 10:01 - Consumer 2 polls, gets records [D, E, F] (offsets 1003, 1004, 1005) Time 10:02 - Consumer 1 acknowledges A, B, C in transaction \"checkpoint-CP1\" Time 10:03 - Consumer 2 acknowledges D, E, F in transaction \"checkpoint-CP1\" Time 10:04 - Consumer 1 crashes before prepare() Broker State: Transaction \"checkpoint-CP1\": ACTIVE (not prepared) Records in transaction: - A (offset 1000): LOCKED \u2192 transaction lock - B (offset 1001): LOCKED \u2192 transaction lock - C (offset 1002): LOCKED \u2192 transaction lock - D (offset 1003): LOCKED \u2192 transaction lock - E (offset 1004): LOCKED \u2192 transaction lock - F (offset 1005): LOCKED \u2192 transaction lock Recovery (JobManager detects failure): Time 10:05 - JobManager aborts checkpoint CP1 Time 10:06 - Rollback transaction \"checkpoint-CP1\" \u2192 A, B, C, D, E, F become AVAILABLE again Time 10:07 - Restore from checkpoint CP1-1 Time 10:08 - Consumer 1 (new instance) polls \u2192 Kafka returns available records \u2192 Might get [A, D, B] (different order than before!) Time 10:09 - Consumer 2 polls \u2192 Might get [C, E, F] ``` *API Changes:* New Consumer API: // Batch-level transaction BatchTransaction beginBatchTransaction(String batchTransactionId); void commitBatchTransaction(BatchTransaction txn, Duration timeout) throws TimeoutException, TransactionException; void rollbackBatchTransaction(BatchTransaction txn); // Task-level transaction TaskTransaction beginTaskTransaction(BatchTransaction parent, String taskId); void acknowledge(TaskTransaction txn, ConsumerRecord<K,V> record, AcknowledgeType type); void prepareTaskTransaction(TaskTransaction txn); void commitTaskTransaction(TaskTransaction txn); void rollbackTaskTransaction(TaskTransaction txn); *New Configuration:* share.acknowledgement.mode=transactional # Default: explicit share.batch.transaction.timeout.ms=300000 share.task.transaction.timeout.ms=120000 share.transaction.commit.timeout.ms=30000 share.transaction.auto.rollback=true *Target Use Cases:* Apache Spark structured streaming, Apache Flink exactly-once checkpoints, any coordinator-worker streaming framework.\nComments: - Ref: https://issues.apache.org/jira/browse/SPARK-54321 https://issues.apache.org/jira/browse/FLINK-38287 - Since Apache Kafka 4.1.0 is already released, I fixed the affected version to 4.2.0. - https://dlcdn.apache.org/kafka/4.1.0/RELEASE_NOTES.html - Thanks for the issue. This certainly will require a KIP. - [~schofielaj] , can you please help in creating KIP ? I was not able to create one here [https://cwiki.apache.org/confluence/] - Start [https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals] to create a KIP. This would be a complicated KIP and it will need to cover how to introduce transactions into the share coordinator. There will be changes to the Java API and also to the Kafka protocol. I do anticipate adding transaction support to share groups one day, but in terms of my priorities, it is behind dead-letter queues. - [~schofielaj] , somehow I am not able to login to confluence and requested for new account but did not get any mail back for account creation.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19882", "url": "https://issues.apache.org/jira/browse/KAFKA-19882"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: JMX tags applied to all client metrics, not just client state for KIP-1091\nDescription: When working on [KIP-1091|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1091%3A+Improved+Kafka+Streams+operator+metrics], we mistakenly applied the `process-id` tag to all client-level metrics, rather than just the `client-state`, `thread-state`, and `recording-level` metrics as specified in the KIP. This issue came to light while working on KIP-1221, which aimed to add the `application-id` as a tag to the `client-state` metric introduced by KIP-1091.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19881", "url": "https://issues.apache.org/jira/browse/KAFKA-19881"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Docker Image CVE Scanner workflow consistently failing\nDescription: That workflow (https://github.com/apache/kafka/actions/workflows/docker_scan.yml) has been failing since February. Is this workflow useful? If so should it gate releases?\nComments: - The exit-code is 1 ([https://github.com/apache/kafka/blob/f685d57f2cb1bcbedcf37d0e2e1cd577fb6ef594/.github/workflows/docker_scan.yml#L39]). From [trivy-action document|https://github.com/aquasecurity/trivy-action], it returns this value when finding vulnerabilities. Currently, it scans released images. However, the released images cannot be changed. IMO, we can build temporary image and only scan this image on branch head like trunk, 4.1, 4.0, and 3.9.", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19880", "url": "https://issues.apache.org/jira/browse/KAFKA-19880"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: First batch in producer epoch can be appended out-of-order even for idempotent producer\nDescription: I observe the following behavior. A Java producer with enable.idempotence=true and max.in.flight.requests.per.connection=5 sends a number of records, enough to form several batches in several Produce requests. The topic is empty, freshly created. {code:java} [2025-11-11 11:41:19,125] DEBUG [Producer clientId=producer-1] Assigned producerId 0 and producerEpoch 0 to batch with base sequence 0 being sent to partition input-topic-0 (org.apache.kafka.clients.producer.internals.RecordAccumulator) [2025-11-11 11:41:19,132] DEBUG [Producer clientId=producer-1] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=13, clientId=producer-1, correlationId=21, headerVersion=2) and timeout 30000 to node 3: {acks=-1,timeout=30000,partitionSizes=[4rrMdxpZQnyDKO4QKjPq-A:input-topic-0=16383]} (org.apache.kafka.clients.NetworkClient) [2025-11-11 11:41:19,133] DEBUG [Producer clientId=producer-1] Assigned producerId 0 and producerEpoch 0 to batch with base sequence 1458 being sent to partition input-topic-0 (org.apache.kafka.clients.producer.internals.RecordAccumulator) [2025-11-11 11:41:19,133] DEBUG [Producer clientId=producer-1] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=13, clientId=producer-1, correlationId=22, headerVersion=2) and timeout 30000 to node 3: {acks=-1,timeout=30000,partitionSizes=[4rrMdxpZQnyDKO4QKjPq-A:input-topic-0=16377]} (org.apache.kafka.clients.NetworkClient) [2025-11-11 11:41:19,133] DEBUG [Producer clientId=producer-1] Assigned producerId 0 and producerEpoch 0 to batch with base sequence 2823 being sent to partition input-topic-0 (org.apache.kafka.clients.producer.internals.RecordAccumulator) [2025-11-11 11:41:19,133] DEBUG [Producer clientId=producer-1] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=13, clientId=producer-1, correlationId=23, headerVersion=2) and timeout 30000 to node 3: {acks=-1,timeout=30000,partitionSizes=[4rrMdxpZQnyDKO4QKjPq-A:input-topic-0=16377]} (org.apache.kafka.clients.NetworkClient) [2025-11-11 11:41:19,133] DEBUG [Producer clientId=producer-1] Assigned producerId 0 and producerEpoch 0 to batch with base sequence 4188 being sent to partition input-topic-0 (org.apache.kafka.clients.producer.internals.RecordAccumulator) [2025-11-11 11:41:19,135] DEBUG [Producer clientId=producer-1] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=13, clientId=producer-1, correlationId=24, headerVersion=2) and timeout 30000 to node 3: {acks=-1,timeout=30000,partitionSizes=[4rrMdxpZQnyDKO4QKjPq-A:input-topic-0=16377]} (org.apache.kafka.clients.NetworkClient) [2025-11-11 11:41:19,135] DEBUG [Producer clientId=producer-1] Assigned producerId 0 and producerEpoch 0 to batch with base sequence 5553 being sent to partition input-topic-0 (org.apache.kafka.clients.producer.internals.RecordAccumulator) [2025-11-11 11:41:19,135] DEBUG [Producer clientId=producer-1] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=13, clientId=producer-1, correlationId=25, headerVersion=2) and timeout 30000 to node 3: {acks=-1,timeout=30000,partitionSizes=[4rrMdxpZQnyDKO4QKjPq-A:input-topic-0=16377]} (org.apache.kafka.clients.NetworkClient){code} The cluster and the target broker are experiencing some connectivity issue affecting metadata propagation. The target broker doesn't think it's the leader of the partition for some time, so it rejects first requests: {code:java} [2025-11-11 11:41:20,456] DEBUG [KafkaApi-3] Produce request with correlation id 21 from client producer-1 on partition 4rrMdxpZQnyDKO4QKjPq-A:input-topic-0 failed due to org.apache.kafka.common.errors.NotLeaderOrFollowerException (kafka.server.KafkaApis) [2025-11-11 11:41:24,876] DEBUG [KafkaApi-3] Produce request with correlation id 22 from client producer-1 on partition 4rrMdxpZQnyDKO4QKjPq-A:input-topic-0 failed due to org.apache.kafka.common.errors.NotLeaderOrFollowerException (kafka.server.KafkaApis) {code} Then it finds out it's the leader of the partition, but doesn't yet learn about the other replica: {code:java} [2025-11-11 11:41:45,498] ERROR [ReplicaManager broker=3] Error processing append operation on partition 4rrMdxpZQnyDKO4QKjPq-A:input-topic-0 (kafka.server.ReplicaManager) org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR : 1 is insufficient to satisfy the min.isr requirement of 2 for partition input-topic-0, live replica(s) broker.id are : Set(3) [2025-11-11 11:41:45,498] DEBUG [KafkaApi-3] Produce request with correlation id 23 from client producer-1 on partition 4rrMdxpZQnyDKO4QKjPq-A:input-topic-0 failed due to org.apache.kafka.common.errors.NotEnoughReplicasException (kafka.server.KafkaApis){code} And finally when it comes to the request with correlation ID 24, it succeeds. So the first written batch is logically out-of-sequence, but accepted. The previously failed batches may be retried later, with new producer epoch (like happened in my case), but this doesn't matter: the intended order of records is violated. I'm able to live reproduce this quite reliably under [Antithesis|https://antithesis.com/]. Probably, it won't be easy to make a local setup to catch this with good frequency. But I boiled this down to the following unit test (in ReplicaManagerTest): {noformat} @Test def testX(): Unit = { val mockLogMgr = TestUtils.createLogManager(config.logDirs.asScala.map(new File(_))) val node = new Node(0, \"host0\", 0) mockGetAliveBrokerFunctions(metadataCache, Seq(node)) val rm = new ReplicaManager( metrics = metrics, config = config, time = time, scheduler = new MockScheduler(time), logManager = mockLogMgr, quotaManagers = quotaManager, metadataCache = new KRaftMetadataCache(config.brokerId, () => KRaftVersion.KRAFT_VERSION_0), logDirFailureChannel = new LogDirFailureChannel(config.logDirs.size), alterPartitionManager = alterPartitionManager, threadNamePrefix = Option(this.getClass.getName)) try { val tp = new TopicPartition(topic, 0) val partition = rm.createPartition(tp) partition.createLogIfNotExists(isNew = false, isFutureReplica = false, new LazyOffsetCheckpoints(rm.highWatermarkCheckpoints.asJava), None) rm.becomeLeaderOrFollower(0, new LeaderAndIsrRequest.Builder(0, 0, brokerEpoch, Seq(new LeaderAndIsrRequest.PartitionState() .setTopicName(topic) .setPartitionIndex(0) .setControllerEpoch(0) .setLeader(config.brokerId()) .setLeaderEpoch(0) .setIsr(Seq[Integer](config.brokerId()).asJava) .setPartitionEpoch(0) .setReplicas(Seq[Integer](config.brokerId()).asJava) .setIsNew(false)).asJava, Collections.singletonMap(topic, topicId), Set(node).asJava).build(), (_, _) => ()) rm.getPartitionOrException(tp) .localLogOrException val topicIdPartition = new TopicIdPartition(Uuid.ZERO_UUID, partition.topicPartition) val producerId = 10L val producerEpoch: Short = 10 val r0 = MemoryRecords.withIdempotentRecords(Compression.NONE, producerId, producerEpoch, 0, new SimpleRecord(\"record 0\".getBytes())) val r1 = MemoryRecords.withIdempotentRecords(Compression.NONE, producerId, producerEpoch, 1, new SimpleRecord(\"record 1\".getBytes())) // val order = List(r0, r1) // normal order -- no ordering violation val order = List(r1, r0) // r1 will be accepted out of order; r0 will be retried later with newer epoch and accepted def callback(responseStatus: Map[TopicIdPartition, PartitionResponse]): Unit = { println(responseStatus(topicIdPartition).error) } for (r <- order) { rm.appendRecords( timeout = 1000, requiredAcks = -1, internalTopicsAllowed = false, origin = AppendOrigin.CLIENT, entriesPerPartition = Map(topicIdPartition -> r), responseCallback = callback, ) } } finally { rm.shutdown(checkpointHW = false) } } {noformat} As far as I understand, it isn't necessary for the partition to be empty. It's enough for the producer state to be empty and the first requests in the producer epoch to fail like above. If a first request in the producer epoch succeeds, the situation will not be possible. For example, we send r1, r2, r3. r1 succeeds, r2 fails with NotLeaderOrFollowerException, r3 won't be accepted and will fail with OutOfOrderSequenceException. I attribute this difference to [this validation on the broker side|https://github.com/apache/kafka/blob/3479ce793bafc6a1c42e6afa77e3fbfc3a36c80c/storage/src/main/java/org/apache/kafka/storage/internals/log/ProducerAppendInfo.java#L144-L148]. AFAIU, this is a bug, at least according to how I read the documentation. I would expect enable.idempotence to prevent this from happening. Am I right or it is acceptable and we should improve the documentation instead? Can we require on the broker side that the first batch in a producer epoch always starts with the sequence 0?\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19879", "url": "https://issues.apache.org/jira/browse/KAFKA-19879"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: How to keep compatibility when the configuration validator gets changed?\nDescription: see the discussion: https://github.com/apache/kafka/pull/20334#discussion_r2511260699\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19878", "url": "https://issues.apache.org/jira/browse/KAFKA-19878"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Connect | Fix order of arguments in assertEquals\nDescription: This sub-task is intended to fix the order of arguments passed in assertions in test cases within the connect package.\nComments: No comments.", "output": "Status: Open\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19877", "url": "https://issues.apache.org/jira/browse/KAFKA-19877"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Clarify Deserializer contract around ByteBuffer position, limit\nDescription: We use custom Serde implementations. We improved our Serde to avoid taking {{byte[]}} copies by using the {{deserialize(String topic, Headers headers, ByteBuffer data)}} overload. This ended up introducing a serious bug into a distant module that relied on {{serializedValueSize}} on the consumer record. Per the {{Deserializer}} documentation, we were careful not to assume anything about the capacity etc. However, I think there are some logical gaps in the contract as currently written. Specifically, the deserialize call \"cannot make any assumptions about the returned ByteBuffer like the position\". The contract does not say what you *can* assume - should you assume the data starts at position 0? This seems to be the case but is not explicitly stated. Relatedly, since you cannot assume the {{limit}} means anything, that means all deserializers must have some kind of internal framing or is otherwise able to safely and cheaply ignore extra data past the end of your structure. Kafka itself violates this contract - the default implementation calls {{Utils.toNullableArray}} which calls {{buffer.remaining() }} ({{{}limit - position{}}}) to understand how many bytes are left because what else can you do, without another level of framing? I think this would be better if the deserializer either stated to always start at position 0, or always provide the buffer positioned to the start of the record. If we really cannot rely on {{{}limit{}}}, then the current call to {{toNullableArray}} is wrong, and I am not sure how to fix it - so I think updating the spec to state that {{position}} and {{limit}} can be relied on to frame the data is the best fix here. Additionally, the contract does not say anything about the {{position}} of the buffer *after* the deserializer is done with it. However, this turns out to be very important in calculating the {{serializedValueSize}} - if your deserializer advances the position of the buffer to read the data (which is *really* easy to do with {{{}ByteBuffer{}}}), that takes away from the serialized size, often leaving it as 0 (all bytes consumed by deserialization). Either the {{CompletedFetch}} and {{ShareCompletedFetch}} implementations should be both be updated to use {{limit}} instead of {{{}remaining{}}}, or the {{Deserializer}} should make it very clear that you must not move the {{position}} pointer.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19876", "url": "https://issues.apache.org/jira/browse/KAFKA-19876"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Replace the base image since openjdk image is deprecated\nDescription: see [https://github.com/docker-library/openjdk/issues/505] and https://hub.docker.com/_/openjdk\nComments: - 4.1: [https://github.com/apache/kafka/commit/fcaf3ba5563702d7d0e295dd049e78692c7be5f7] - 4.0: [https://github.com/apache/kafka/commit/5aafa9ad31f3c0c1d98882681dd2695395604fbb] - Summary: 1) the new base image is `eclipse-temurin:17-jdk-jammy` for 4.x and eclipse-temurin:8-jdk-jammy for 3.9 2) use symbolic link instead of `PATH` to expose the java path", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19875", "url": "https://issues.apache.org/jira/browse/KAFKA-19875"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Duplicated topic config prevents broker start\nDescription: KIP-1161 tightened the validation of configurations. Unfortunately, there is a situation in which the tighter validation introduces a blocking issue for AK 4.2. If I create a broker in AK 4.1 and then create a topic with a duplicated value for the cleanup.policy config, the broker will not start with the AK 4.2 code. Here's an example of the topic creation which succeeds in AK 4.1: $ bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic T1 --config cleanup.policy=delete,delete And here's the exception stack trace from the 4.2 broker failing to start: [2025-11-10 09:14:28,730] ERROR Encountered fatal fault: Error starting LogManager (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler) org.apache.kafka.common.config.ConfigException: Configuration 'cleanup.policy' values must not be duplicated. at org.apache.kafka.common.config.ConfigDef$ValidList.ensureValid(ConfigDef.java:1050) at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:540) at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524) at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:118) at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:149) at org.apache.kafka.storage.internals.log.LogConfig.<init>(LogConfig.java:299) at org.apache.kafka.storage.internals.log.LogConfig.fromProps(LogConfig.java:426) at kafka.log.LogManager.$anonfun$fetchTopicConfigOverrides$1(LogManager.scala:590) at kafka.log.LogManager.$anonfun$fetchTopicConfigOverrides$1$adapted(LogManager.scala:586) at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630) at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628) at scala.collection.AbstractIterable.foreach(Iterable.scala:936) at kafka.log.LogManager.fetchTopicConfigOverrides(LogManager.scala:586) at kafka.log.LogManager.startup(LogManager.scala:579) at kafka.server.metadata.BrokerMetadataPublisher.initializeManagers(BrokerMetadataPublisher.scala:344) at kafka.server.metadata.BrokerMetadataPublisher.onMetadataUpdate(BrokerMetadataPublisher.scala:141) at org.apache.kafka.image.loader.MetadataLoader.initializeNewPublishers(MetadataLoader.java:315) at org.apache.kafka.image.loader.MetadataLoader.lambda$scheduleInitializeNewPublishers$0(MetadataLoader.java:272) at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:134) at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:217) at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:188) at java.base/java.lang.Thread.run(Thread.java:1583)\nComments: - Hi [~schofielaj] \uff0c I think I can handle this issue, If you haven't started working on it yet. - I have left it unassigned so feel free to take it. - Ongoing discussion: https://github.com/apache/kafka/pull/20334#discussion_r2504944928 - this will be addressed in the follow-up ([https://github.com/apache/kafka/pull/20844)] also, please take a look at the discussion https://github.com/apache/kafka/pull/20334#discussion_r2507680776 - [~chia7712] Thanks for your kindly reminder. [~m1a2st] Feel free to take on this task yourself if you prefer to handle it.{*}{*}", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19874", "url": "https://issues.apache.org/jira/browse/KAFKA-19874"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix resource leak in TopicBasedRemoteLogMetadataManager initialization\nDescription: ## Problem There is a resource leak in the TopicBasedRemoteLogMetadataManager initialization process. If ProducerManager is created successfully but ConsumerManager creation fails, the ProducerManager instance is not properly closed, leading to resource leaks (e.g., unclosed Kafka producer, threads, network connections). ## Current Code (Problematic) lock.writeLock().lock(); try { producerManager = new ProducerManager(rlmmConfig, partitioner); consumerManager = new ConsumerManager(rlmmConfig, remotePartitionMetadataStore, partitioner, time); consumerManager.startConsumerThread(); // If exception occurs here, producerManager is not closed } catch (Exception e) { log.error(\"Encountered error while initializing producer/consumer\", e); initializationFailed = true; } finally { lock.writeLock().unlock(); }## Solution Use temporary variables to hold manager instances and only assign to instance variables after successful initialization. Clean up temporary resources in the catch block if any exception occurs. ## Impact - Prevents resource leaks during initialization failures - Ensures proper cleanup of Kafka clients - Improves system stability and resource management\nComments: - [~lightzhao] If this change is not planed (as I understand from PR), may be it makes sense to close ticket?", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19873", "url": "https://issues.apache.org/jira/browse/KAFKA-19873"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add explicit liveness check for transactional producers\nDescription: The producer does not have an explicit liveness check like the consumer, which sends periodic heartbeats if it's part of a consumer group. Because there is no \"producer group\" this is fine in general. However, for transactional producers, the missing liveness check has quite some downsides (for example KAFKA-19853). The problem is, that there is only an indirect liveness check via `transaction.timeout.ms` config. The purpose of `transaction.timeout.ms` is to avoid head-of-line blocking for read-committed consumers though, and it's just a side effect that a crashed producer does also hit this timeout eventually, too. The transaction timeout by itself, is not a liveness check. For the Kafka Streams case in particular, to react to a failed producers more quickly, we set an aggressive default transaction timeout of only 10 seconds, allowing the broker to abort a transaction quickly, allowing some other consumer to fetch offset quickly after a rebalance (otherwise, fetching offset is blocked on an open TX \u2013 cf [KIP-447|https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics]). However, in many cases (not limited to Kafka Streams), it is desirable to actually allow transaction to take more time, but this implies that the producer error detection and failover mechanism gets slowed down. For this reason, users are hesitant to increase the transaction timeout, what may fire back by getting TX aborted too aggressively causing unwanted errors (it's particularly problematic for Kafka Streams, because we can't re-use previous `transaction.id` to fence off a pending TX pro-actively, as we moved off EOSv1 to EOSv2 implementation). Thus, for transactional producers, it would make sense to follow the consumer model, which allows for aggressive hard failure detection via `session.timeout.ms` plus longer processing loops via `max.poll.interval.ms` decoupling liveness check and \"max processing\" time. \u2013 We propose to add a new producer `session.timeout.ms` plus a new heartbeat RPC for transactional producers. If a tx-producer has a hard failure and stops sending heartbeats to the broker side transaction coordinator, the coordinator can abort the TX right away without the need to wait for the TX timeout. This allows uses to configure a low session timeout in combination with a larger transaction timeout, providing swift hard error detection plus longer transaction times.\nComments: - Hi [~mjsax], the proposal looks good. If you're not starting to work on it, I can help to write a KIP for it. I'm curious that does this producer heartbeat mechanism only enable when the producer starts a transaction? If there is no transaction and we start this heartbeat mechanism, it may waste bandwidth. - This is interesting but it needs to be done very, very carefully. There's lots of scope for unintended consequences I think. - We had a similar proposal for idempotent producers to heartbeat in order to better enforce idempotency guarantees and expire idempotent producers only when they are no longer heartbeating. Have we considered extending the rpc to idempotent producers as well? - I am not planing to work to work on this. Just filed the ticket. \u2013 Agree to Andrew, that this is a larger piece of work and need careful design and scoping. {quote}I'm curious that does this producer heartbeat mechanism only enable when the producer starts a transaction? {quote} That was the original idea. But seems Justin would like to also do something like this for idempotent producer. {quote}Have we considered extending the rpc to idempotent producers as well? {quote} I did not, but I am open to it. :) - Hopefully I'm not derailing the ticket here but what would be even better in the grand scheme of stream processing would be a producer heartbeat that is somehow accessible from downstream consumers. In Flink (and probably other stream processors), we have a hard time to figure out if a lack of incoming data is originating in a lack of upstream data or temporal producer issues. To trigger certain temporal operations, we would need to assume completeness of data for a window of time, which is much easier to do with a steady stream of input. In case of idle partitions, we need to resort to heuristics which ultimately may result in data loss for cases with temporal producer issues. Having the producer heartbeats available on consumer-side would allow us to distinguish the two cases. (In a way the heartbeat would provide the steady stream of input) At that point, however, it would make sense to also allow non-idempotent producers to emit heartbeats. - Arvid, I agree to the problem, and I would like to support something like this. In Kafka Streams, the \"there is no new data and time does not advance\" issue is also there... But that would be an entirely different feature you are asking for, so let's keep this ticket as-is, as its goal is something totally different than what you are asking for. We can of course file a new ticket for what you are asking for. (I have already ideas how to do this...)", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19872", "url": "https://issues.apache.org/jira/browse/KAFKA-19872"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move DelayedOperations and Partition to server module\nDescription: # As title to move scala class / trait to server module. # Rewrite them in Java.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19871", "url": "https://issues.apache.org/jira/browse/KAFKA-19871"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: KIP-1238: Multipartition for TopologyTestDriver in Kafka Streams\nDescription: see : [KIP-1238|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1238%3A+Multipartition+for+TopologyTestDriver+in+Kafka+Streams]\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19870", "url": "https://issues.apache.org/jira/browse/KAFKA-19870"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support nullable/non-nullable context for shared schemas\nDescription: As discussed in [https://github.com/apache/kafka/pull/20614#discussion_r2482897053], a shared schema currently cannot differentiate between nullable and non-nullable use cases when reused by different fields. This task is to implement a solution (e.g., via a wrapper or schema enhancement) to support this distinction, ensuring generated code and APIs correctly reflect the nullability requirement of each field.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19869", "url": "https://issues.apache.org/jira/browse/KAFKA-19869"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Mark RPCs as stable\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19868", "url": "https://issues.apache.org/jira/browse/KAFKA-19868"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add client id to share consumers in ShareConsumerPerformance\nDescription: Reference - If the thread count is bigger than one, should we add a postfix to the {{{}client.id{}}}? This is necessary because otherwise, the metrics will be mixed up, making observation difficult https://github.com/apache/kafka/pull/18415#discussion_r2494539169\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19867", "url": "https://issues.apache.org/jira/browse/KAFKA-19867"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Broker only node sending UpdateVoteRequest when it can't really become a voter\nDescription: I am in the process of implementing support for controllers scaling within the Strimzi project (running Apache Kafka on Kubernetes) by also using the Apache Kafka code in the current \"trunk\" branch so the future 4.2.0 release because I want to leverage the auto-join feature. When scaling down controllers, the auto-join related documentation mentions that you should first shutdown the controller and later running the remove-controller (via the kafka-metadata-quorum tool, or programmatically in a Kubernetes operator case by using the RemoveRaftVoter via the Admin Client API), otherwise it's pretty clear the node enters in a loop where you remove it but it rejoins automatically again. When managing a Kafka cluster running on bare metal/VMs, this approach works fine even in case the controller scale-down is happening by removing the controller role from a mixed node (shutdown the node, run kafka-metadata-quorum tool to remove-controller, restart the node as broker only). But in a cloud-native environment like Kubernetes, the pod rolling is driven by the platform so there is no way to run a RemoveRaftVoter admin call in between the shutdown and restart. For this reason, the remove-controller is done when the node restarts as broker only. The issue I am facing is that when such a node restarts as broker only, but it's still in the quorum voter (because the remove-controller isn't happened yet), I get the following exception: {code:java} 2025-10-31 08:01:21 TRACE [kafka-1-raft-io-thread] KafkaRaftClient:2899 - [RaftManager id=1] Sent outbound request: OutboundRequest(correlationId=13, data=UpdateRaftVoterRequestData(clusterId='zsn8QaOzTICYZBhUYQpJBg', currentLeaderEpoch=2, voterId=1, voterDirectoryId=ceZ1jCL9DirrUuCxwsv-jw, listeners=[], kRaftVersionFeature=KRaftVersionFeature(minSupportedVersion=0, maxSupportedVersion=1)), createdTimeMs=1761897681990, destination=my-cluster-broker-0.my-cluster-kafka-brokers.myproject.svc:9090 (id: 0 rack: null isFenced: false))2025-10-31 08:01:21 TRACE [kafka-1-raft-io-thread] KafkaRaftClient:2830 - [RaftManager id=1] Received inbound message InboundResponse(correlationId=13, data=UpdateRaftVoterResponseData(throttleTimeMs=0, errorCode=42, currentLeader=CurrentLeader(leaderId=0, leaderEpoch=2, host='my-cluster-broker-0.my-cluster-kafka-brokers.myproject.svc', port=9090)), source=my-cluster-broker-0.my-cluster-kafka-brokers.myproject.svc:9090 (id: 0 rack: null isFenced: false))2025-10-31 08:01:21 ERROR [kafka-1-raft-io-thread] ProcessTerminatingFaultHandler:46 - Encountered fatal fault: Unexpected error in raft IO threadjava.lang.IllegalStateException: Received unexpected invalid request error at org.apache.kafka.raft.KafkaRaftClient.maybeHandleCommonResponse(KafkaRaftClient.java:2679) at org.apache.kafka.raft.KafkaRaftClient.handleUpdateVoterResponse(KafkaRaftClient.java:2569) at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:2737) at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:2836) at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:3680) at org.apache.kafka.raft.KafkaRaftClientDriver.doWork(KafkaRaftClientDriver.java:64) at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:136) {code} It's happening because the node (which is now broker only) is sending a UpdateRaftVoter request (because it sees itself still in the voters list) even if it's not actually a controller and, of course, it's not able to handle the response which is unexpected because it's a broker only node. I think, despite the remove-controller was not done yet, the broker-only node should not send such a request even because in any case it's not able to handle the response so it ends in a \"broken\" code path. The code where it's happening is within the {{KafkaRaftClient.shouldSendUpdateVoteRequest}} where it's not checking the {{canBecomeVoter}} flag before sending the request (here https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java#L3299). Such a check is available in the {{shouldSendAddOrRemoveVoterRequest}} method instead (here https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java#L3355). I think that adding the check would fix the issue because actually the node is not a controller anymore and it can't really become a voter and that flag is, of course, false avoiding the node to send the UpdateRaftVoter request. If accepted, I would be willing to open a PR to fix this.\nComments: - Adding this canBecomeVoter check will not impact/break anything because you should be a voter to enter the `pollFollowerAsVoter` method. If this is because of the not cloud native friendly design for the auto.join voter feature, adding this additional check to workaround the combined-node issue might be an option IMO. Meanwhile, we should still find a better solution for the root cause of KAFKA-19850. - [~mimaison] [~showuon] After our offline discussion I was trying to apply the check on the {{canBecomeVoter}} flag within the {{KafkaRaftClient.pollFollower()}} method to avoid calling the {{pollFollowerAsVoter}} at all, so something like this: {code:java} private long pollFollower(long currentTimeMs) { FollowerState state = quorum.followerStateOrThrow(); if (quorum.isVoter() && canBecomeVoter) { return pollFollowerAsVoter(state, currentTimeMs); } else { return pollFollowerAsObserver(state, currentTimeMs); } } {code} In this case, this change breaks a bunch of tests where a voter is expecting to move in the {{Prospective}} state in order to take part at a new leader election. The tests are the following: {code:java} testHandleEndQuorumRequest testVoterBecomeProspectiveAfterFetchTimeout testHandleVoteRequestAsProspective testFetchResponseIgnoredAfterBecomingProspective testHandleEndQuorumRequestWithLowerPriorityToBecomeLeader {code} Taking the first one as an example. It starts with the local node being part of voters and another node being the leader. The leader sends an {{EndQuorumRequest}} to step back being the leader and a new election should start so the local node is expected to move into {{Prospective}} state. But it happens because the voter sends a fetch request, doesn't get an answer (from leader) and based on the timeout it moves to {{Prospective}}. This happens within the {{pollFollowerAsVoter}} that is not called at all given that {{canBecomeVoter}} is false (by default in the tests). Now, one aspect of this failure is that with the change of checking the {{canBecomeVoter}} flag, we should update the above tests by building the {{RaftClientTestContext}} instance with a {{withCanBecomeVoter(true)}} because the context has that flag set to false by default. It would allow recreating the expected conditions (the node can become a voter, because it's a controller, and makes sense to take part at the leader election) and the tests pass. Despite that, I had a doubt. In the main use case I described in this JIRA, the node was a combined one, and it's now restarting as broker only, so the {{canBecomeVoter}} is false (that flag indicates the presence of \"controller\" role in the node, when the {{KafkaRaftClient}} is created). It's still part of the voters list (because the remove-controller was not called yet) but it's not a controller anymore so it can't take part to a leader election. With the above change, instead of polling as voter, it's going to poll as an observer (the {{pollFollowerAsObserver}} method is called) and I was wondering if it's really correct taking into account that we are in a sort of \"hybrid\" state. The overall cluster sees the node being in the list of the voters but it's not a controller anymore and can't vote. Polling as an observer means that it's going to call {{shouldSendAddOrRemoveVoterRequest}} which anyway avoid the node to make this request because the {{canBecomeVoter}} is false (and it's checked there). My question is ... is it really an observer? If in this timeframe you request the quorum status, you will see it as voter and not in the observers list until making the remove-controller. Is it acceptable? Anyway, I tested that change by building a custom Apache Kafka 4.2 container image for Strimzi and it works fine. In conclusion I think the change makes sense, together with adapting the tests (to recreate the expected conditions) but I wanted insights from you experts first. - > My question is ... is it really an observer? If in this timeframe you request the quorum status, you will see it as voter and not in the observers list until making the remove-controller. Is it acceptable? You are totally right. This is definitely not a correct state for sure. I think the right way to do is to fail start the node because it is not in the correct state. But as I said earlier, if this is because of the not cloud native friendly design for the auto.join voter feature, adding this additional check to workaround the combined-node issue might be a possible temporary option. However, if there is a user really having a bad setting to accidentally change the combined-role node into a broker-role only node, there might be no exception thrown and the quorum is basically running with one voter missing state. So, I don't really know what is the right way to go for v4.2.0. Choose the \"right way\" to throw the exception as is, or adopt the temporary solution to workaround the auto.join issue? Maybe the former is better? I'd like to hear other thoughts on this.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19866", "url": "https://issues.apache.org/jira/browse/KAFKA-19866"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support transactional producer in console producer\nDescription: Console producer is the producer a CLI to allow users to try to write data to kafka. But currently it doesn't support transactional producer. We should add this support in console producer.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19865", "url": "https://issues.apache.org/jira/browse/KAFKA-19865"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Document queues metrics changes in ops.html\nDescription: Given that KIP-1103 introduces additional queue metrics, a new 'Queues Monitoring' section should be added under 6.7 Monitoring in ops.html. !image-2025-11-05-09-51-10-496.png!\nComments: - Hi [~apoorvmittal10] [~schofielaj] , Before started, I'd like to hear your thoughts that do you think this documentation update is necessary or is anyone already working on this? Thanks. - Hmmm, it might be a good idea to have a section covering `Share Group Monitoring`. - [~apoorvmittal10] Rogar that. - [~jimmywang611] Do you think you'll be able to get this done before the feature freeze? - Because this is a docs change, it can be done after the feature freeze if strictly necessary. Of course, it would be better to get it done sooner rather than later.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19864", "url": "https://issues.apache.org/jira/browse/KAFKA-19864"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Handle TimeoutException from initializeIfNeeded() in StateUpdater Code\nDescription: Identified [here|https://github.com/apache/kafka/pull/20818/files#r2490887612] When using the StateUpdater mode, tasks that throw TimeoutException during initializeIfNeeded() are not handled, making it inconsistent compared to the non-StateUpdater mode. [Link to Code|https://github.com/apache/kafka/blob/55764f837b744dcaf2295f4af5170bf25c2f48e9/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java#L1080]\nComments: No comments.", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19863", "url": "https://issues.apache.org/jira/browse/KAFKA-19863"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add documentation on how to implement a custom exception handler\nDescription: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1034%3A+Dead+letter+queue+in+Kafka+Streams] introduced DLQs, which in turn introduced a new reason to implement a custom exception handler. We should update the documentation to explain how to implement one correctly\nComments: - Hi. I'm interested in this issue. Would it be okay if I take this ticket? - Hi [~moonyoung] yes, please", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19862", "url": "https://issues.apache.org/jira/browse/KAFKA-19862"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Group coordinator loading may fail when there is concurrent compaction\nDescription: For consumer and streams groups, we reject replay of {{Consumer/StreamsGroupCurrentMemberAssignment}} records when we detect a partition / task is already owned by another member. During group coordinator load, we replay the records in {{{}__consumer_offsets{}}}. When compaction is running concurrently, we can load uncompacted data, followed by a newly swapped in compacted segment, followed by the uncompacted head of the log. This allows for situations where the record unassigning a partition/task is missed during loading. eg. We can load a record \\{ Member A is assigned partition X }, then miss the record \\{ Member A is unassigned partition X }, then load the record \\{ Member B is assigned partition X }, which fails with an exception like {{[GroupCoordinator id=2] Failed to load metadata from __consumer_offsets-4 with epoch 10 due to java.lang.RuntimeException: Replaying record CoordinatorRecord(key=ConsumerGroupCurrentMemberAssignmentKey(groupId='...', memberId='ZxHk7W53S_aHFdpxYc-_Jw'), value=ApiMessageAndVersion(ConsumerGroupCurrentMemberAssignmentValue(memberEpoch=854659, previousMemberEpoch=854633, state=0, assignedPartitions=[TopicPartitions(topicId=9lL1aTMuSC22QAXsHgzhew, partitions=[1, 2]), TopicPartitions(topicId=RHKM682KQYyOfF1XsOSF1A, partitions=[0]), TopicPartitions(topicId=rKx9q1JmS1uP-ug_cj56ug, partitions=[0]), TopicPartitions(topicId=I7EtFwesTRubnj-VHClqbQ, partitions=[2]), TopicPartitions(topicId=ydAln6IUTZe-od9UUkn3rg, partitions=[2])], partitionsPendingRevocation=[]) at version 0)) from __consumer_offsets-4 at offset 3889549 with producer id -1 and producer epoch -1 failed..}} {{java.lang.IllegalStateException: Cannot set the epoch of RHKM682KQYyOfF1XsOSF1A-0 to 854659 because the partition is still owned at epoch 853490}}\nComments: No comments.", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19861", "url": "https://issues.apache.org/jira/browse/KAFKA-19861"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optimization of the Murmur2 hash computation\nDescription: The Murmur2 hash computation can be significantly sped up by utilizing Java's VarHandle to fetch multiple bytes simultaneously. see [PR|https://github.com/apache/kafka/pull/20359]\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19860", "url": "https://issues.apache.org/jira/browse/KAFKA-19860"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implement integration tests for share group lag\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19859", "url": "https://issues.apache.org/jira/browse/KAFKA-19859"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bug: Start offset moved incorrectly on receiving acknowledgements for offset post LSO movement\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19858", "url": "https://issues.apache.org/jira/browse/KAFKA-19858"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add configurable min.insync.replicas (default=2) for __remote_log_metadata topic\nDescription: The __remote_log_metadata internal topic currently lacks a configurable min.insync.replicas setting, relying on the broker-level default (typically 1). This creates a data loss risk in production environments, as writes may be acknowledged by only a single replica. This is inconsistent with __transaction_state, which explicitly sets min.insync.replicas=2 via the transaction.state.log.min.isr broker configuration. Both topics store critical metadata and should have equivalent durability guarantees.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19857", "url": "https://issues.apache.org/jira/browse/KAFKA-19857"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CoordinatorExecutorImpl.cancelAll always throws IllegalStateException when there are running tasks\nDescription: We call iterator.remove() without a preceding iterator.next() when there are running tasks and remove() is defined to throw an IllegalStateException if the next method has not yet been called. This causes unclean unloads of group coordinator partitions when there is a running regex resolution for example.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19856", "url": "https://issues.apache.org/jira/browse/KAFKA-19856"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Stabilize ClientUtilsTest.testParseAndValidateAddressesWithReverseLookup\nDescription: We receive a bunch of mails saying ClientUtilsTest.testParseAndValidateAddressesWithReverseLookup is failed It seems the reversed addresses get different due to region. Maybe we should list all possible addresses for that test\nComments: - [~mimaison] it seems the CI is using your branch \u201ctest-PowerPC\u201d. Could you please take a look? Maybe it can be fixed by backporting KAFKA-18546 - Maybe we can just disable the ppc64le CI. We've requested ppc64le runners for GitHub Actions in https://issues.apache.org/jira/browse/INFRA-26198 but it's taking a very long time. - I have disabled the CI https://ci-builds.apache.org/job/Kafka/view/All/job/Kafka%20PowerPC%20Daily/", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19855", "url": "https://issues.apache.org/jira/browse/KAFKA-19855"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix typo in README.md file\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19854", "url": "https://issues.apache.org/jira/browse/KAFKA-19854"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: update the docs of AcknowledgementCommitCallback\nDescription: It may be executed by `poll`, `commitSync`, `commitAsync`, and `close`\nComments: - I'm working on this :)", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19853", "url": "https://issues.apache.org/jira/browse/KAFKA-19853"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Transaction Failure when StreamThread blocks on StateUpdater during onAssignment()\nDescription: We've observed that the `StreamThread` blocks waiting for a `Future` from the `StateUpdater` in the `StreamsPartitionAssigner#onAssignment()` method when we are moving a task out of the `StateUpdater` and onto the `StreamThread`. This can cause problems because, during restoration or with warmup replicas, the `StateUpdater#runOnce()` method can take a long time (upwards of 20 seconds) when RocksDB stalls writes to allow compaction to keep up. In EOS this blockage may cause the transaction to time out, which is a big mess. This is because the `StreamThread` may have an open transaction before the `StreamsPartitionAssignor#onAssignment()` method is called. Some screenshots from the JFR below (credit to [~eduwerc]).\nComments: - Thanks for report, [~coltmcnealy-lh] ! If RocksDB stalls for 20s, and we have a transaction timeout of 10s, I wonder if this wouldn't be a problem on its own. Couldn't these stalls also cause transaction timeouts during regular processing? Are you using a custom RocksDB config that may cause this? We indeed attempted to make this non-blocking, but it turned out to introduce many difficult race conditions. - [~lucasbru] Hi Lucas\u2014the write stalls only occur on RocksDB instances being updated by the `StateUpdater`. We don't have any fancy RocksDB configs that would cause this. RocksDB by default (which we haven't touched) slows down writes when you reach 20 L0 files, and stops writes when you reach 36 L0 files. When you have full-speed write as fast as you can, RocksDB compaction (either with the default KS configs or with an optimized config that performs better) struggles to keep up, so the RocksDB instances under the state updater normally stall writes. In fact, one thing the `ChangelogReader` does exacerbates this: it flushes the RocksDB store every 10k records, which means we end up rapidly accumulating a bunch of small (1MB) files in L0. As to your first question, doesn't this cause problems during normal processing? The answer is, it doesn't, because the rate of writing to the normal stores (active tasks) is much lower and we also don't flush every 1MB or so, so the rate of flushing files is dramatically lower, which means RocksDB does not stall writes enough. Separately, it is my hope that KIP-1035 will allow us to no longer flush manually in the `ChangelogReader.`In my tests when I disabled this manual flushing (it was a hack...don't judge) the restoration throughput improved 4x. In most cases, during restoration your throughput is bottlenecked by Disk Bandwidth (used up by compaction), and reducing the rate of flushing by allowing the RocksDB WriteBufferManager to flush whenever it needs to can dramatically reduce the intensity of compactions. - [~lucasbru] also, in our soak tests we were dealing with 500GB+ of compressed data with a roughly 80% compression ratio, which means close to 2.5TB total data without compression. We cannot reproduce this bug with smaller amounts of state. With that setup, and limited CPU resources, it was perfectly reliable to reproduce the bug. I made a few 'hack' changes on a private branch and was able to make it work ~90% of the time by: * Flushing once every 1M records instead of 10k records in the ChangelogReader (`StateManagerUtil.java`) to reduce the number of L0 files * Calling `taskManager.commitAll()` in the beginning of the `StreamsPartitionAssignor#onAssignment()` method which helped quite a bit. - Yeah, this is a valid ticket, but I am not sure whether we can fix it by making the state updater task hand-off non-blocking. Q: In the non-state updater code-path, wouldn't we also stall for 20s and thereby run into the transaction timeout? Since rebalances should be fairly infrequent and commits are pretty frequent, I am actually not sure why we don't just commit all tasks at the beginning of the rebalance. IIRC, we only commit the ones that are being revoked, but in EOSv2 we need to commit them all anyway. So your solution of just committing every time there is a rebalance may not be such a bad idea. - Yes, the StateUpdater being slow can cause the StreamThread to block for 20s+ and therefore run into a transaction timeout. Our soak got into a place where it couldn't recover, which was unfortunate. Committing all tasks before the start of the rebalance seems like a good plan (: - What I meant was - why wasn't this the case in the old code path (or was it?) - Oh, sorry, I misunderstood your question (didn't realize you were talking about before SU). That's a good question\u2014I confess that our soak tests in mid-2024 (around 3.8) were not up to par, so we didn't run into this. However, let's look at the two cases when we have unbridled writes into RocksDB: 1) we lose Instance X, so Instance X's standby's get rescheduled onto Instance Y as standby's / warmups. 2) active task restoration. In the old code path, for Case 1, the writes into RocksDB would occur in the main processing loop and as such may have been slowed down by the processing logic (which has to do deserialization / serialization for the store, etc). I'm not sure if that would be enough, though. For case 2), there was a ticket that was similar to this problem but not exactly the same: a transaction that wasn't closed: https://issues.apache.org/jira/browse/KAFKA-13295 I do like your idea of committing any open transactions at the start of a rebalance, so long as it's possible. - Yes, I remember that bug. And you are right, that is a very similar problem. I think you bug is in the end a variation of 13295 , that can happen in rather extreme circumstances (I hope at least, that 20s stalls are not to common, unless you have huge state and compression). - I am not sure if committing by itself would really solve it? Even during a rebalance, we are getting new data from `poll()` for all partitions/tasks which are not revoked, and we keep processing. So if we commit, we would also need to ensure to not process data and not open a new TX? \u2013 However, it seems to defeat the purpose of incremental rebalancing to stop processing during a rebalance? Especially with KIP-1071, it seems the handler should just tell the SU what to do, and whenever the SU is ready, we would updates some later HB response with the reconciliation update? - > Even during a rebalance, we are getting new data from `poll()` for all partitions/tasks which are not revoked, and we keep processing. That's true; however, the problem is that the `StreamsPartitionAssignor#onAssignment()` callback executes on the stream thread (as it calls `poll()`) and blocks on a `Future` which waits on the State Updater. So, during that `onAssignment()` callback, the whole Stream Thread is parked (see the JFR's above), and that extended blocking causes transactions to time out. - Yes. As colt says, it's just about the time when the stream thread is blocked, and there is no processing in that timeframe. I think committing would help. But for me the root cause is the write stall. If we have a transaction timeout of 10s and writing to rocks DB takes 20s, we are in trouble. Would kip-1035 resolve the write stalls? - KIP-1035 would _help_ the write stalls but not remove them. Note that the write stalls are not \"global\" but rather per-rocksdb instance, because (in the case I observed) they were specifically caused by the number of L0 files in the RocksDB for the state store under restoration. Active tasks did not experience stalls or even slowdowns because compaction was easily able to keep up with the write rate. As an experiment, I disabled manual flushing (changing `StateManagerUtil.OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT` to 1M instead of 10k). The results: * I still got write stalls on the restoring tasks + warmup replicas, but they were less bad. * Restoration throughput improved 3x (from 4MB/s to 12MB/s) with identical conditions as measured by restore consumer throughput. - I have opened a PR which addresses this specific issue (the transaction timeout under EOS) and have tested it in our soak test with ~600GB state. Feedback would be appreciated (: [https://github.com/apache/kafka/pull/20833/files] - Updating here\u2014I've tried forcefully stopping instances, rolling bounces, and adding instances back into the cluster with this patch, and I was unable to reproduce the problem of transaction failures.", "output": "Status: Patch Available\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19852", "url": "https://issues.apache.org/jira/browse/KAFKA-19852"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: MetadataShell read 0000-0000.checkpoint will cause infinite loop\nDescription: \nComments: - The zero checkpoint does not contain any leaderChange record so the behaviour is expected.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19851", "url": "https://issues.apache.org/jira/browse/KAFKA-19851"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Delete dynamic config that were removed by Kafka\nDescription: [KIP-724: Drop support for message formats v0 and v1|https://cwiki.apache.org/confluence/display/KAFKA/KIP-724%3A+Drop+support+for+message+formats+v0+and+v1] and Kafka 4.0.0 removed support for the dynamic configs like message.format.version. When the user upgrades from 3.x to 4.x it can cause the dynamic configuration validation to fail if there are dynamic configurations that are no longer supported. {code:java} Caused by: org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: message.format.version {code} One solution for solving this problem is to have a well define list of allowed dynamic configuration. Any config that doesn't match this list will be automatically deleted. This deletion can be done implicitly when applying the ConfigRecord record. This behavior can be further guarded by checking that the metadata version is greater than 4.0. This also means that upgrading the metadata version to 4.0 would cause all of the removed config to get deleted.\nComments: - Another option would be to ignore such configs, but keep the configs in the log.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19850", "url": "https://issues.apache.org/jira/browse/KAFKA-19850"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: KRaft voter auto join will add a removed voter immediately\nDescription: In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller: {noformat} controller.quorum.auto.join.enable: Controls whether a KRaft controller should automatically join the cluster metadata partition for its cluster id. If the configuration is set to true the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat} This \"shutdown the to-be-removed controller first\" operation might break the quorum in the worst case. For example, 3 controller nodes quorum (C1, C2, C3), C1 is the leader, C3 is already caught up with C1, C2 is still catching up with the leader. When users want to remove C3, following the guide, users shutdown the C3 first. But at this point of time, the quorum is broken and the kafka cluster is basically unavailable. Furthermore, this is not a user friendly behavior. And it will cause many confusion to users and thought there is something wrong in the controller removal. Besides, In the kubernetes environment which is controlled by the operator, it is not the cloud native way to shutdown a node, do some operation, then start it up. So, I propose we can improve it by \"the removed controller will not be auto joined before this controller restarted\". That is: 1. Once the controller is removed from voters set, it won't be auto joined even if `controller.quorum.auto.join.enable=true` 2. The controller can be manually join the voters in this state 3. The controller node will be auto join the voters set after node restarted. So in short, the semantics of auto join is updated as \"a node will be auto-joined only when node startup\". I think it makes more sense to users. Thoughts?\nComments: - [~kevinwu2412] [~chia7712] [~jsancio] , any thought on this? - If we only allow anto-join voter to join the cluster once (it can be retried until success), is it simpler than using removed voter RPC? - Do you mean the auto-join can only join the cluster once? But if the node is already a controller, and now it gets removed by remote voter RPC, it will auto join the voters immediately, which doesn't fix the problem. - We are seeing the same UX issue while implementing the dynamic quorum support with controllers scaling within Strimzi (running Apache Kafka on Kubernetes) and specifically when a user starts with some mixed nodes (broker + controller) then scale up by adding dedicated controller nodes but then they want to remove the controller role from the mixed nodes. In such case, the mixed nodes are not shutdown and removed (forever) but they are rolled (without controller role) and, as Luke pointed out, in a cloud native environment there is no such a waiting time to have an operator taking actions (like remove Raft voter call) between the shutdown and the restarting of a pod (which is out of control and done by Kubernetes itself). For this reason, it would be great having the remove Raft voter being called before shutting down the nodes (as opposite what the documentation says when using auto-join) which works fine when auto-join is not enabled. But at the same time the auto-join is a very useful feature that, even more in an automated scenario, helps with controller registration on scaling up. So I would be for allowing the removal of Raft voters before their scale down but avoiding the immediate re-join. The FollowerState class could track the node being removed and this could help skipping the immediate re-join within the shouldSendAddOrRemoveVoterRequest (in the KafkaRaftClient). Of course, the node is able to join again on restart or because the user runs the corresponding command manually. - [~showuon] Sorry for unclear, the following PR is my prototype and I think this idea can work. [https://github.com/TaiJuWu/kafka/pull/58] The main change is only add a additional flag (hasJoin) to KraftClient and it only allow send auto-join one time, with this change, the client will not send AddRaftVoterRequest to leader again. Do I miss any case need to consider? - [~taijuwu], I know what you mean. But if a controller is joined as `--initial-controllers` or `--standalone` controller, it will not be added via addRaftVoterRequest/Response. So that means, when these controllers are removed, it will still be auto-joined. Do we have any way to get the initial controllers? Maybe parse the \"00000000000000000000-0000000000.checkpoint\" and get the initial controller lists? Is that a good solution? - [~showuon] We can get the init voter from KraftControlStateMachine when initializing so we don't need to parse log. Then, we can skip the initial voter to send AddRaftVoter instantly.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19849", "url": "https://issues.apache.org/jira/browse/KAFKA-19849"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move ThrottledChannelExpirationTest to server-common module\nDescription: With *KAFKA-19817 moving all related components to `server-common`, we can rewrite and migrate `ThrottledChannelExpirationTest`. Also, the test that was moved in KAFKA-17372* should be merged into it.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19848", "url": "https://issues.apache.org/jira/browse/KAFKA-19848"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Revert KIP-939 API's and Client Code for 4.2\nDescription: KIP-939 has been paused but some of the client code and new API's have already been merged to trunk. We need to revert the changes in the 4.2 release branch.\nComments: No comments.", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19847", "url": "https://issues.apache.org/jira/browse/KAFKA-19847"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Adjust quorum-related config lower bounds\nDescription: Some config settings are related to each other, or certain configuration constraints haven\u2019t been discussed. KIP: [https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=396790313&draftShareId=4b9d0d2e-beff-450a-8a73-fcb8b0fe805b&] See: [https://github.com/apache/kafka/pull/18998#discussion_r1988001109] [https://github.com/apache/kafka/pull/20318/files#r2465660429]\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19846", "url": "https://issues.apache.org/jira/browse/KAFKA-19846"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix ShareFetch RPC doesn't allow '/' and '+' in memberId field\nDescription: While working on QfK implementation in librdkafka, I found that {{memberId}} field doesn't accept {{/}} and {{{}+{}}}. I get following error on the {*}broker side{*}. {code:java} [2025-10-23 15:10:55,319] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2) -- ShareFetchRequestData(groupId='share-group-1', memberId='/FvjN95PRhGaAuA8aQMuTw', shareSessionEpoch=0, maxWaitMs=500, minBytes=1, maxBytes=52428800, maxRecords=500, batchSize=500, topics=[FetchTopic(topicId=sgd0qCnHRL-t80afMzN9nA, partitions=[FetchPartition(partitionIndex=0, partitionMaxBytes=0, acknowledgementBatches=[])])], forgottenTopicsData=[]) with context RequestContext(header=RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2), connectionId='127.0.0.1:9092-127.0.0.1:41442-0-4', clientAddress=/127.0.0.1, principal=User:ANONYMOUS, listenerName=ListenerName(PLAINTEXT), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=librdkafka, softwareVersion=2.12.0-RC1-13-g82dbc3-dirty-devel-O0), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@1648925]) (kafka.server.KafkaApis) java.lang.IllegalArgumentException: Illegal base64 character 2f at java.base/java.util.Base64$Decoder.decode0(Base64.java:848) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:566) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:589) ~[?:?] at org.apache.kafka.common.Uuid.fromString(Uuid.java:136) ~[kafka-clients-4.1.0.jar:?] at kafka.server.KafkaApis.handleShareFetchRequest(KafkaApis.scala:3157) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaApis.handle(KafkaApis.scala:236) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:158) [kafka_2.13-4.1.0.jar:?] at java.base/java.lang.Thread.run(Thread.java:840) [?:?] {code} *Client* gets {{UNKNOWN_SERVER_ERROR}} as response from the broker. Digging deeper I found that Uuid *{{fromString()}}* uses Base64 Url decoding and similarly *{{toString()}}* uses Base64 Url encoding. {code:java} public static Uuid fromString(String str) { if (str.length() > 24) { throw new IllegalArgumentException(\"Input string with prefix `\" + str.substring(0, 24) + \"` is too long to be decoded as a base64 UUID\"); } ByteBuffer uuidBytes = ByteBuffer.wrap(Base64.getUrlDecoder().decode(str)); if (uuidBytes.remaining() != 16) { throw new IllegalArgumentException(\"Input string `\" + str + \"` decoded as \" + uuidBytes.remaining() + \" bytes, which is not equal to the expected 16 bytes \" + \"of a base64-encoded UUID\"); } return new Uuid(uuidBytes.getLong(), uuidBytes.getLong()); } {code} {code:java} public String toString() { return Base64.getUrlEncoder().withoutPadding().encodeToString(getBytesFromUuid()); } {code} {{I feel that Uuid should use normal Base64 encoding or decoding instead of Url one. If we use Url encoding and decoding then the Uuid itself changes. Generated Uuid and Url Base64 encoding for the Uuid will be different. }}{{}}\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19845", "url": "https://issues.apache.org/jira/browse/KAFKA-19845"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Share consumer changes to support renew ack.\nDescription: \nComments: - [~sushmahajn] can i pick this ? - [~goyarpit] I'm afraid the work on this issue is already in progress so I'm going to take it. If we get some subtasks, you can certainly help. - Thanks [~schofielaj] .", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19844", "url": "https://issues.apache.org/jira/browse/KAFKA-19844"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Modification of share fetch to accommodate piggybacked renewals.\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19843", "url": "https://issues.apache.org/jira/browse/KAFKA-19843"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: SharePartition changes to support renew ack.\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19842", "url": "https://issues.apache.org/jira/browse/KAFKA-19842"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Additional tests in ShareConsumerTest\nDescription: \nComments: - Addressed in [https://github.com/apache/kafka/pull/20838]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19841", "url": "https://issues.apache.org/jira/browse/KAFKA-19841"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix the negative remote bytesLag and segmentsLag metric\nDescription: * Each broker rotates the segment independently. * When the leadership changes to the next replica, then the highest offset uploaded to the remote storage might already be higher than the baseOffset of the current leader active segment. * During this case, the {{onlyLocalLogSegmentsSize}} and {{onlyLocalLogSegmentsCount}} returns 0, so the metrics shows negative value (0 - activeSegment.size).\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19840", "url": "https://issues.apache.org/jira/browse/KAFKA-19840"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded\nDescription: The test has been observed as flaky in the recent runs: [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.sortField=FLAKY&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D] After tracing the failures, the root cause was narrowed down to commit [https://github.com/apache/kafka/commit/87657fdfc721055835f5b1f22151c461e85eab4a]. This change introduced a new try/catch around {{handleCompletedAcknowledgements()}} in {{{}ShareConsumerImpl.java{}}}. The side effect is that all exceptions coming from the commit callback \u2014 including GroupMaxSizeReachedException \u2014 are now swallowed and only logged, preventing the test from ever receiving the exception. - If the ack callback fires while {{poll()}} is executing \u2192 exception is caught & swallowed \u2192 test times out - If the callback fires outside that path \u2192 exception escapes \u2192 test passes So the same test randomly passes/fails depending on scheduling of the ack callback. Also, the test testAcknowledgementCommitCallbackThrowsException has been marked as Flaky for the same Root cause.\nComments: - Will pick this up as part of the KIP-1222 work.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19839", "url": "https://issues.apache.org/jira/browse/KAFKA-19839"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Native-image (dockerimage) does not work with compression.type=zstd\nDescription: When sending records to a topic created like this: {quote} admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1) .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\")))); {quote} With a producer configured with: {quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\"); {quote} The server fails with: {quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager) java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?] ... at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] ... at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] {quote} The file: {{docker/native/native-image-configs/jni-config.json}} seems to have it referred: {quote}{ \"name\":\"com.github.luben.zstd.ZstdInputStreamNoFinalizer\", \"fields\":[\\{ \"name\":\"dstPos\"}, \\{ \"name\":\"srcPos\"}] },{quote}\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19838", "url": "https://issues.apache.org/jira/browse/KAFKA-19838"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Follow up for KIP-1217\nDescription: [KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19837", "url": "https://issues.apache.org/jira/browse/KAFKA-19837"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Follow up for KIP-1188\nDescription: [KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19836", "url": "https://issues.apache.org/jira/browse/KAFKA-19836"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Decouple ConsumerConfig and ShareConsumerConfig\nDescription: ShareConsumerConfig and ConsumerConfig are inherent at the moment. The drawback is the config logic is mixed, for example: ShareAcknowledgementMode. We can decouple to prevent the logic is complicated in the future.\nComments: - Feel free to close if this is not necessary.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19835", "url": "https://issues.apache.org/jira/browse/KAFKA-19835"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: The Content-Security-Policy header must not be overridden\nDescription: [https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13] The Content-Security-Policy header must not be overridden. There is now a standard way to add local exceptions to the CSP: [https://infra.apache.org/tools/csp.html] Please update the .htaccess file accordingly. Please note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file\nComments: - brandboat opened a new pull request, #733: URL: https://github.com/apache/kafka-site/pull/733 The Content-Security-Policy header must not be overridden. There is now a standard way to add local exceptions to the CSP: https://infra.apache.org/tools/csp.html - brandboat commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3447983723 quickstart local dev env screenshot: <img width=\"1725\" height=\"887\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f06abbe0-ec48-428f-b0e0-edf417a915ef\" /> - sebbASF commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448072791 Note that the .htaccess file needs to document why the override is allowed. - brandboat commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448101426 @sebbASF, thanks for the comment! The reason we add youtube here is due to we use embedded youtube videos in QuickStart, KafkaStreams page, see https://kafka.apache.org/quickstart, https://kafka.apache.org/documentation/streams/ Without this, browsers will block these iframes and videos won't display. - sebbASF commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448151327 Your explanation covers why the override is needed. However it does not cover why the override is allowed. According to https://infra.apache.org/tools/csp.html \"Each additional host you add MUST have been pre-approved by VP Data Privacy ([privacy@apache.org](mailto:privacy@apache.org)), and SHOULD have an accompanying comment in the .htaccess file explaining why the CSP is changed and where permission was obtained.\" - brandboat commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448248462 > Each additional host you add MUST have been pre-approved by VP Data Privacy ([privacy@apache.org](mailto:privacy@apache.org)) @sebbASF, thanks again for the comment! Could you please explain how this approval process is supposed to be done? I don\u2019t seem to have access to the email threads at `privacy@apache.org`, so I\u2019m wondering whether this step needs to be handled by an Apache member or a project lead. Sorry if this is a basic question \u2014 I\u2019m just a casual contributor and not very familiar with the internal process. c.c. @chia7712 - sebbASF commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448441347 Such questions should be directed to the Kafka PMC in the first instance - mjsax commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3518961534 Thanks for raising this @sebbASF - chia7712 commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3520349583 > do you want to own it sure, no problem \ud83d\ude03", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19834", "url": "https://issues.apache.org/jira/browse/KAFKA-19834"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Cleanup suppressions.xml\nDescription: Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19833", "url": "https://issues.apache.org/jira/browse/KAFKA-19833"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Refactor Nullable Types to Use a Unified Pattern\nDescription: see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676] Regarding the implementation of the nullable vs non-nullable types. We use 3 different approaches. # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES. # For array, we use one class ArraryOf, which takes a nullable param. # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA. We need to pick one approach to implement all nullable types in a consistent way.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19832", "url": "https://issues.apache.org/jira/browse/KAFKA-19832"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move ClientOAuthIntegrationTest to clients-integration-tests\nDescription: This is continue with the effort to move the clients test to {{clients-integration-tests}} a la KAFKA-19042.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19831", "url": "https://issues.apache.org/jira/browse/KAFKA-19831"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Failures in the StateUpdater thread may lead to inability to shut down a stream thread\nDescription: If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown. See the attached test that reproduces the issue.\nComments: - Hi [~nikita-shupletsov] if you have not started ,can i pick this up ? - Hi [~goyarpit] I am already working on it. I will let you know if I need help. thanks!", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19830", "url": "https://issues.apache.org/jira/browse/KAFKA-19830"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Refactor KafkaRaftClient to use event scheduler framework\nDescription: \nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19829", "url": "https://issues.apache.org/jira/browse/KAFKA-19829"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implement group-level initial rebalance delay\nDescription: During testing, an artifact of the new rebalance protocol showed up. In some cases, the first joining member gets all active tasks assigned, and is slow to revoke the tasks after more member has joined the group. This affects in particular cases where the first member is slow (possibly overloaded in the case of cloudlimits benchmarks) and there are a lot of tasks to be assigned. To help with this situation, we want to introduce a new group-specific configuration to delay the initial rebalance.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19828", "url": "https://issues.apache.org/jira/browse/KAFKA-19828"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Intermittent test failures when using chained emit strategy on window close\nDescription: Hi, I have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue The problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of !https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810 to make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline. Greg\nComments: - Most of the time I run the test, I see this: [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.880 s \u2013 in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3 [INFO] [INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] However, like I mentioned, every once in a while I see this: [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.058 s <<< FAILURE! \u2013 in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3 [ERROR] com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3.testChainedWindowedAggregationsWithDifferentGracePeriods \u2013 Time elapsed: 1.049 s <<< FAILURE! java.lang.AssertionError: expected:<1> but was:<0> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:647) at org.junit.Assert.assertEquals(Assert.java:633) at com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest3.java:116) What that means, most of the time a record passes from input to output topic in the time expected, but every once in a while it does not. - the goal, of course, is to have unit test that is reliable, i.e. doesn't fail intermittently. I need to understand whether the issue is in my test code (attached) or elsewhere in kafka test framework. - I run the test in a loop in a shell script like this: #!/bin/bash set -e for i in \\{1..100}; do echo ======================================================== echo ======================== $i ============================ echo ======================================================== mvn surefire:test -Dtest=ChainedEmitStrategyTopologyTest3 done I am attaching output of one such run, where first 5 iterations succeeded, but the 6th one failed - @[~mjsax] ^^^^ - I am not sure \u2013 did spend some time but don't understand yet what's happening. It could be a bug in Kafka Streams. Needs more investigation. - Thanks. Good you were able to reproduce it. I've been banging my head at it for a week now. Keep me posted. Thanks - Looks like it's because you use real time in the test. if you replace it with something like {{{}Instant flowTime = Instant.ofEpochMilli(0);{}}}, it should fix it - Could you please elaborate why using the real time in the test is a problem as opposed to what you are suggesting. Thanks. I have two windows - one 30 secs, another one 1 hr, 10 min grace I thought by starting my flows 3 hours behind the current real time and sending flows every few seconds I would be ok. Why does my strategy not work? - I poked a bit more, truncating the time to hour also works: {{Instant flowTime = baseTime.minus(3, ChronoUnit.HOURS).truncatedTo(ChronoUnit.HOURS);}} I am not 100% sure why, but my impression so far is that because we aggregate every hour: {{{}TimeWindows.ofSizeAndGrace(Duration.ofHours(1), Duration.ofMinutes(10){}}}, so if we are start from not a round hour, it's possible that the messages we send in the test will not always end up in the windows we expect them to. I still need to spend more time looking at the code to find the exact piece that's responsible for that, so my conclusion may not be 100% accurate, but the empirically achieved results kind of back up my assumptions.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19827", "url": "https://issues.apache.org/jira/browse/KAFKA-19827"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Call acknowledgement commit callback at end of waiting calls\nDescription: The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19826", "url": "https://issues.apache.org/jira/browse/KAFKA-19826"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator\nDescription: Add a new allowed value for group.coordinator.append.linger.ms and share.coordinator.append.linger.ms of -1. When append.linger.ms is set to -1, use the flush strategy outlined in the KIP.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19825", "url": "https://issues.apache.org/jira/browse/KAFKA-19825"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: KIP-1224: Add batch-linger-time and batch-flush-time metrics\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19824", "url": "https://issues.apache.org/jira/browse/KAFKA-19824"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: New ConnectorClientConfigOverridePolicy with allowlist of configurations\nDescription: Jira for KIP-1188: https://cwiki.apache.org/confluence/x/2IkvFg\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19823", "url": "https://issues.apache.org/jira/browse/KAFKA-19823"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions\nDescription: There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}} due to which we are setting {{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than acquiredPartitionsSize\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19822", "url": "https://issues.apache.org/jira/browse/KAFKA-19822"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove all static classes in Field except TaggedFieldsSection\nDescription: All static classes in Field except TaggedFieldsSection are not really being used. We should remove them.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19821", "url": "https://issues.apache.org/jira/browse/KAFKA-19821"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Duplicated batches should be logged\nDescription: When writing records with idempotent producer, the broker can help de-duplicate records. But when records being de-duplicated, there is no log in the broker side. This will confuse the producer because the records sent going to nowhere. Sometimes it's caused by the buggy producer that keeps using the wrong sequence number to cause the duplicated records. We should at least log the duplicated records info instead of pretending nothing happened.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19820", "url": "https://issues.apache.org/jira/browse/KAFKA-19820"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: remove the unnecessary copy from AbstractFetch#fetchablePartitions\nDescription: see https://github.com/apache/kafka/pull/14359#issuecomment-3372367774\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19819", "url": "https://issues.apache.org/jira/browse/KAFKA-19819"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move BrokerMetadataPublisher to metadata module\nDescription: \nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19818", "url": "https://issues.apache.org/jira/browse/KAFKA-19818"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move ClientQuotaMetadataManager to metadata module\nDescription: \nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19817", "url": "https://issues.apache.org/jira/browse/KAFKA-19817"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move DynamicTopicClusterQuotaPublisher to metadata module\nDescription: \nComments: No comments.", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19816", "url": "https://issues.apache.org/jira/browse/KAFKA-19816"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails\nDescription: EosIntegrationTest.shouldNotViolateEosIfOneTaskFails is currently flaky for tests run with the \"streams\" group protocol.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19815", "url": "https://issues.apache.org/jira/browse/KAFKA-19815"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implementation of ShareConsumer.acquisitionLockTimeoutMs() method\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19814", "url": "https://issues.apache.org/jira/browse/KAFKA-19814"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Protocol schema and public API changes\nDescription: This just adds the RPC schema and public API changes to let the implementation progress with multiple teams.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19813", "url": "https://issues.apache.org/jira/browse/KAFKA-19813"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager\nDescription: see https://github.com/apache/kafka/pull/14873#discussion_r2442794887, In below code snippets https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L116 https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L333 we use max.poll.interval.ms (default 300000) as the jitter value. Although RequestState.remainingBackoffMs guards with Math.max(0, \u2026), which won\u2019t make the value become negative, using a jitter that isn\u2019t in (0\u20131) is unexpected. In addition, we should validate that ExponentialBackoff receives a jitter strictly within (0, 1) to prevent this scenario in the future.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19812", "url": "https://issues.apache.org/jira/browse/KAFKA-19812"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Unbound Error Thrown if some variables are not set for SASL/SSL configuration\nDescription: I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file \u2014 it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}. This issue is {*}just an enhancement to properly log the error details{*}.\nComments: - Hi [~scienmanas] , I\u2019d like to take this issue and work on improving the error handling for missing SASL/SSL environment variables in the Docker setup. Could you please assign this to me? - Hi [~crw31] , I guess I don't have access to assign the issues. - Hi [~scienmanas] , thanks for letting me know. Since I don\u2019t have assign permissions, I\u2019ll start working on this. If someone with commit rights wants to assign the ticket later, that would be great. Thanks!", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19811", "url": "https://issues.apache.org/jira/browse/KAFKA-19811"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Acked record on new topic not immediately visible to consumer\nDescription: h2. Steps to reproduce * program uses a single broker (we see the issue with an in-JVM embedded kafka server) * create a new topic with 1 partition * produce a record with {{acks=all}} * await acknowledgement from the broker * start a consumer (configured to read from beginning of topic) * spuriously, _the consumer never sees the record_ The problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop. A delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible. We observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue. h2. Expected behavior A record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be immediately visible for all consumers.\nComments: - Addition: when the consumer misses the just produced record, it will also not see any record produced thereafter either (I didn't wait until a rebalance happened). - [~erikvanoosten] Can you provide the code for the consumer application? I am pretty confident that the problem as described in the issue would have been caught by automated tests. I wonder whether the consuming application design is making this occur. - Yes! The problem happens in the unit test of the open source zio-kafka library. Here are some pointer to tests that sometimes fail to consume any record (they time-out after 2 minutes): * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/SubscriptionsSpec.scala#L22-L54] * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/ConsumerSpec.scala#L170-L203] * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/ConsumerSpec.scala#L250-L289] (there are a few more) These test have in common that they follow the steps described in this issue. - Please let me know if you need help interpreting the code. - I've had a quick look at the code, but I must admit that I'm a bit of a Scala amateur. I suggest using a ConsumerRebalanceListener to ensure that your test has been assigned partitions to consume. Given the symptom about a test failing to receive the first record fails to receive any of them, it sounds like its not been assigned the partition to subscribe from. If this has occurred since the move to Apache Kafka 4.x, it may be related to the difference in behaviour of the `poll(long)` method which was removed and its replacement with `poll(Duration)`. - The problem has been around for some time (before 4.x) although anecdotally it did start to occur more in this year. I will add some debug logging that triggers when no partitions were assigned. - We just had a consumer lockup, where even the 10ms delay after producing was not enough. - [~schofielaj] Under what circumstances could no partition be assigned, even though there is only one broker and one consumer?", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19810", "url": "https://issues.apache.org/jira/browse/KAFKA-19810"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Kafka streams with chained emitStrategy(onWindowClose) example does not work\nDescription: Hi, I got this example by using the following prompt in Google: # kafka streams unit testing with chained \"emitStrategy\" # Provide an example of testing chained suppress with different grace periods [https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d] Compiled and ran the example using latest kafka jars only to get [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s <<< FAILURE! -- in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest [ERROR] com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods -- Time elapsed: 1.096 s <<< FAILURE! org.opentest4j.AssertionFailedError: Final output should contain one result ==> expected: <1> but was: <0> at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694) at com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123) It appears that the test is not able to drive the kafka stream to emit the 2nd event. Could be a bug in test code/test driver/kafka streams? Thanks in advance Greg\nComments: - The record you use to close the \"second window\" (ie, `inputTopic.pipeInput(\"D\", \"value6\", secondWindowCloseTime);`) will be processed by the first window, and will get \"stuck there\". It does open a new 1-minute window [10:07; 10:08) which is still open, and thus no result is emitted. Hence, the time for the second windowed-aggregation does not advance to `10:07` and it's own window [10:00; 10:05) does not close yet. - Thanks. is this a bug or a feature? I don't want several 1-minute windows for this test How do I fix the test so the end result (finalOutputTopic) contains a single aggregated value? - It's not a bug, but behavior by design. For this particular test, you would need to send one more even, with ts => 10:08:30, to close the [10:07; 10:08) window; when this window gets closed the result goes into the second window operator, advancing the time there accordingly emitting the result of window [10:00; 10:05) - Hi you may close this issue as you wish. I have another one that I am about to open, hope to get your help with it. https://issues.apache.org/jira/browse/KAFKA-19828 Thanks", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19809", "url": "https://issues.apache.org/jira/browse/KAFKA-19809"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CheckStyle version upgrade: 10 -->> 12\nDescription: *Task:* upgrade checkstyle version from *10.20.2* to *12.0.1* *Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1] *Note:* difference between versions is quite big: * version *10.20.2* (published in Novemeber 2024) * version *12.0.1* (published in Octoober 2025) * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]\nComments: - @Anyone: feel free to check PR: [https://github.com/apache/kafka/pull/20726]", "output": "Status: Patch Available\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19808", "url": "https://issues.apache.org/jira/browse/KAFKA-19808"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Handle batch alignment when share partition is at capacity\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19807", "url": "https://issues.apache.org/jira/browse/KAFKA-19807"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add RPC-level integration tests for StreamsGroupHeartbeat\nDescription: Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHeartbeatRequestTest` for `StreamsGroupHeartbeat`\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19806", "url": "https://issues.apache.org/jira/browse/KAFKA-19806"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Hook to enable / disable multi-partition remote fetch feature\nDescription: \nComments: No comments.", "output": "Status: Patch Available\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19805", "url": "https://issues.apache.org/jira/browse/KAFKA-19805"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add acks dimension to BrokerTopicMetrics for produce requests\nDescription: h3. *Title* Add {{acks}} dimension to {{BrokerTopicMetrics}} to measure per-acks produce performance impact ---- h3. *Summary* Currently, Kafka\u2019s {{BrokerTopicMetrics}} only tracks produce metrics such as {{{}BytesInPerSec{}}}, {{{}MessagesInPerSec{}}}, and {{ProduceRequestsPerSec}} at the topic level. However, these metrics do not distinguish between different producer acknowledgment ({{{}acks{}}}) configurations ({{{}acks=0{}}}, {{{}acks=1{}}}, {{{}acks=-1{}}}). In high-throughput environments, different {{acks}} levels have significantly different impacts on broker CPU, I/O, and network utilization. This proposal introduces an additional {{acks}} label to existing topic-level metrics, enabling more granular visibility into broker performance under various producer reliability modes. ---- h3. *Motivation* The current aggregated produce metrics make it difficult to assess the performance and stability implications of different {{acks}} settings on brokers. For example, asynchronous ({{{}acks=0{}}}) and fully acknowledged ({{{}acks=-1{}}}) produces can have very different effects on disk I/O, request queues, and replication latency, but these effects are hidden in current metrics. By introducing an {{acks}} dimension, operators and performance engineers can: * Quantify the resource cost of different producer acknowledgment strategies. * Analyze how {{acks}} configuration affects cluster throughput, replication load, and latency. * Perform fine-grained benchmarking and capacity planning. ---- h3. *Proposed Changes* # *Extend {{BrokerTopicStats}}* Add a new {{perTopicAcksStats}} structure to track metrics per {{(topic, acks)}} combination: {code:java} val perTopicAcksStats = new Pool[(String, Short), BrokerTopicMetrics]( Some((key) => new BrokerTopicMetrics(Some(s\"${key._1},ack=${key._2}\"))) ){code} # *Instrument Produce Handling* In {{{}KafkaApis.handleProduceRequest{}}}, extract the producer {{acks}} value and record metrics accordingly: {code:java} val ackVal = produceRequest.acks() brokerTopicStats.topicStats(topic).bytesInRate.mark(bytes) brokerTopicStats.topicAcksStats(topic, ackVal).bytesInRate.mark(bytes){code} The same logic applies to: * ** {{messagesInRate}} * ** {{produceRequestsRate}} # *Automatic Metric Naming* Since {{BrokerTopicMetrics}} extends {{{}KafkaMetricsGroup{}}}, the new label will automatically generate JMX metrics like: {{kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=-1 kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=1 kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=0}} # *Performance Considerations* * ** {{perTopicAcksStats}} uses lazy initialization and caching via {{Pool}} to avoid excessive metric object creation. * ** Expiration or cleanup logic can be added for inactive metrics. ---- h3. *Example Metrics Output* {code:java} kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=0 kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=1 kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=-1{code} ---- h3. *Compatibility & Impact* * No breaking changes to existing metrics. * Existing metric names and topic-level aggregation remain unaffected. * New metrics are additive and optional.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19804", "url": "https://issues.apache.org/jira/browse/KAFKA-19804"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improve heartbeat request manager initial HB interval\nDescription: With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it. That 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure). Probably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766] * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L255] We should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought) High level goals here would be to: * maintain the behaviour of sending a first HB without delay * ensure no unneeded activity on the HB mgr poll in the background, in tight loop, while we're just waiting for the first HB response with an interval * ensure the app thread poll timeout is not affected\nComments: - [~lianetm], are you planning to work on this issue? If not, I\u2019d be happy to take it over. Thanks! - Sure, feel free to take it and I can help with reviews. Thanks for your help! - [~brandboat] It seems you are already working on other issues .Can i take this up ? .It would be my first hand on the new consumer group protocol. - [~goyarpit] Thanks for checking! I\u2019ve already started exploring this one. If you are interested, you can help review once I have a PR ready. - Hi [~lianetm], I took a closer look at this issue and wanted to share a few thoughts. {quote}That 0 ms initial interval causes the HB manager poll to run continuously in a tight loop, executing logic that may not really be needed\u2014it mostly just waits for a response or failure. {quote} This actually shouldn\u2019t happen because we already check whether there\u2019s any in-flight heartbeat request before sending a new one: [https://github.com/apache/kafka/blob/1330870efbb4efd9fd394ec5cb8a0fecf8e69b24/clients/src/main/java/org/apache/kafka/clients/consumer/internals/HeartbeatRequestState.java#L96] [https://github.com/apache/kafka/blob/80f31224aad543dbfc892bce1ad73b6bb693855a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestState.java#L79] {quote}We should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought) {quote} -It\u2019s also worth noting that if we set a non-zero initial interval, the first poll() call will block until the timeout because no partitions are assigned yet. That means no Fetch requests are sent and the fetchBuffer remains empty until the poll timeout expires. This \"tight loop\" behavior on the application thread is actually intentional per PR [https://github.com/apache/kafka/pull/14835] to mitigate the case where Consumer.poll(Duration timeout) would otherwise block for the entire duration.- -I\u2019m thinking we could set a small non-zero initial heartbeat interval, what about 1s? to slightly loosen the tight loop. Of course, this means the first poll will still block for up to 1s. Using request.timeout.ms (default 30 s) with pollTimeout (e.g., 15 s) would make the first poll take too long before doing any useful work.- - Hey [~brandboat] , thanks for the investigation. I too faced this issue of pollTimer in {color:#0747a6}ShareConsumerImpl{color} being 0 initially (the heartbeatInterval=0 makes the maximumTimeToWait=0). {quote}It\u2019s also worth noting that if we set a non-zero initial interval, the first poll() call will block until the timeout because no partitions are assigned yet. {quote} I am not sure here, the {color:#0747a6}pollTimeout{color} also is based on the timeout argument passed to poll(). We will take the minimum of the {color:#0747a6}applicationTimeout{color} (in this case would be 30 seconds as we set {color:#0747a6}heartbeatInterval{color} to 30s) and the pollTimeout(which the user passes in poll). Assuming the user passes in 30000ms in poll, the poll only blocks the application thread(until data is received), the request managers in the background thread meanwhile will still send the next heartbeat request and reconcile the assignment when the heartbeat response is received? So not sure if we would really block the operations by setting the value to 30s? - Hi [~shivsundar], you're right \u2014 I misunderstood the code. I just ran a quick test to verify your explanation, and it matches what you said. So there\u2019s no blocking issue here. I originally noticed this \u201cblocking\u201d behavior during my implementation and found the test {{testAsyncConsumeUnsubscribeWithoutGroupPermission}} failed. I did some investigation at that time but ended up drawing the wrong conclusion. Based on your explanation, it seems that the issue in that test is actually unrelated. I\u2019ll take another look later. Thanks again for the correction! - Hi [~shivsundar], Turns out the blocking issue does exist \u2014 I spoke too soon last time, sorry for the confusion. Here\u2019s what I found: Let\u2019s say there are already some records in the topic, and we call {{poll(Duration.ofSeconds(10))}} with: * consumer heartbeat interval = 30s * broker heartbeat interval = 5s * auto-commit interval = 5s * auto.offset.reset = earliest *Behavior:* # {{subscribe}} + {{enable.auto.commit=true}} \u2192 first {{poll()}} blocks for around 5s, then return records # {{subscribe}} + {{enable.auto.commit=false}} \u2192 first {{poll()}} blocks for around 10s (poll timeout), then return records # {{assign}} \u2192 first {{poll()}} doesn\u2019t block and we get records immediately *Why it happens:* In (1) and (2), during the first {{{}poll(){}}}, no Fetch request is send to broker because we haven\u2019t received any partition assignments yet, i.e. the heartbeat request hasn\u2019t returned. This causes {{pendingFetchRequestFuture}} completed and set to null {^}[1]{^}{^}[2]{^}{^}[3]{^}, preventing FetchRequestManager from issuing further fetches {^}[4]{^}, which in turn makes the fetch buffer wait until it times out {^}[5]{^}. * For (1), since auto-commit is on, the wait time is basically {code:java} Math.min(applicationEventHandler.maximumTimeToWait(), timer.remainingMs()) {code} which ends up being the auto-commit interval (5s). * For (2), it just uses the {{{}pollTimeout{}}}. * For (3), since partitions are already assigned, {{AbstractFetch}} can start fetching records right away, so there\u2019s no blocking. Now I\u2019m looking into how to reduce or avoid this initial blocking on the first {{poll()}} call. Any suggestions or feedback is welcome :) I\u2019ve also submitted a draft PR if you\u2019d like to take a look. [1]: [https://github.com/apache/kafka/blob/5c49b48eb0732043400ec1ffb23022ebb3b47085/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractFetch.java#L438] [2]: [https://github.com/apache/kafka/blob/9e424755d4d236442847b13863580f44f27e22a6/clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetchRequestManager.java#L162] [3]: [https://github.com/apache/kafka/blob/9e424755d4d236442847b13863580f44f27e22a6/clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetchRequestManager.java#L171] [4]: [https://github.com/apache/kafka/blob/9e424755d4d236442847b13863580f44f27e22a6/clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetchRequestManager.java#L142] [5]: [https://github.com/apache/kafka/blob/10f26c86297dd2770cd7c93e35b27d4c4ceb0e1c/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1797] - The [above comment|https://issues.apache.org/jira/browse/KAFKA-19804?focusedCommentId=18034215&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18034215] is not correct anymore, after [KAFKA-18376|https://issues.apache.org/jira/browse/KAFKA-18376] was resolved, in AsyncKafkaConsumer#pollForFetches now the pollTimeout is reduced to retryBackoffMs (default 100ms) when there aren't any assigned partitions. See https://github.com/apache/kafka/blob/72532b6f738d2582354232d061d4d37ae5b52bee/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1873-L1894 That means by default the first pollForFetches will retry after 100ms until the partition assignments are back, which should mitigate the problem mentioned in the [above comment|https://issues.apache.org/jira/browse/KAFKA-19804?focusedCommentId=18034215&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18034215]. I think we can move on.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19803", "url": "https://issues.apache.org/jira/browse/KAFKA-19803"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Relax state directory file system restrictions\nDescription: The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept. We could also make this configurable, which would require a KIP. KIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19802", "url": "https://issues.apache.org/jira/browse/KAFKA-19802"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update ShareGroupCommand to use share partition lag information\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19801", "url": "https://issues.apache.org/jira/browse/KAFKA-19801"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce deliveryCompleteCount in DescribeShareGroupStateOffsets\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19800", "url": "https://issues.apache.org/jira/browse/KAFKA-19800"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Compute share partition lag in GroupCoordinatorService\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19799", "url": "https://issues.apache.org/jira/browse/KAFKA-19799"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce deliveryCompleteCount in ReadShareGroupStateSummary\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19798", "url": "https://issues.apache.org/jira/browse/KAFKA-19798"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Persist deliveryCompleteCount in ShareSnapshot and ShareUpdate records\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19797", "url": "https://issues.apache.org/jira/browse/KAFKA-19797"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implement deliveryCompleteCount in writeShareGroupStateRPC\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19796", "url": "https://issues.apache.org/jira/browse/KAFKA-19796"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce computations for inFlightTerminalRecords\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19795", "url": "https://issues.apache.org/jira/browse/KAFKA-19795"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Mark the minOneMessage as false when delayedRemoteFetch is present in the first partition\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19794", "url": "https://issues.apache.org/jira/browse/KAFKA-19794"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: MirrorCheckpointTask causes consumers on target cluster to rewind offsets or skip messages due to erroneous offset commits\nDescription: h2. *Description* The MirrorCheckpointTask in Mirror Maker 2 commits offsets for active consumer groups on the target cluster, causing consumers to rewind their offsets or skip messages. Typically brokers prevent committing offsets for consumer groups that are not in `EMPTY` state by throwing {{{}UnknownMemberIdException{}}}. In addition, {{MirrorCheckpointTask}} has logic in place to prevent committing offsets older than target for {{EMPTY}} consumer groups. However, due to a bug in {{MirrorCheckpointTask}} code, this prevention check is not enforced and it attempts to commit offsets for {{STABLE}} consumers. These calls can go through if consumers were momentarily disconnected moving the group state to {{{}EMPTY{}}}. This results in consumers' offsets getting reset to older values. If the offset is not available on the target broker (due to retention), the consumers can get reset to \"{{{}earliest\"{}}} or \"{{{}latest\"{}}}, thus reading duplicates or skipping messages. h2. *Bug location* 1. In [MirrorCheckpointTask|#L305], we only update the latest target cluster offsets ({{{}idleConsumerGroupsOffset{}}}) if target consumer group state is {{{}EMPTY{}}}. 2. When {{syncGroupOffset}} is called, we check if the target consumer group is present in {{{}idleConsumerGroupsOffset{}}}. The consumer group won't be present as it's an active group. We assume that this is a new group and start syncing consumer group offsets to target. These calls fail with {_}{{{{{}Unable to sync offsets for consumer group XYZ. This is likely caused by consumers currently using this group in the target cluster. (org.apache.kafka.connect.mirror.MirrorCheckpointTask{}}}}}{_}. When consumers have failed over, the logs typically contain a lot of these messages. These calls can succeed if consumer is momentarily disconnected due to restarts. The code should not assume the lack of consumer group in {{idleConsumerGroupsOffset}} map as a new consumer group. 3. These erroneous behavior can also be triggered calls to {{describeConsumerGroups}} or {{listConsumerGroupOffsets}} fail in {{refreshIdleConsumerGroupOffset}} method due to transient timeouts. h2. *Fix* Potential fix would be to add an explicit check to only sync offsets for EMPTY consumer group. We should also skip offset syncing for consumer groups for which we couldn't refresh the offsets. {code:java} // Fixed code adds state checking: ConsumerGroupState groupStateOnTarget = targetConsumerGroupStates.get(consumerGroupId); if (!isGroupPresentOnTarget || groupStateOnTarget == ConsumerGroupState.DEAD) { // Safe to sync - new or dead group syncGroupOffset(consumerGroupId, convertedUpstreamOffset); } else if (groupStateOnTarget == ConsumerGroupState.EMPTY) { // Safe to sync - idle group // ... existing offset comparison logic } else { // Skip active groups (STABLE, PREPARING_REBALANCE, COMPLETING_REBALANCE) log.info(\"Consumer group: {} with state: {} is being actively consumed on the target, skipping sync.\", consumerGroupId, groupStateOnTarget); } {code}\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19793", "url": "https://issues.apache.org/jira/browse/KAFKA-19793"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Disable topic autocreation for streams consumers.\nDescription: Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.\nComments: - Hi [~nikita-shupletsov] can i pick this up ? - Hi, [~goyarpit]. yes, please - [~nikita-shupletsov] I saw in main consumer we made this change after override . So should we allow the overrride for this field or not ?. I will make the changes accordingly. - I don't think we should allow override for this field. [~mjsax] what do you think? - Yes, we should not allow users to set this config. So we should add it to \"consumer default overwrites\" which are applied to all consumers. - [~nikita-shupletsov] [~mjsax] PR is open for revview https://github.com/apache/kafka/pull/20723 - [~nikita-shupletsov] [~mjsax] can you review this. I have addressed the comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19792", "url": "https://issues.apache.org/jira/browse/KAFKA-19792"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Gradle build fails after Swagger patch version update\nDescription: *How to reproduce:* * checkout trunk (Swagger version: 2.2.25) * change `swaggerVersion` in `gradle.properties` to some more recent version (something in between of 2.2.27 and 2.2.39) * execute *./gradlew clean releaseTarGz* * build fails with an error _*Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.*_ * see details below * (!) note: build works for swagger version 2.2.26 (although reporter had some issues once or twice with that particular version, so please keep that on your mind) {code:java} dejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew clean releaseTarGz > Configure project : Starting build with version 4.2.0-SNAPSHOT (commit id 34581dff) using Gradle 9.1.0, Java 17 and Scala 2.13.17 Build properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0 [Incubating] Problems report is available at: file:///home/dejan/kafka/build/reports/problems/problems-report.html FAILURE: Build failed with an exception. * Where: Build file '/home/dejan/kafka/build.gradle' line: 3728 * What went wrong: A problem occurred evaluating root project 'kafka'. > Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String. * Try: > Run with --stacktrace option to get the stack trace. > Run with --info or --debug option to get more log output. > Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 10. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. For more on this, please refer to https://docs.gradle.org/9.1.0/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation. BUILD FAILED in 5s dejan@dejan-HP-ProBook-450-G7:~/kafka$ {code}\nComments: - Thanks for reporting this! I\u2019ll help look into the issue.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19791", "url": "https://issues.apache.org/jira/browse/KAFKA-19791"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add Idle Thread Ratio Metric to MetadataLoader\nDescription: KIP-1229: [https://cwiki.apache.org/confluence/x/ZQteFw]\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19790", "url": "https://issues.apache.org/jira/browse/KAFKA-19790"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Parsing of the scope claim does not comply with RFC-8693\nDescription: I notice that the code in Kafka for handling of the scopes claim does not comply with the RFC. [https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim |https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim]says: {quote}The value of the {{scope}} claim is a JSON string containing a space-separated list of scopes associated with the token, in the format described in [Section 3.3|https://www.rfc-editor.org/rfc/rfc6749#section-3.3] of [[RFC6749|https://datatracker.ietf.org/doc/html/rfc6749]] {quote} However the code in Kafka that parses the JWT payload does not permit a space separated list. It would treat a value like \"email phone address\" as a single scope \"email phone address\" rather than a three separate scopes of \"email\", \"phone\", \"address\". The affected code is here: [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/BrokerJwtValidator.java#L166] [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java#L343] Impact: Looking at the production code in Apache Kafka itself, I think the defect currently harmless. As far as I can tell, there's no production code that makes use of org.apache.kafka.common.security.oauthbearer.internals.secured.BasicOAuthBearerToken#scope. I think there would be a potential for impact for a user writing their own OAuthBearerValidatorCallbackHandler that uses Kafka's BrokerJwtValidator and made use of the scope value. Failing unit test: [https://github.com/apache/kafka/compare/trunk...k-wall:kafka:KAFKA-19790]\nComments: - This issue is not blocking me (I noticed it in passing), but I'm happy to put up a PR if there is interest in fixing it.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19789", "url": "https://issues.apache.org/jira/browse/KAFKA-19789"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Handle situations where broker responses appear logically incorrect\nDescription: We have seen a situation in which the ShareFetch responses from the broker appeared logically incorrect. The consumer should be more defensive, and at least log the problem.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19788", "url": "https://issues.apache.org/jira/browse/KAFKA-19788"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0\nDescription: * kafka server version is 2.5.1 * kafka-client version bigger than 3.1.1 {code:java} import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; public class KafkaSendTest { public static void main(String[] args) { Properties props = new Properties(); props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\"); props.put(\"transaction.timeout.ms\", \"300000\"); props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully props.put(\"compression.type\", \"lz4\"); props.put(\"security.protocol\", \"SASL_PLAINTEXT\"); props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\"); props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\"); Producer<byte[], byte[]> producer = new KafkaProducer<>(props); try { String topic = \"topic1\"; byte[] value = new byte[]{1,2}; // example ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topic, null, value); producer.send(record, (metadata, exception) -> { if (exception == null) { System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\", record.key(), new String(record.value()), metadata.partition(), metadata.offset()); } else { exception.printStackTrace(); } }); producer.close(); } catch (Exception e) { e.printStackTrace(); } } } {code} pom.xml config {code:java} <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-clients</artifactId> <version>3.4.0</version> </dependency> {code} When kafka producer acks=-1, It will throw exception. {code:java} org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state at org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:1010) at org.apache.kafka.clients.producer.internals.TransactionManager.maybeAddPartition(TransactionManager.java:328) at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:1061) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:962) at com.mvad.realtime.show.converter.DataEntryToRTLogConverterTest.main(DataEntryToRTLogConverterTest.java:34)Caused by: org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed. {code} If acks=1 or acks=0 it will send successfully {code:java} Sent record(key=null ) meta(partition=6, offset=321496) {code} acks=-1 is just a param, How it effects ClusterAuthorization of kafka producer. Is this a bug or a mechanism in itself? If change kafka-client verison to 3.1.0. When kafka producer acks=-1, It will send successfully {code:java} <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-clients</artifactId> <version>3.1.0</version> </dependency> {code}\nComments: - Hi [~zhumingustc] I tried to replicate this with acks: -1. I am able to send the record, could you share your server setup? {code:java} 14:28:31.273 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. Sent record(key=null ){code} - Thanks. I find the reason of this ClusterAuthorizationException. It involves the config of idempotence. https://issues.apache.org/jira/browse/KAFKA-13673 In this kafka-client 3.1.1 issue {color:#de350b}b. enable.idempotence unset && acks=all => enable idempotence .{color} So Idempotence is set to true based on the producer properties However, the Kafka server has not enabled the idempotence right for admin. Finally it throws such ClusterAuthorizationException. After following command executed in kafka server. Producer can send record successfully with acks: -1 {code:java} sh kafka-acls.sh --authorizer-properties zookeeper.connect=xxxx:2191/xxx --add --allow-principal User:admin --allow-host \"*\" --operation IdempotentWrite --cluster {code}", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19787", "url": "https://issues.apache.org/jira/browse/KAFKA-19787"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0\nDescription: * kafka server version is 2.5.1 * kafka-client version bigger than 3.1.1 {code:java} import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; public class KafkaSendTest { public static void main(String[] args) { Properties props = new Properties(); props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\"); props.put(\"transaction.timeout.ms\", \"300000\"); props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully props.put(\"compression.type\", \"lz4\"); props.put(\"security.protocol\", \"SASL_PLAINTEXT\"); props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\"); props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\"); Producer<byte[], byte[]> producer = new KafkaProducer<>(props); try { String topic = \"topic1\"; byte[] value = new byte[]{1,2}; ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topic, null, value); producer.send(record, (metadata, exception) -> { if (exception == null) { System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\", record.key(), new String(record.value()), metadata.partition(), metadata.offset()); } else { exception.printStackTrace(); } }); producer.close(); } catch (Exception e) { e.printStackTrace(); } } } {code} pom.xml config {code:java} <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-clients</artifactId> <version>3.4.0</version> </dependency> {code} When kafka producer acks=-1, It will throw exception. {code:java} org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state at org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:1010) at org.apache.kafka.clients.producer.internals.TransactionManager.maybeAddPartition(TransactionManager.java:328) at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:1061) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:962) at com.mvad.realtime.show.converter.DataEntryToRTLogConverterTest.main(DataEntryToRTLogConverterTest.java:34)Caused by: org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed. {code} If acks=1 or acks=0 it will send successfully {code:java} Sent record(key=null ) meta(partition=6, offset=321496) {code} acks=-1 is just a param, How it effects ClusterAuthorization of kafka producer. Is this a bug or a mechanism in itself?\nComments: No comments.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19786", "url": "https://issues.apache.org/jira/browse/KAFKA-19786"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Session mode compileScala reports missing classes java main directory\nDescription: *Problem* When running *{{:core:compileScala}}* under {*}{{keepAliveMode=SESSION}}{*}, joint compilation adds a non-existent directory ({_}{{.../core/build/classes/java/main}}{_}) to {{{}javac{}}}\u2019s classpath. This triggers: {code:java} Unexpected javac output: warning: [path] bad path element \".../core/build/classes/java/main\": no such file or directory error: warnings found and -Werror specified {code} The same task succeeds under {{{}keepAliveMode=DAEMON{}}}. *Current Workaround* Added a scoped compiler flag ({{{}-Xlint:-path{}}}) only under {{keepAliveMode=SESSION}} to suppress the {{[path]}} warning and allow the build to succeed. *Steps to Reproduce* {code:java} ./gradlew clean :core:compileScala -PkeepAliveMode=session # fails before workaround ./gradlew clean :core:compileScala -PkeepAliveMode=daemon # succeeds {code} *Goal of This Ticket* Identify why SESSION-mode joint compilation includes the missing {{classes/java/main}} entry in {{{}javac{}}}\u2019s classpath, align its behavior with DAEMON mode, and remove the temporary suppression once the classpath is corrected.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19785", "url": "https://issues.apache.org/jira/browse/KAFKA-19785"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Two Kafka brokers were not active in 3 node cluster setup\nDescription: Hi Team, We were facing kafka issue where two of the kafka brokers were fenced and Kafka was not able to process messages. We are using Kafka 4.0.0 version. Below are the errors. Sep 22 09:41:42 s99ossv152 kafka[42245]: [2025-09-22 07:41:42,419] ERROR Encountered fatal fault: Unexpected error in raft IO thread (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler) Sep 22 09:41:42 s99ossv152 kafka[42245]: java.lang.IllegalStateException: Received request or response with leader OptionalInt[3] and epoch 55 *which is inconsistent with current leader* OptionalInt.empty and epoch 55 Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeTransition(KafkaRaftClient.java:2528) ~[kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeHandleCommonResponse(KafkaRaftClient.java:2484) ~[kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleFetchResponse(KafkaRaftClient.java:1707) ~[kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:2568) ~[kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:2724) ~[kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:3460) ~[kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClientDriver.doWork(KafkaRaftClientDriver.java:64) [kafka-raft-4.0.0.jar:?] Sep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:136) [kafka-server-common-4.0.0.jar:?] Below metrics shows Fenceborker count as 2.0 kafka_controller_KafkaController_Value\\{name=\"ActiveBrokerCount\",} 1.0 kafka_controller_KafkaController_Value\\{name=\"GlobalTopicCount\",} 23.0 kafka_controller_KafkaController_Value{name=\"{*}FencedBrokerCount{*}\",} 2.0 Please help us to resolve this issue.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19784", "url": "https://issues.apache.org/jira/browse/KAFKA-19784"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Expose Rack ID in MemberDescription\nDescription: Currently, the {{{}AdminClient{}}}\u2019s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field. This causes users to be unable to retrieve member rack information through the Admin API. Rack information is crucial for: * Monitoring and visualization tools * Operational analysis of rack distribution * Diagnosing rack-aware assignment issues In addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent. The PR: [https://github.com/apache/kafka/pull/20691] The KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19783", "url": "https://issues.apache.org/jira/browse/KAFKA-19783"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9\nDescription: Prologue: KAFKA-19174 See here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324\nComments: - Hi [~dejan2609], could you take a look at [this|https://github.com/apache/kafka/pull/20684]? It might have already resolved this issue. - Hi [~isding_l] ! Yes, it seems that your PR resolves this issue. I will leave a review on your PR (maybe I can save you few lines). - Duplicate, will be solved here: [https://github.com/apache/kafka/pull/20684]", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19782", "url": "https://issues.apache.org/jira/browse/KAFKA-19782"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie\nDescription: Class Authorizer.authorizeByResourceType under the hood uses HashMap for check if the caller is authorized to perform the given ACL operation on at least one resource of the given type. It check each character in allowPatterns and pass to deny patterns {code:java} // For any literal allowed, if there's no dominant literal and prefix denied, return allow. // For any prefix allowed, if there's no dominant prefix denied, return allow. for (Map.Entry<PatternType, Set<String>> entry : allowPatterns.entrySet()) { for (String allowStr : entry.getValue()) { if (entry.getKey() == PatternType.LITERAL && denyPatterns.get(PatternType.LITERAL).contains(allowStr)) continue; StringBuilder sb = new StringBuilder(); boolean hasDominatedDeny = false; for (char ch : allowStr.toCharArray()) { sb.append(ch); if (denyPatterns.get(PatternType.PREFIXED).contains(sb.toString())) { hasDominatedDeny = true; break; } } if (!hasDominatedDeny) return AuthorizationResult.ALLOWED; } } {code} To improve performance better use Prefix Tree\nComments: - [~evkuvardin] can I take up this issue?", "output": "Status: Patch Available\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19781", "url": "https://issues.apache.org/jira/browse/KAFKA-19781"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Consumer NoOffsetForPartitionException for partitions being revoked\nDescription: Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this: * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions. This task is to review if there are no other implications I may be missing? Then fix to ensure we don't update positions for partitions being revoked (aligned with how we don't allow fetching from them)\nComments: - Hi [~lianetm] can i pick this up ? - This is what i understand , can can try reproducing it through test also . 1. Partitions marked for revocation (pendingRevocation = true) are excluded from fetching but NOT from position updates 2. shouldInitialize() only checks if state is INITIALIZING, ignoring pendingRevocation 3. hasValidPosition() only checks fetch state, ignoring pendingRevocation 4. This causes updateFetchPositions() to attempt fetching offsets for partitions being revoked - Hi [~goyarpit], I was already doing some work on this one. I will ping you when ready and would be great if you help with reviews (or follow-ups if I find). Will keep you in the loop. Thanks!", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19780", "url": "https://issues.apache.org/jira/browse/KAFKA-19780"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Create a CI to verify all gradle tasks\nDescription: see https://github.com/apache/kafka/pull/19513#issuecomment-3390047209\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19779", "url": "https://issues.apache.org/jira/browse/KAFKA-19779"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Relax offset commit validation to allow member epochs since assignment\nDescription: h2. Fencing offset commits In the Kafka protocol, when a consumer commits offsets or a producer tries to add offsets to a transaction, it includes its epoch/generation of the consumer group. The point of this is for the group coordinator to fence against zombie commit requests, that is, commit requests that include an offset for a partition that was since reassigned to a different member. If such a guard was not in place, a zombie offset commit may overwrite offsets of the new owner, or its offsets may be committed to the consumer offset topic but not be included in the result of the new owners offset fetch request. In KIP-848, when receiving an offset commit request that includes the client-side member epoch and a member ID, the group coordinator performs the check {{Client-Side Member Epoch == Broker-Side Member Epoch}} and, if the check fails, returns a {{STALE_MEMBER_EPOCH}} error for regular offset commits and a {{ILLEGAL_GENERATION}} for transactional offset commits. If the member epoch sent in the request is the current broker-side member epoch, KIP-848 guarantees that the partition cannot also be owned by a different member at the same or a larger epoch. Therefore, this is sufficient for fencing zombie commits. Note that we assume zombie commits will always contain offsets for partitions that were owned by the member at the member epoch sent in the request. Commit requests that commit offsets for partitions that are _not_ owned by the member in that epoch, are not possible in a correct client-side implementation of the protocol. Note that the broker-side member epoch is not the group epoch or the target assignment epoch. For details, see KIP-848. Note also that commits can also be fenced because a member falls out of the group (e.g. because it does not revoke partitions within the rebalance timeout). At this point, its commits will be fenced solely based on the member ID (which is not part of the group anymore). We therefore ignore this case in this document, and only consider zombie commits from members that are still part of the group. h2. Downsides of the current approach This fencing is, however, unnecessarily strict. Assume, for example, a member owns P1 at epoch 1. The broker-side member epoch is bumped to 2, but the member still has P1 assigned at epoch 2. The member may not learn about the new broker-side member epoch in time, and submit an offset commit commit for P1 with epoch 1. This is not a zombie commit request as define above (because P1 was not reassigned to a different member), but it will still be rejected by a KIP-848 group coordinator. The trouble with this fencing mechanism is that it is very difficult to avoid the broker-side member epoch being bumped concurrently with an offset commit. Seen from the client-side, the broker-side member epoch may be bumped at any time while a heartbeat to the group coordinator is in-flight. To make sure the member epoch sent in an offset commit request is up-to-date would require making sure that no consumer group or streams group heartbeat request is in-flight at the same time. h2. Why a broker-side fix is warranted This problem is particularly challenging to solve on the client side for transactional offset commits, since they are performed by the producer, not the consumer, and the producer has no way of knowing when a consumer group heartbeat or streams group heartbeat is in-flight. The member epoch is passed from the Java consumer to the Java producer using the {{ConsumerGroupMetadata}} object, which is passed into {{{}sendOffsetsToTransaction{}}}. By the time the transactional offset commit is sent, the member epoch may be stale, the broker will return an {{ILLEGAL_GENERATION}} exception. This will force the Java producer into an abortable error state, surfacing the error as a {{CommitFailedException}} to the user, the user has no other way to recover from this to abort the transaction. This may hurt in any Kafka client application, since aborting transactions means throwing away work and restarting from an earlier point. But it is a particularly big problem in Kafka Streams with exactly-once semantics, where aborting a transaction usually means wiping and restoring the state store, so each aborted transaction means some downtime for apps using state stores of non-negligible size. Furthermore, since Kafka Streams commits every 100ms by default in EOS, this is likely to happen fairly often. h1. Conceptual Design n this design document, we therefore propose to relax the condition for offset commit fencing. h2. Identifying zombies using the last epoch a partition was assigned to the member To derive a more relaxed check, we need to identify an epoch which separates zombie commits from commits of the current owner. As mentioned above, zombie commit requests are commit requests that include a partition, member ID and member epoch combination, so that the member owned the partition at that epoch. However, the partition has since been reassigned to a different member. Most precisely, on the level of a single partition, a relaxed offset commit check can be defined using a *assignment epoch* for each assigned partition and each member, which is the epoch at which the partition was assigned to that member. To fence from zombie commit requests, we can reject all offset commit requests from a member that either does not have the partition assigned, or that includes any member epoch that is smaller or equal than the assignment epoch for that member and that partition. Assignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch The correctness of this is obvious: all commits of the current partition owner will be accepted, since the Client-Side Member Epoch of the current owner must always have an epoch that is larger or equal than the assignment epoch (a partition that is revoked in one epoch is never reassigned in the same epoch). It will also correctly reject any zombie commits from that member, because if a partition was owned by the member A at Client-Side Member Epoch (which we assume for zombie commits), but it was reassigned to member B since, we have two possible cases: # Member A currently does not have the partition assigned # Member A does currently have the partition assigned, but then it must have been reassigned to member A after being assigned to member B. By KIP-848 this cannot all happen in the same epoch, so we must have Assignment Epoch > Client-Side Member Epoch. h3. Differences to the design above This design does not need to disable commits on the client-side. The need to disable commits came from the fact that we are tracking epochs \u201cimprecisely\u201d, on the member-level and not on the partition-level. So in the RevocationEpoch design, when we have just revoked P2 on the client, we may attempt to commit a partition P1, triggering the race condition because the broker can concurrently bump the revocation epoch for that member because of the revocation of P2. We prevent this by disabling commits while a partition is revoked, and by \u201cwhile a partition is revoked\u201d I mean the timeframe from executing the revocation on the client, and seeing the following epoch bump on the client. In the partition-level AssignmentEpoch design, if we are committing P1, we are still convinced that we own P1, so we must also still own it on the broker. At the same time, we may remove the assignment epoch for P2 on the broker, but it doesn\u2019t matter since this doesn\u2019t impact whether we can commit P1, and we are not going to try to commit P2 after having revoked it on the client side. h1. Proposed Changes h3. Introducing Per-Member and Per-Partition Assignment Epoch We extend the model of a consumer group / streams group member with one integer per assigned partition for each member of a group. This includes both partitions directly assigned to the member, and partitions pending revocation. The assignment epoch is set to the epoch in which the partition was assigned to the member, and we have the invariant Assignment Epoch <= MemberEpoch <= TargetAssignmentEpoch <= GroupEpoch. The AssignmentEpoch is added as a field to TopicPartitions in ConsumerGroupCurrentMemberAssignmentValue, so that it can be stored and replayed from the committed offsets topic. For streams groups, we will use the same logic but add assignment epochs only for active tasks in StreamsGroupCurrentMemberAssignmentValue, since only active tasks commit offsets in Kafka Streams. h3. Relaxing the offset commit validation We replace the current check offset commit validation check Client-Side Member Epoch == Broker-Side Member Epoch by Assignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch where, for simplicity, we can assume the assignment epoch of a partition that is not assigned to that member to be Integer.maxValue.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19778", "url": "https://issues.apache.org/jira/browse/KAFKA-19778"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Share Partition Lag Persistence and Retrieval\nDescription: Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets\nComments: - Hi [~chiragwadhwa55] \uff0c I could help with some of the sub-tasks if you'd like to share the workload, thanks~ - Hi [~chiragwadhwa55] i am\u0964happy to help in any of the subtasks Let me know if i can be of any help", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19777", "url": "https://issues.apache.org/jira/browse/KAFKA-19777"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Generator | Fix order of arguments to assertEquals in unit tests\nDescription: This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.\nComments: No comments.", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19776", "url": "https://issues.apache.org/jira/browse/KAFKA-19776"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Incorrect leaderId and leaderEpoch logged in Partition#makeFollower\nDescription: [Partition::makeFollower|https://github.com/apache/kafka/blob/be3e40c45ac9a70833ce0d5735ad141a5bd24627/core/src/main/scala/kafka/cluster/Partition.scala#L877] logs the existing leader-id and existing leader epoch if the partition registration has a leader epoch > the existing leader epoch. There's a bug in the logging as we update the existing leaderId and leaderEpoch with the latest values from partition registration prior to logging them, thereby never logging the existing values.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19775", "url": "https://issues.apache.org/jira/browse/KAFKA-19775"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Error if an empty topic is created when there is a regex source KS\nDescription: If there is a KS application that uses a regex source, and we create a new topic that matches that regex, but produce no messages, the application will get into an {{ERROR}} state. if we take {}[RegexSourceIntegrationTest#testRegexRecordsAreProcessedAfterNewTopicCreatedWithMultipleSubtopologies|#L206{}}}], but without producing any messages to {{TEST-TOPIC-2}} the problem will reproduce: {quote}{{org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}} {{ at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:981)}} {{ at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:898)}} {{Caused by: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}} {{ at org.apache.kafka.streams.processor.internals.StreamTask.findOffsetAndMetadata(StreamTask.java:480)}} {{ at org.apache.kafka.streams.processor.internals.StreamTask.committableOffsetsAndMetadata(StreamTask.java:511)}} {{ at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:454)}} {{ at org.apache.kafka.streams.processor.internals.TaskExecutor.commitTasksAndMaybeUpdateCommittableOffsets(TaskExecutor.java:145)}} {{ at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:2025)}} {{ at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1992)}} {{ at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:1836)}} {{ at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1288)}} {{ at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:938)}} {{ ... 1 more}} {quote}\nComments: - Using 4.0.1 (CP 8.0.2) after upgrading from 3.X, this error is new. I wonder if this is a variant on the same issue? {code} level: ERROR logger: org.apache.kafka.streams.processor.internals.TaskManager Error flushing caches of dirty task 0_0 stack: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TOPIC-0 at org.apache.kafka.streams.processor.internals.StreamTask.findOffsetAndMetadata(StreamTask.java:476) at org.apache.kafka.streams.processor.internals.StreamTask.committableOffsetsAndMetadata(StreamTask.java:507) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:450) at org.apache.kafka.streams.processor.internals.TaskManager.closeTaskDirty(TaskManager.java:1420) at org.apache.kafka.streams.processor.internals.TaskManager.closeAndCleanUpTasks(TaskManager.java:1538) at org.apache.kafka.streams.processor.internals.TaskManager.lambda$shutdown$16(TaskManager.java:1459) at org.apache.kafka.streams.processor.internals.TaskManager.executeAndMaybeSwallow(TaskManager.java:2120) at org.apache.kafka.streams.processor.internals.TaskManager.shutdown(TaskManager.java:1457) at org.apache.kafka.streams.processor.internals.StreamThread.completeShutdown(StreamThread.java:1577) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:701) } {code} - [~amccague] seems so, yes. updating to 4.0.2 should help - Neither 4.0.2 (nor CP 8.0.3), which contain the fix, are release yet. Only AK 4.1.1 was released. (CP 8.1.1 [expected Q4/25] and CP 8.2 should contain the fix as well, but not release yet.).", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19774", "url": "https://issues.apache.org/jira/browse/KAFKA-19774"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Mechanism to cordon brokers and log directories\nDescription: Jira for KIP-1066: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19773", "url": "https://issues.apache.org/jira/browse/KAFKA-19773"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Include push interval in ClientTelemetryReceiver context\nDescription: Jira for KIP-1217 https://cwiki.apache.org/confluence/display/KAFKA/KIP-1217%3A+Include+push+interval+in+ClientTelemetryReceiver+context\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19772", "url": "https://issues.apache.org/jira/browse/KAFKA-19772"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: enhance the documentation for `Node#isFanced`\nDescription: The `isFenced` value is only valid for broker nodes. It is always `false` for quorum controller nodes.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19771", "url": "https://issues.apache.org/jira/browse/KAFKA-19771"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25\nDescription: *Prologue:* KAFKA-19664 *In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106 *Action points:* - upgrade SpotBugs version (use Java 25 compatible version) - enable SpotBugs checks for Java 25 Github actions build *Related links: * - [https://issues.apache.org/jira/projects/BCEL/versions/12354966] - [https://github.com/spotbugs/spotbugs/issues/3564]\nComments: - *{{Apache commons-bcel}}* Java 25 compatible version should be released soon: https://github.com/apache/commons-bcel/releases/tag/commons-bcel-6.11.0-RC1 - *Apache* *{{commons-bcel}}* new version has been released; Spotbugs version should be released soon: - [https://github.com/spotbugs/spotbugs/pull/3763] - [https://github.com/spotbugs/spotbugs/milestone/31?closed=1] - [https://github.com/spotbugs/spotbugs/discussions/3771] - GitHub PR is created here: https://github.com/apache/kafka/pull/20704 - [~chia7712] Please review when you find some time: [https://github.com/apache/kafka/pull/20704]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19770", "url": "https://issues.apache.org/jira/browse/KAFKA-19770"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove the `gradlew` workaround for running the old `gradle-wrapper.jar`\nDescription: see https://github.com/apache/kafka/pull/20658\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19769", "url": "https://issues.apache.org/jira/browse/KAFKA-19769"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: [Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)\nDescription: {panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce} * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_* * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel} *Reproducer:* * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561]) * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions * test results: ** {*}Java 17{*}: (/) ** J{*}ava 24{*}: (/) ** {*}Java 25{*}: 159 tests completed, *8 failed* (x) *Test results on Github CI:* !Screenshot from 2025-10-07 19-59-32.png! *Test results locally:* {code:java} dejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline a37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS release) c6bbbbe24d KAFKA-19174 Gradle version upgrade 8 -->> 9 (#19513) f5a87b3703 KAFKA-19748: Add a note in docs about memory leak in Kafka Streams 4.1.0 (#20639) dejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version openjdk version \"17.0.16\" 2025-07-15 OpenJDK Runtime Environment Temurin-17.0.16+8 (build 17.0.16+8) OpenJDK 64-Bit Server VM Temurin-17.0.16+8 (build 17.0.16+8, mixed mode, sharing) dejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q Starting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 17 and Scala 2.13.17 Build properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0 MessageGenerator: processed 197 Kafka message JSON file(s). MessageGenerator: processed 4 Kafka message JSON file(s). Note: Some input files use or override a deprecated API. Note: Recompile with -Xlint:deprecation for details. OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended dejan@dejan-HP-ProBook-450-G7:~/kafka$ sdk use java 24.0.2-tem Using java version 24.0.2-tem in this shell. dejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version openjdk version \"24.0.2\" 2025-07-15 OpenJDK Runtime Environment Temurin-24.0.2+12 (build 24.0.2+12) OpenJDK 64-Bit Server VM Temurin-24.0.2+12 (build 24.0.2+12, mixed mode, sharing) dejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q Starting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 24 and Scala 2.13.17 Build properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0 MessageGenerator: processed 197 Kafka message JSON file(s). MessageGenerator: processed 4 Kafka message JSON file(s). Note: Some input files use or override a deprecated API. Note: Recompile with -Xlint:deprecation for details. OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended dejan@dejan-HP-ProBook-450-G7:~/kafka$ sdk use java 25-tem Using java version 25-tem in this shell. dejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version openjdk version \"25\" 2025-09-16 LTS OpenJDK Runtime Environment Temurin-25+36 (build 25+36-LTS) OpenJDK 64-Bit Server VM Temurin-25+36 (build 25+36-LTS, mixed mode, sharing) dejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q Starting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 25 and Scala 2.13.17 Build properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0 MessageGenerator: processed 197 Kafka message JSON file(s). MessageGenerator: processed 4 Kafka message JSON file(s). Note: Some input files use or override a deprecated API. Note: Recompile with -Xlint:deprecation for details. org.apache.kafka.common.network.SslTransportLayerTest.testListenerConfigOverride(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testListenerConfigOverride(Args)[1].test.stdout OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended org.apache.kafka.common.network.SslTransportLayerTest.testInterBrokerSslConfigValidation(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testInterBrokerSslConfigValidation(Args)[1].test.stdout org.apache.kafka.common.network.SslTransportLayerTest.testServerTruststoreDynamicUpdate(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testServerTruststoreDynamicUpdate(Args)[1].test.stdout org.apache.kafka.common.network.SslTransportLayerTest.testECKeyPair(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testECKeyPair(Args)[1].test.stdout org.apache.kafka.common.network.SslTransportLayerTest.testPemFiles(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testPemFiles(Args)[1].test.stdout org.apache.kafka.common.network.SslTransportLayerTest.testClientAuthenticationRequiredValidProvided(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testClientAuthenticationRequiredValidProvided(Args)[1].test.stdout org.apache.kafka.common.network.SslTransportLayerTest.testDsaKeyPair(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testDsaKeyPair(Args)[1].test.stdout org.apache.kafka.common.network.SslTransportLayerTest.testPemFilesWithoutClientKeyPassword(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testPemFilesWithoutClientKeyPassword(Args)[1].test.stdout 159 tests completed, 8 failed FAILURE: Build failed with an exception. * What went wrong: Execution failed for task ':clients:test'. > There were failing tests. See the report at: file:///home/dejan/kafka/clients/build/reports/tests/test/index.html BUILD FAILED in 8m 22s dejan@dejan-HP-ProBook-450-G7:~/kafka$ {code}\nComments: - [~dejan2609] Thanks for reporting this. I think this is because [TestUtils::CertificateBuilder|https://github.com/dejan2609/kafka/blob/a21a1b0a84b577724d0030a02b2ad7f2d08890e2/clients/src/test/java/org/apache/kafka/test/TestSslUtils.java#L402]'s constructor uses {{SHA1withRSA}} as the signature algorithm which is likely disabled by default. Please try updating the signature algorithm to say {{SHA256withRSA}} - You got that right [~gnarula] :) ! Thank you for the tip (y)", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19768", "url": "https://issues.apache.org/jira/browse/KAFKA-19768"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: `ci-complete` needs to work with active branches after the JDK is updated\nDescription: The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail\nComments: - Perhasp we could use multiple branches to handle this case. {code:java} jobs: upload-build-scan-main: if: github.event.workflow_run.head_branch == 'trunk' runs-on: ubuntu-latest strategy: matrix: # ... rest of the steps upload-build-scan-other: if: github.event.workflow_run.head_branch != 'trunk' runs-on: ubuntu-latest strategy: matrix: # ... rest of the steps{code} - Another idea to be considered (on): https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/github-code-transformation-workflow-advanced.html FYI [~mingyen066] - Do we know if it's possible to upload a build scan using Gradle version 9 that was produced using Gradle 8? I expect it might not work since the build scan data format may change. One thing we could try is to include a file in the build scan archive that indicates what version of Gradle and JDK were used. We can load these properties before running setup-gradle so we can select the correct version. There's nothing built in to GHA that helps here (as far as i know), maybe we can use JSON and \"jq\"? I won't be able to work on this unfortunately, but I'm happy to help with reviews. - Thanks for the suggestions, [~dejan2609] and [~davidarthur] After understanding the situation, I think the problem is not just the JDK version \u2013 the artifact names are also different. The newer archive uses the names with suffix ['flaky' | 'noflaky']-['new'|'nonew'], which 4.0 does not have. - I don\u2019t think we can rely on determining which branch the PR is being merged into, because we don\u2019t have access to target branch information. Therefore, I will change the approach to uploading all artifacts whose names start with {{{}build-scan-{}}}. Here\u2019s why we can\u2019t get the target branch information: When I create a PR from my own fork, I do see which branch the PR is targeting. But when the PR is to the upstream Kafka repository, the {{pull_request}} array is empty. (GitHub removed this for security reasons) Example \u2014 PR from my fork: {code:java} \"event\": \"pull_request\", \"head_branch\": \"merge-to-4.0\", \"pull_requests\": [ { \"base\": { \"ref\": \"4.0\" }, \"head\": { \"ref\": \"merge-to-4.0\" } } ] {code} Example \u2014 PR (created, synchronized) to upstream (we cannot get the target branch info here): {code:java} \"event\": \"pull_request\", \"head_branch\": \"merge-to-4.0\", \"pull_requests\": [] {code} After the upstream merge triggers the build scan, we can correctly determine the branch: {code:java} \"event\": \"push\", \"head_branch\": \"4.0\" {code}", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19767", "url": "https://issues.apache.org/jira/browse/KAFKA-19767"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improve handling of long processing times\nDescription: If the share consumer application takes a long time to process records, the share consume request manager can do a better job of time-outs and liveness.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19766", "url": "https://issues.apache.org/jira/browse/KAFKA-19766"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade Kafka repo to use JUnit6\nDescription: As JUnit6 was released [https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.\nComments: - JUnit 6 requires Java 17 while Kafka still supports Java 11 for some modules. So, we cannot move to JUnit 6 for all modules. It may be confusing to use different JUnit versions for different modules. - Unless JUnit 6 brings major enhancements, I'd rather stick to JUnit 5 across the full code base instead of using different versions. - Are there any plans to drop Java 11 support for those specific modules, or is that still on the distant horizon? - We only change the supported Java version in a major Kafka version. We currently have no plans for Kafka 5.0.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19765", "url": "https://issues.apache.org/jira/browse/KAFKA-19765"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Store the last used assignment configuration in the group metadata\nDescription: We should store the last used assignment configuration in the group metadata and bump the group epoch if the assignment configuration is changed.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19764", "url": "https://issues.apache.org/jira/browse/KAFKA-19764"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator\nDescription: Ticket for KIP-1224. * Add a new allowed value for {{group.coordinator.append.linger.ms}} and {{share.coordinator.append.linger.ms}} of -1. * When {{append.linger.ms}} is set to -1, use the flush strategy outlined in the KIP. * Add batch-linger-time metrics. * Add batch-flush-time metrics.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19763", "url": "https://issues.apache.org/jira/browse/KAFKA-19763"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Parallel remote reads causes memory leak in broker\nDescription: This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915 Broker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. Steps to reproduce: 1. Start a one node broker and configure LocalTieredStorage as remote storage. 2. Create a topic with 5 partitions. 3. Produce message and ensure that few segments are uploaded to remote. 4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log. 5. The broker crashes with the OOM error. 6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes. cc [~showuon] [~satish.duggana]\nComments: - [~ckamal] , I tired to reproduce it using your steps, but the heap only increase ~ 100 MB, not a big issue IMO. Have you figured it out where do we leak the memory? - [~showuon] Able to reproduce the issue consistently. Uploaded the RemoteReadMemoryLeakReproducer. The leak was due to that the DelayedRemoteFetchPurgatory holding the references of previously completed DelayedRemoteFetch objects. DelayedRemoteFetch contains the RemoteReadResult internally. > Have you figured it out where do we leak the memory? In a given FETCH request, if 1 out of 5 partition, read the data from local log, then the watcherKey for that partition holds the reference of the DelayedRemoteFetch in the purgatory; if there are no other remote-read happens for that partition, then it won't get removed until the reaper thread cleans it up after the purgeInterval (entries) of 1000. {code:java} % sh kafka-topics.sh --create --topic apple --partitions 5 --replication-factor 1 --bootstrap-server localhost:9092 --config remote.storage.enable=true --config local.retention.ms=60000 --config retention.ms=7200000 --config segment.bytes=104857600 --config file.delete.delay.ms=1000 % for i in `seq 1 100`; do echo $i; sleep 1; sh kafka-producer-perf-test.sh --topic apple --num-records 1200000000 --record-size 1024 --throughput 1000 --producer-props bootstrap.servers=localhost:9092; done {code} - This patch fixes the problem. But it slows down the read throughput because it takes time to clone the buffer. There should be other better solutions. {code:java} --- a/core/src/main/scala/kafka/server/DelayedRemoteFetch.scala +++ b/core/src/main/scala/kafka/server/DelayedRemoteFetch.scala @@ -22,6 +22,7 @@ import kafka.utils.Logging import org.apache.kafka.common.TopicIdPartition import org.apache.kafka.common.errors._ import org.apache.kafka.common.protocol.Errors +import org.apache.kafka.common.record.MemoryRecords import org.apache.kafka.server.LogReadResult import org.apache.kafka.server.metrics.KafkaMetricsGroup import org.apache.kafka.server.purgatory.DelayedOperation @@ -121,7 +122,8 @@ class DelayedRemoteFetch(remoteFetchTasks: util.Map[TopicIdPartition, Future[Voi result.error, result.highWatermark, result.leaderLogStartOffset, - info.records, + // clone the record buffer to release the memory + MemoryRecords.readableRecords(info.records.asInstanceOf[MemoryRecords].buffer()), Optional.empty(), if (result.lastStableOffset.isPresent) OptionalLong.of(result.lastStableOffset.getAsLong) else OptionalLong.empty(), info.abortedTransactions, @@ -132,7 +134,8 @@ class DelayedRemoteFetch(remoteFetchTasks: util.Map[TopicIdPartition, Future[Voi tp -> result.toFetchPartitionData(false) } } - + // clear the map to avoid memory leak + remoteFetchResults.clear() responseCallback(fetchPartitionData) } } {code} - This issue is fixed with https://github.com/apache/kafka/pull/20654 and https://github.com/apache/kafka/pull/20706", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19762", "url": "https://issues.apache.org/jira/browse/KAFKA-19762"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Turn on Gradle reproducible builds feature\nDescription: *Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] *Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future) *Related Gradle issues and links:* * [https://github.com/gradle/gradle/issues/34643] * [https://github.com/gradle/gradle/issues/30871] * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives] * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps] *Definition of done (at the minimum):* * *./gradlew releaseTarGz* works as expected * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]\nComments: - Hi [~dejan2609] Per the docs reproducible builds is enabled by deafult on gradle 9 {code:java} Starting with Gradle 9, archives are reproducible by default. {code} is this code block no longer required? {code:java} tasks.withType(AbstractArchiveTask).configureEach { reproducibleFileOrder = false preserveFileTimestamps = true useFileSystemPermissions() } {code} Sources: [Gradle Docks|https://docs.gradle.org/current/userguide/working_with_files.html#sec:reproducible_archives] [Gradle Reproducible Plugin|https://gradlex.org/reproducible-builds/] - Hi [~naveenthuvana] My suggestion for you is to try to build with *reproducibleFileOrder = true* (this is a default, so it can be left out) and *preserveFileTimestamps = true*", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19761", "url": "https://issues.apache.org/jira/browse/KAFKA-19761"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Reorder Gradle tasks (in order to bump Shadow plugin version)\nDescription: *Prologue:* * JIRA ticket: KAFKA-19174 * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027] *Scenario:* * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+ * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x) *Action points (what needs to be done):* * use `com.gradleup.shadow` recent version (9+) * reorder Gradle tasks so that Gradle command mentioned above can work *Definition of done (at the minimum):* * Gradle command mentioned above works as expected * also: *./gradlew releaseTarGz* works as expected * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]\nComments: - Hi Dejan, Can I pick this up. I am new to open source. I think this could be a good one to start with. - Hi [~nish4_nth] Although I don't actually decide who can work on this (because I'm not a maintainer but only a contributor) I guess you are free to start if you want to contribute :) - Sure Thanks Dejan for updating the notes as well. - [~nish4_nth] I pushed PR (created via Gemini CLI): it turns out that this task is far from being simple. I would suggest that you take a look at KAFKA-19762 (from where I see it should not be that hard to start with that ticket, but it certainly requires some thorough testing and Gradle docs investigation). If you opt to take that one, I'll volunteer to be your non-binding reviewer until we have some solution to present for maintainers (without any modesty: I know a thing or two related to Gradle builds: https://github.com/apache/kafka/commits?author=dejan2609)", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19760", "url": "https://issues.apache.org/jira/browse/KAFKA-19760"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used\nDescription: The coordinator runtime can be overly optimistic about compression ratios when filling up batches and can often overflow `max.message.bytes` of the topic. When this happens, all the group coordinator requests that wrote to the batch fail with an UNKNOWN_SERVER_ERROR. This error is non-retriable and disruptive to client applications.\nComments: No comments.", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19759", "url": "https://issues.apache.org/jira/browse/KAFKA-19759"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores\nDescription: In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios. Today, achieving this requires *manual handling* using: * Custom timestamp tracking per key, * Punctuators to periodically scan and remove expired entries, and * Manual emission of tombstones to maintain changelog consistency. These workarounds are: * {*}Inconsistent across applications{*}, and * {*}Operationally costly{*}, as each developer must reimplement the same logic. *Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration. Introduction to new Api's like : StoreBuilder<T> withTTL(Duration ttl); Materialized<K, V, S> withTtl(Duration ttl); When configured: * Each record\u2019s timestamp (from event-time or processing-time) is tracked. * Expired keys are automatically evicted by a background task (via ProcessorContext.Schedule()). * Corresponding tombstones are flushed to changelog. This feature can provide a *TTL abstraction* that simplifies common use cases as: * Maintaining cache-like state (e.g., last-seen values with limited lifespan) * Automatically purging inactive or stale keys without manual cleanup. Points of Risk and Benifits i considered it can bring : * Consistency as automatic changelog tombstones will preserve correctness across rebalances and restores. * Will help to avoid boilerplate punctuator code for manual expiration. * TTL is optional and opt-in; existing stores remain unaffected so backward compatibility would be maintaoined. Example to StateStore/ kTable inferface : KTable<String, UserSession> sessions = builder .table(\"sessions\", Materialized.<String, UserSession, KeyValueStore<Bytes, byte[]>>as(\"session-store\") .withTtl(Duration.ofHours(1)) .withValueSerde(userSessionSerde)); Here, session entries older than 1 hour will be automatically expired and deleted from the local RocksDB store and hence a flush ~ tombstone to changelog topic.\nComments: - Dear [~ankursinha07] - * I see the positives in implementing TTL concept since it's augmenting developer's productivity by eliminating repetitive TTL boilerplate and punctuator code. * It ensures changelog consistency with automatic tombstones. * Opt-in design maintains existing store behavior. Few aspects which we should thought about - * Eviction tasks might increase I/O load on RocksDB, especially in large stores. * For operational needs - A metrics/tracing hooks (e.g., number of expired keys, sweep duration, lag in eviction) to monitor TTL behavior. * During changelog restoration, TTL-expired records should not be reloaded \u2014 implementation must ensure cleanup consistency across restore cycles. * Deleting many keys generates tombstones - the compaction strategy must efficiently reclaim disk space. overall, built-in TTL is a meaningful option simplifying state management and improving reliability in long-running Kafka Streams applications. With careful attention to eviction performance and observability, this feature could significantly elevate the developer experience. - Is there any difference to https://issues.apache.org/jira/browse/KAFKA-4212 \u2013 or is this ticket a duplicate? - Hello [~mjsax] This ticket obviously is relates to KAFKA-4212 and went through the code pushed & discussions, but the concept is not a duplicate. KAFKA-4212 introduced a specific TTL store, whereas with this the proposal is to have *general TTL support for all state stores* with automatic eviction and changelog tombstones. Currently, i implement TTL manually using a timestampedkeyvalstore: context.schedule(Duration.ofDays(scheduledFrequencyDays), PunctuationType.WALL_CLOCK_TIME, timestamp - > { try (var iterator = stateStore.all()) { while (iterator.hasNext()) { var entry = iterator.next(); if (entry.value.timestamp() + Duration.ofDays(retentionDays).toMillis() <= timestamp) { stateStore.delete(entry.key); } } } }); Right now i consider wallclocktime only to trigger cleanup at fixed time irrespective of incoming records to puntuate ~ StreAM_tIME, independent of new records. With this idea is to simplify things as below {{Stores.persistentKeyValueStore(\"state-store\") .withTtl(Duration.ofDays(retentionDays));}} * TTL evictionwould be {*}automatic{*}. * expired entries would generate {*}changelog tombstones{*}. * no manual scheduling or iteration required. * to work consistently for all state stores, not just a specific store type. * .withTtl(duration){{{}{}}} is {*}optional at the API level{*}: passing null would mean {*}no TTL is applied{*}, and the store behaves like a normal persistent store. * Any store without a TTL set will still function normally, maintaining {*}backward compatibility{*}. - Thanks for clarifying.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19758", "url": "https://issues.apache.org/jira/browse/KAFKA-19758"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Weird behavior on Kafka Connect 4.1 class loading\nDescription: I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\" Then there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector. Now I'm running the following image {code:java} FROM quay.io/debezium/connect:3.3.0.Final ENV MAVEN_REPO=\"https://repo1.maven.org/maven2\" ENV GROUP_ID=\"io/debezium\" ENV DEBEZIUM_VERSION=\"3.3.0.Final\" ENV ARTIFACT_ID=\"debezium-openlineage-core\" ENV CLASSIFIER=\"-libs\" COPY log4j.properties /kafka/config/log4j.properties Add OpenLineage RUN mkdir -p /tmp/openlineage-libs && \\ curl \"$MAVEN_REPO/$GROUP_ID/$ARTIFACT_ID/$DEBEZIUM_VERSION/$ARTIFACT_ID-${DEBEZIUM_VERSION}${CLASSIFIER}.tar.gz\" -o /tmp/debezium-openlineage-core-libs.tar.gz && \\ tar -xzvf /tmp/debezium-openlineage-core-libs.tar.gz -C /tmp/openlineage-libs --strip-components=1 RUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-postgres/ RUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-mongodb/ ADD openlineage.yml /kafka/ {code} So is practically debezium connect image with just openlineage jars copied into postgres and mongodb connector folders. When I register the PostgreSQL connector {code:java} { \"name\": \"inventory-connector-postgres\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"postgres\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"postgres\", \"database.server.id\": \"184054\", \"database.dbname\": \"postgres\", \"topic.prefix\": \"inventory\", \"snapshot.mode\": \"initial\", \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\", \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\", \"slot.name\": \"postgres\", \"openlineage.integration.enabled\": \"true\", \"openlineage.integration.config.file.path\": \"/kafka/openlineage.yml\", \"openlineage.integration.job.description\": \"This connector does cdc for products\", \"openlineage.integration.tags\": \"env=prod,team=cdc\", \"openlineage.integration.owners\": \"Mario=maintainer,John Doe=Data scientist,IronMan=superero\", \"transforms\": \"openlineage\", \"transforms.openlineage.type\": \"io.debezium.transforms.openlineage.OpenLineage\" } } {code} I get the following error {code:java} 2025-10-03T14:22:09,761 ERROR || WorkerSourceTask{id=inventory-connector-postgres-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted [org.apache.kafka.connect.runtime.WorkerTask] org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:260) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:180) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:58) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.sendRecords(AbstractWorkerSourceTask.java:415) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:376) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:243) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:298) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:83) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:254) ~[connect-runtime-4.1.0.jar:?] at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) [?:?] Caused by: java.lang.IllegalStateException: DebeziumOpenLineageEmitter not initialized for connector ConnectorContext[connectorLogicalName=inventory, connectorName=postgresql, taskId=0, version=null, config=null]. Call init() first. at io.debezium.openlineage.DebeziumOpenLineageEmitter.getEmitter(DebeziumOpenLineageEmitter.java:176) ~[debezium-openlineage-api-3.3.0.Final.jar:3.3.0.Final] at io.debezium.openlineage.DebeziumOpenLineageEmitter.emit(DebeziumOpenLineageEmitter.java:153) ~[debezium-openlineage-api-3.3.0.Final.jar:3.3.0.Final] at io.debezium.transforms.openlineage.OpenLineage.apply(OpenLineage.java:74) ~[debezium-core-3.3.0.Final.jar:3.3.0.Final] at org.apache.kafka.connect.runtime.TransformationStage.apply(TransformationStage.java:95) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:58) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:208) ~[connect-runtime-4.1.0.jar:?] at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:244) ~[connect-runtime-4.1.0.jar:?] ... 13 more {code} Full logs [^connect-service.log] This is evidence that the emitters map is not shared between the connector and the SMT. The situation becomes weirder if I remove all connectors from the image except PostgreSQL and MongoDB. In that case, the PostgreSQL connector works perfectly. The plugins are in the folder */kafka/connect* (that is, the only `plugin.path` configured folder), each under a dedicated folder with their dependencies. I then started to add more connectors, and it continued to work until I added the SQL Server connector. To summarize, the problem arises when I put one or all of [sqlserver, spanner,vitess]. The commonality for these connectors seems to be that they support multi-task. The others don't. Am I correct that Kafka Connect guarantees that each connector is loaded with an isolated class loader with its dependencies so that the static emitters should be shared between the Connector and the SMT? To add more, if I run the image from 3.2.0.Final (so Kafka 4.0.0) with all connectors, it works fine. I did other tests, and things are more and more weird. All tests were done with *{{plugin.path=/kafka/connect}}* and *KC 4.1* My original tests were with this directory structure {code:java} /kafka/connect |___ debezium-connector-postgres |___ debezium-connector-mongodb |___ debezium-connector-sqlserver{code} In this case, each connector should be isolated from each others (having a dedicated class loader). In that case, the sharing between the connector and SMT does not work for KC 4.0 Then I tried with {code:java} /kafka/connect |___ debezium-connectors |___ debezium-connector-postgres |___ debezium-connector-mongodb |___ debezium-connector-sqlserver{code} So all connectors are not isolated and share the same class loader. In this case, no issue. And I'll say that this is expected. Then I tried with {code:java} /kafka/connect |___ debezium-connectors | |___ debezium-connector-postgres | |___ debezium-connector-mongodb |___ debezium-connector-sqlserver{code} where *{{postgres}}* and *{{mongodb}}* are not isolated (same classloader) and *{{sqlserver}}* is isolated (different classloader), and in this case, it still works. I expected this to fail as with the first setup. The SMT is in the *debezium-core* jar that and each connector has its own copy So in each connector folder, there are: {code:java} debezium-api-3.3.0.Final.jar debezium-common-3.3.0.Final.jar debezium-connector-[connectorName]-3.3.0.Final.jar debezium-core-3.3.0.Final.jar debezium-openlineage-api-3.3.0.Final.jar{code}\nComments: - After some investigations, it looks like KIP-891 broke plugin isolation. For example if we have the same transformation under multiple directories, when Connect tries to execute it, it doesn't call apply with the classloader from the right connector. The issue seems to be in DelegatingClassLoader.findPluginLoader() where it always loops through all directories to find the transformation (or predicate) and keeps the last instance. It then uses the classloader of that instance which may not be the same one as the connector. Adding custom tracing I see: {noformat} ConnectorConfig.getTransformationOrPredicate connector=io.debezium.connector.postgresql.PostgresConnector connectorRange=null range=[3.4.0-SNAPSHOT,3.4.0-SNAPSHOT] ConnectorConfig.getTransformationOrPredicate classloader=PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} OpenLineage instantiated io.debezium.transforms.openlineage.OpenLineage@2675b74a from /tmp/plugins/debezium-connector-sqlserver/debezium-core-3.4.0-SNAPSHOT.jar classloader null PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/} {noformat} The connector is running from PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} and trying to run its transformation using PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/}. The OpenLineage transformation exists in both directories but the DelegatingClassLoader.findPluginLoader() fails to find the appropriate one. We should clarify the expected behavior when a plugins exists under multiple directories. Having multiple times the same connector with the same version should probably be rejected as from the connector configuration, where you have the name and version, you can't decide which one to run. On the other hands, the other connector plugins can exists in multiple copies as long as they are in different directories, thus isolated, as they are always associated with a connector so we can identify a preferred copy. Another inconsistency found while looking at this is that we have default versions for transformations and predicates but the other plugins default to null. For example, the connector configuration submitted via the REST API does not include any version but the computed configuration injects 3.4.0-SNAPSHOT for the transformation. {noformat} config.action.reload = restart connector.class = io.debezium.connector.postgresql.PostgresConnector connector.plugin.version = null errors.log.enable = false errors.log.include.messages = false errors.retry.delay.max.ms = 60000 errors.retry.timeout = 0 errors.tolerance = none header.converter = null header.converter.plugin.version = null key.converter = null key.converter.plugin.version = null name = inventory-connector-postgres predicates = [] tasks.max = 1 tasks.max.enforce = true transforms = [openlineage] transforms.openlineage.negate = false transforms.openlineage.plugin.version = 3.4.0-SNAPSHOT transforms.openlineage.predicate = null transforms.openlineage.type = class io.debezium.transforms.openlineage.OpenLineage value.converter = null value.converter.plugin.version = null {noformat} - [~gharris] [~snehashisp] WDYT? - I have not had a chance to inspect the code or reproduce this for myself, but my first impressions: # I don't think communication via static fields is good practice, and should be avoided if possible. I personally have been on the receiving end of bugs caused by static fields within a single plugin; I also think I personally would not like to debug static fields shared across plugins. # In deployments which make use of the plugin.version configs, changing the pinned version will necessarily change which instance of a static variable is used. So even if the isolation is bug-free, it is trivial for a user or automated tooling to foot-gun a plugin which uses static fields by changing the plugin.version # The concern about the backwards-compatibility when plugin.version is unset is valid. We made some attempts to use the connector's class loader or delegating class loader like in the old implementation, so perhaps the version being explicitly injected is not just a cosmetic problem. IMHO we should investigate the backwards-compatibility problem and try and make a minimal fix to preserve the old behavior when plugin.version is unset. And the affected plugins should eliminate their usage of static variables in order to be well behaved when the plugin.version is set. - ??I don't think communication via static fields is good practice, and should be avoided if possible. I personally have been on the receiving end of bugs caused by static fields within a single plugin; I also think I personally would not like to debug static fields shared across plugins.?? I agree that this is not the best approach, but the problem here is to understand what is possible and what is not. AFAIK, until KC 4.0, this was possible due to plugin isolation, as the same classloader was used to load the connector and its dependencies. Even I think that plugin versioning is compatible also with the isolation (and this seems to be the goal of [KIP-891|https://cwiki.apache.org/confluence/display/KAFKA/KIP-891%3A+Running+multiple+versions+of+Connector+plugins]), if this is something that is not true anymore, I would have expected it to be changed in a major release and not in a minor one. ??In deployments which make use of the plugin.version configs, changing the pinned version will necessarily change which instance of a static variable is used. So even if the isolation is bug-free, it is trivial for a user or automated tooling to foot-gun a plugin which uses static fields by changing the plugin.version?? Can you better clarify this point? ??IMHO we should investigate the backwards-compatibility problem and try and make a minimal fix to preserve the old behavior when plugin.version is unset?? Another thing to say here is that the `DelegatingClassLoader.findPluginLoader()` is not only used for transformations and predicates but also for loading the connectors themselves. So to me, this also seems to break the versioning of the connectors. ??And the affected plugins should eliminate their usage of static variables in order to be well behaved when the plugin.version is set?? Honestly, I'm not getting the point here. I expect that even with a correct management of `plugin.version`, the isolation must be preserved. What happens in the following situation where `plugin.path=opt/plugins/`: {noformat} opt/ plugins/ blue/ foo-connector-1.8/ foo-connector-1.8.jar foo-dependencies-1.0.jar foo-connector-1.9/ foo-connector-1.9.jar foo-dependencies-1.1.jar {noformat} And I register a connector `foo-connector` with version 1.8 and SMT (suppose it is in the foo-dependencies) with version 1.1? Is this possible? In the same way, this breaks the isolation. Isn't it? - 1. I agree, I don't think it's a good pattern. But I think this is only a symptom that highlights the non-deterministic, and IMHO unsafe, way plugins are loaded as I try to explain in 2. / 3. 2. / 3. When looking for a plugin that matches the requested name and version, I think we should prioritize the instance in the classloader of the current connector. If multiple copies of the same plugin exists, the current logic loads the instance from the last plugin path directory ordered alphabetically, which to me is pretty much a random one as it depends on which plugins (and their names) you have installed. If you have a class in your JAR file, you don't want that class to be loaded from another JAR file. - We did/should continue to have a policy of choosing the co-located plugin when all else is equal, see KAFKA-8819. This has unintentionally regressed with KIP-891 and should be fixed. However my point was that this is a very brittle setup that will only work as long as a plugin and it's connector are not isolated from one another (in the same PluginClassLoader). As soon as they are isolated, or once someone uses plugin.version to select another instance, or perhaps once a newer version is installed, I would expect this to break again. Mario, your example plugin path is an incorrect installation, because the plugins are all in the same class loader (corresponding to the blue directory). Only one version will be discovered, and it will be arbitrary based on filesystem order. It is very likely that the different versions of the classes collide and cause more serious problems, so Connect does not consider those cases (and it's also distinct from the bug being discussed here). That directly structure is meant to be used with `plugin.path=opt/plugins/blue/`. - > That directly structure is meant to be used with `plugin.path=opt/plugins/blue/`. Sorry, I wrongly typed the plugin.path. I intend to write `plugin.path=opt/plugins/blue/` > As soon as they are isolated, or once someone uses plugin.version to select another instance, or perhaps once a newer version is installed, I would expect this to break again. I still didn't get your point here. If a connector is isolated, then all other plugins that are listed in the same directly should be loaded with the same classloader. Isn't it? - In your example, foo-connector-1.8 and foo-connector-1.9 are isolated from one another. They are each co-located/not-isolated from their dependencies and any contained SMTs. If you dont specify plugin.version at all, you should get the 1.9 (latest) version of the connector, the co-located version of the dependencies (non-plugins), and the 1.1 (latest) version of the SMT. This is the good path, because the latest plugins are all present in the same location. But there are several bad paths where static variables cannot be shared: * If you reference an SMT that is not co-located (installed in some other directory) * If you specify the 1.0 version of the SMT, the 1.9 version of the connector will be loaded because it's latest * If you specify version 1.8 of the connector, the 1.1 version of the SMT will be loaded because it's latest * Another plugin is installed with a later version (1.2) of the SMT - Now it's clear. Thanks for the detailed explanation. I think this needs to be reported in the documentation since it's crucial. Well, in that case, I have to say that the new versioning feature introduced more flexibility, which is good, but at the same time, you can have the same \"strict\" level of isolation as before, if you configure your versions to maintain it. Previously the isolation was achieved with the directory structure on the file system. In my example, if I want the full isolation for both connector versions, I need to specify for {noformat} 1.8: connector verions = 1.8 and SMT version = 1.0 19.: connector version = 1.9 and SMT version = 1.1 {noformat} That said. I dunno if this full flexibility will be used so often since it can lead to strange behavior, but maybe this is just my limited experience view. - I don't think you and I are using the word \"isolation\" in the same way, and maybe that's the cause of some of the friction here. The boundaries of isolation are still specified at installation time in the same way as they were before, only now users can choose older versions for each plugin. It feels strange (that 1.8+1.1 and 1.9+1.0 are broken) because static field sharing adds context that operators have to keep in mind when doing upgrades. Users are going to want to focus on data-flow/semantics when performing upgrades, not debugging static field aliasing. FWIW, static field sharing between plugins has never been in the mental model for Connect plugins, and the threading model/initialization order/lifecycle of plugins isn't well defined enough for static fields to be safe to use. That is why I want to push back strongly on this practice, while still admitting that we broke backwards compat and need to fix it. - > I don't think you and I are using the word \"isolation\" in the same way, and maybe that's the cause of some of the friction here. The boundaries of isolation are still specified at installation time in the same way as they were before, only now users can choose older versions for each plugin. For me, isolation means loading with the same classLoader. > It feels strange (that 1.8+1.1 and 1.9+1.0 are broken) because static field sharing adds context that operators have to keep in mind when doing upgrades. This could happen not only for static field sharing but for any breaking change. For example, imagine a source connector that renames some field in a non-backward-compatible manner (in a major release) and and SMT should adapt to that change. An operator has to keep in mind this right? > FWIW, static field sharing between plugins has never been in the mental model for Connect plugins, and the threading model/initialization order/lifecycle of plugins isn't well defined enough for static fields to be safe to use. That is why I want to push back strongly on this practice, As said previously, also before this issue, I had a bad feeling about this static field sharing, and now I have the proof that it was a design flaw. I'm currently looking for a different way to achieve it. That said, here the problem is another. If my understanding is correct, please correct me if I'm wrong, in KC version until 4.0 the isolation of the connector was done just with directory separation inside the `plugin.path` folder. Level 1 directories are isolated from each other. This is similar to what happens to war files in application servers, where each war is loaded with a different class loader. With KC 4.1 this is no more true, since all is guided by version and your level 1 directory has no more importance for class loader isolation since you could have a connector in a level 0 folder, let's say connector a, that could then use a plugin (SMT, Converter, etc) that is in another lavel 1 folder, so loaded with a different class loader, just because it is the one thath mathes the desidered version. If my understanding of the past and current status for me, this is a huge behavior change that shouldn't have happened in a minor release, and so we need to be as backward compatible as possible. - For me, two plugins are \"isolated\" when they have different classloaders. This allows them to pull in other dependencies completely independently. You're right that this generalizes to any binary compatibility between classes. If you have classes in the same class loader, they need to be binary compatible and loadable together. Because this is difficult to manage, plugin isolation was implemented to permit plugins to be distributed independently from one another. No your understanding of isolation is not correct, as the directory structure has not changed. A plugin path (config) is made up of plugin path elements (level 0). Each plugin path element contains multiple plugin locations (level 1). Each plugin location may contain multiple jars (level 2+). PluginClassLoaders are instantiated at level 1, and different jar files at level 2+ are included together in the same loader. What has changed in a backwards-incompatible way is that when plugin.version is not specified, the copy in the connector is preferred. Now an arbitrary copy of the latest version is preferred. This should go back to how it was before. - I think we now all agree on the issue. It's clear there's a regression in 4.1.0. We found it via some questionable logic used in Debezium but this is clearly a change in behavior that is not correct. I've started working on a fix, unfortunately so far I've not found a quick/small fix. I hope to have something ready to review later this week. - > PluginClassLoaders are instantiated at level 1, and different jar files at level 2+ are included together in the same loader. [~gharris1727] This matches my understanding. But now, apart from when `plugin.version` is not specified, this will no more true. If you have this situation where the `plugin.path` is `opt/ plugins/ blue/`: {noformat} opt/ plugins/ blue/ foo-connector-1.8/ foo-connector-1.8.jar foo-dependencies-1.0.jar foo-connector-1.9/ foo-connector-1.9.jar foo-dependencies-1.1.jar{noformat} And based on what you said, in KC 4.0, I'll have a classloader for : * foo-connector-1.8 that loads the foo-connector-1.8.jar and foo-dependencies-1.0.jar * foo-connector-1.9 that loads the foo-connector-1.9.jar foo-dependencies-1.1.jar With KC 4.1, if I do not specify any version, the expected behavior will be to have foo-connector-1.9.jar and foo-dependencies-1.1.jar used, and by coincidence, they are in the same level 1 folder. But I can set the version in a way that I could use foo-connector-1.8.jar and foo-dependencies-1.1.jar( suppose here there are SMT or any other plugin), leading to having a plugin loaded from a different class loader. Isn't it? - [~mimaison] Thanks for working on it. - Pointing out some more details here. While classloading transformations or predicates we did make a conscious effort to ensure that, in the absence of any provided version, we try to load them using the classloader of the connector it is associated with. This happens [here|https://github.com/apache/kafka/blob/59f51fb3cac66d8096e647eb90be6c5e4ba9f485/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java#L418] where we pass in the plugin loader for the connector class to the [newPlugin|https://github.com/apache/kafka/blob/59f51fb3cac66d8096e647eb90be6c5e4ba9f485/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java#L376] method, which should load the transformation using the PluginClassLoader of the connector. If the connector and transformation is colocated in the same path this should have ensured the same isolation behaviour as the previous connect version. - Yes we have all the data in getTransformationOrPredicate() but it's not used correctly in newPlugin(). From this path, range is never null, even if it's not set in the configuration because Connect always injects a default version for transformations and predicates in EnrichablePlugin.enrich(). So newPlugin() ends up not using the classloader passed from getTransformationOrPredicate(). - [~snehashisp] It would be great if you could take a look at https://github.com/apache/kafka/pull/20675. Thanks - Hey [~mimaison] , just touching base regarding the 4.1.1 cut. Do we have an idea of how far are we with the PR/fix? Thanks! - [~lianetm] I put a PR out (https://github.com/apache/kafka/pull/20675) last week. Now we're waiting for reviews. - [~lianetm] Sorry for the delay. The fix is merged on trunk and I backported it to 4.1 too. - Great, thanks!", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19757", "url": "https://issues.apache.org/jira/browse/KAFKA-19757"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Mark KIP-932 interfaces as stable for GA release\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19756", "url": "https://issues.apache.org/jira/browse/KAFKA-19756"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Write down the steps for upgrading Gradle\nDescription: Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below. # upgrade gradle-wrapper.properties to the latest gradle # upgrade dependencies.gradle as well # use latest gradle to run command `gradle wrapper`to update gradlew # update wrapper.gradle to ensure the generated \"download command\" works well\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19755", "url": "https://issues.apache.org/jira/browse/KAFKA-19755"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move `KRaftClusterTest` from core module to server module\nDescription: It should include following tasks # rewrite by java # move to server module\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19754", "url": "https://issues.apache.org/jira/browse/KAFKA-19754"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add RPC-level integration tests for StreamsGroupDescribe\nDescription: Add integration tests similar to {{ShareGroupDescribeRequestTest}} and {{ConsumerGroupDescribeRequestTest for StreamsGroupDescribeRequest}}\nComments: - https://github.com/apache/kafka/pull/20632", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19753", "url": "https://issues.apache.org/jira/browse/KAFKA-19753"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Metrics from FetchMetricsManager containing a topic tag are duplicated\nDescription: Hello, Since Kafka 4.1.0, we're experiencing an issue with Kafka Streams metrics for topics containing dots in their names. (I think the problem is also for simple usage of consumers not only kstream) In FetchMetricsManager, methods like {{recordRecordsFetched()}} now create duplicate sensors for the same topic if they contain dot in their name: {code:java} void recordRecordsFetched(String topic, int records) { String name = topicRecordsFetchedMetricName(topic); maybeRecordDeprecatedRecordsFetched(name, topic, records); <-- here we create another sensor if the topic name contains dot Sensor recordsFetched = new SensorBuilder(metrics, name, () -> Map.of(\"topic\", topic)) .withAvg(metricsRegistry.topicRecordsPerRequestAvg) .withMeter(metricsRegistry.topicRecordsConsumedRate, metricsRegistry.topicRecordsConsumedTotal) .build(); recordsFetched.record(records); } {code} It currently record two sensors, one with my original topic name, one time with a topic name with dots replaced by underscore. -While we can work around this by reversing the transformation in our case (replacing underscores back to dots in Micrometer filters) or by removing this specific list of metrics, this does not feel like a long-term solution for us.- Currently using spring-boot and micrometer, it's really hard to remove one of those duplicated metrics. Could a configuration option be added to disable one of the two sensor to avoid having duplicated metrics? Thanks in advance\nComments: - [~chosante] Thanks for the report. Out of curiosity, why are the underscores being replaced with dots? I would have assumed you could simply focus on the new sensor and ignore the deprecated one - Hey you are right, it is not clear, but in our case if we put the same name and tags, under the hood they are considered duplicates when registered and one of them will be discarded. (like we want) EDIT: after doing tests, what's above does not work, the metrics are merged instead of being deduplicated and therefore each increment of one of them is incrementing the same counter - Excuse me. I don\u2019t get the point of the issue. Kafka now create two sensor and then why the deprecated one makes trouble on your application? The deprecated one is kept for the compatibility, and so it means it exists for a while already. - Because it means now that I have two metrics reporting the same number so if I want to know the number of records fetched, my number is now multiplied by 2 in my dashboards reading those metrics. - An example in some app using 4.1.0: !Screenshot 2025-10-03 at 09.44.11.png! and another using 3.9.1: !Screenshot 2025-10-03 at 09.46.42.png! tell me if that's not clear enough, I'll try to explain myself better... - Hi [~chosante], yes that's a downside for a deprecated metric and the expectation is to filter the incorrect metric i.e. one which do not preserve the original topic name, dots replaced with underscore. The reason there exists the deprecated metric is because of backward compatibility, what [~chia7712] also mentioned. - I understand, so I should bring this issue to micrometer instead? Since I cannot easily filter out those metrics myself in the end. I tried multiple hours to find an elegant solution to filter out the new or the deprecated one but currently it is not that easy as everything is auto-configured in spring. I would love to being able to not create them in the first place. I believe that this issue will arise in many projects not only mine as the only thing to do is to use kafka 4.1.0 with spring-boot and spring-kafka and there you go you have duplicated metrics. - I agree with [~chosante] \u2013 I think the thing that is missing (from the KIP?), is just an (internal?) config, to allow people to _disable_ double reporting? We obviously need to double report by default, to not break anybody, but not having a way to opt-out of double reporting, and only getting the new metrics (or maybe even also only getting the old metrics) seems to be something we should support? Given that the KIP is already shipped, it might be simplest to add an internal config, to close this gap? Or would we want to alter the KIP and add a public config for it? - Have you considered using metrics.jmx.exclude to prevent JmxReporter from exposing deprecated metrics? - I could do that, though I am not using Jmx to export kafka metrics, currently using KafkaConsumer.metrics(), KafkaStreams.metrics(), etc... under the hood. As all the apps using spring-kafka. - I'm thinking about the scope of the internal confg. Since the consumer is not the only component with deprecated metrics, perhaps this config should be applied to other components as well? What do you think?", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19752", "url": "https://issues.apache.org/jira/browse/KAFKA-19752"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Move UnifiedLogTest to storage module\nDescription: as title\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19751", "url": "https://issues.apache.org/jira/browse/KAFKA-19751"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix the \"6.6 Java Version\" for branch 3.9\nDescription: see https://lists.apache.org/thread/xv04txcy4wwtpkn1zfqxqf95098ls61h\nComments: - The web site needs to be updated :) - FrankYang0529 opened a new pull request, #747: URL: https://github.com/apache/kafka-site/pull/747 https://issues.apache.org/jira/browse/KAFKA-19751 - FrankYang0529 commented on PR #747: URL: https://github.com/apache/kafka-site/pull/747#issuecomment-3536055823 Preview: <img width=\"564\" height=\"153\" alt=\"Screenshot 2025-11-15 at 1 54 24 PM\" src=\"https://github.com/user-attachments/assets/d46be563-9f7f-470d-9015-fc7135a02e00\" /> - chia7712 merged PR #747: URL: https://github.com/apache/kafka-site/pull/747 - ijuma commented on code in PR #747: URL: https://github.com/apache/kafka-site/pull/747#discussion_r2532633140 ########## 39/ops.html: ########## @@ -1249,10 +1249,10 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, and Java 17 are supported. + Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. Review Comment: Not sure we want to include Java 23, which is a non LTS version that is not supported by anyone else. - chia7712 commented on code in PR #747: URL: https://github.com/apache/kafka-site/pull/747#discussion_r2533091951 ########## 39/ops.html: ########## @@ -1249,10 +1249,10 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, and Java 17 are supported. + Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. Review Comment: That's a good question. Since the 3.9 branch will be a specific version with potentially many patch releases, its lifecycle may be inconsistent with the JDK EOL schedule. Perhaps we should state that JDK 23 is used by the CI but is not officially supported by Apache Kafka due to its EOL status. We can adopt this strategy again when JDK 8 reaches its EOL. - FrankYang0529 commented on code in PR #747: URL: https://github.com/apache/kafka-site/pull/747#discussion_r2534194394 ########## 39/ops.html: ########## @@ -1249,10 +1249,10 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, and Java 17 are supported. + Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. Review Comment: JDK 23 is EOL in March 2025. We can mention it's one of JDK version which is used in CI and it's not officially supported. I will create another PR for this. - FrankYang0529 opened a new pull request, #748: URL: https://github.com/apache/kafka-site/pull/748 https://issues.apache.org/jira/browse/KAFKA-19751 - FrankYang0529 commented on code in PR #747: URL: https://github.com/apache/kafka-site/pull/747#discussion_r2534212318 ########## 39/ops.html: ########## @@ -1249,10 +1249,10 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, and Java 17 are supported. + Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. Review Comment: https://github.com/apache/kafka-site/pull/748 - FrankYang0529 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3542015662 Preview: <img width=\"531\" height=\"99\" alt=\"Screenshot 2025-11-17 at 10 07 52 PM\" src=\"https://github.com/user-attachments/assets/52f9a2c8-1b7a-4164-ab45-a5d7288c21a3\" /> - chia7712 commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2534481282 ########## 39/ops.html: ########## @@ -1249,7 +1249,7 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Java 8, Java 11, Java 17, and Java 21 are supported. Java 23 is used in CI pipelines to verify compatibility, but it's not officially supported because it's non-LTS version. Review Comment: Maybe we could copy the description from trunk? > We build and test Apache Kafka with 17 and 25. The release parameter in javac is set to 11 for the clients and streams modules, and 17 for the rest, ensuring compatibility with their respective minimum Java versions. Similarly, the release parameter in scalac is set to 11 for the streams modules and 17 for the rest. Scala 2.13 is the only supported version in Apache Kafka. - FrankYang0529 commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2538802585 ########## 39/ops.html: ########## @@ -1249,7 +1249,7 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Java 8, Java 11, Java 17, and Java 21 are supported. Java 23 is used in CI pipelines to verify compatibility, but it's not officially supported because it's non-LTS version. Review Comment: @chia7712 Thanks for the review. I update a version like following: ``` Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac to 8 to ensure the generated binaries are compatible with Java 8 or higher (independently of the Java version used for compilation). ``` - chia7712 commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2538996823 ########## 39/ops.html: ########## @@ -1249,7 +1249,9 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac Review Comment: I apologize, but I feel that is too detailed for the users. Perhaps we should simply remind them that JDK 23 is not officially supported ``` Java 8, Java 11, Java 17, and Java 21 are fully supported. Java 23 is used by CI but is not officially supported since it is not an LTS version. ``` - srdo commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2539467907 ########## 39/ops.html: ########## @@ -1249,7 +1249,9 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac Review Comment: Talking about non-LTS seems odd to me, since non-LTS versions can be supported up until they hit end of life? Surely the reason Java 23 is not supported is not that it's non-LTS, it's that Java 23 has hit end of life at this point? - srdo commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2539467907 ########## 39/ops.html: ########## @@ -1249,7 +1249,9 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac Review Comment: Talking about non-LTS seems odd to me, since non-LTS versions can be supported up until they hit end of life? Surely the reason Java 23 is not supported is not that it's non-LTS, it's that Java 23 has hit end of life at this point, and it's an extremely bad idea to still be using it? - srdo commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2539467907 ########## 39/ops.html: ########## @@ -1249,7 +1249,9 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac Review Comment: Talking about non-LTS seems odd to me, since non-LTS versions can be supported up until they hit end of life? Surely the reason Java 23 is not supported is not that it's non-LTS, it's that Java 23 has hit end of life at this point, and it's an extremely bad idea to still be using it? Wouldn't it be good enough to just say that Java 8, 11, 17 and 21 are supported and leave it at that? - chia7712 commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2539477157 ########## 39/ops.html: ########## @@ -1249,7 +1249,9 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac Review Comment: > Talking about non-LTS seems odd to me You are right. We should focus on the EOL status, not the non-LTS status - chia7712 commented on code in PR #748: URL: https://github.com/apache/kafka-site/pull/748#discussion_r2539601910 ########## 39/ops.html: ########## @@ -1249,7 +1249,9 @@ <h4 class=\"anchor-heading\"><a id=\"prodconfig\" class=\"anchor-link\"></a><a href=\"# <h3 class=\"anchor-heading\"><a id=\"java\" class=\"anchor-link\"></a><a href=\"#java\">6.6 Java Version</a></h3> - Java 8, Java 11, Java 17, Java 21 and Java 23 are supported. + Build and test Apache Kafka with Java 8, 11, 17, 21, and 23. Set the release parameter in javac and scalac Review Comment: > Wouldn't it be good enough to just say that Java 8, 11, 17 and 21 are supported and leave it at that? I'm fine with it :) - gharris1727 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3549490179 Java 23 and 24 are significant due to the removal of the SecurityManager, despite being non-LTS versions. If we wish to remove 23 from the documentation, we should replace it with 24 or 25. Otherwise we will invite further questions about whether we support Java versions without the SecurityManager. Also if the 3.9 documentation says 23 but the newer documentation says 21, it appears that our version support has regressed, when I don't think it really has. - gharris1727 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3549496971 That being said, I think the current documentation is accurate and helpful as-is, so I don't think this PR should be merged, -1. Adding support for Java 23+ was hard-fought for by users of Kafka, and this documentation should reflect the reality. Even if 23 and 3.9 go out of support, the documentation can still reflect the reality of the situation. - srdo commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3549504916 > If we wish to remove 23 from the documentation, we should replace it with 24 or 25 The problem is that 3.9 CI runs Java 23. If 3.9 wants to list support for 25, we'd need to backport the Java upgrade (and various changes) to 3.9. IIRC there were no production code changes needed to support 25, just various upgrades for plugins like Spotbugs, so I guess whether backporting those changes is worth it depends on whether there is demand for it? It feels iffy to me to claim support for a Java version for 3.9 that 3.9's CI doesn't run against, so I think 3.9 can at most claim to be compatible with 23, and 22-24 are EOL and no one should be using them anymore. > Also if the 3.9 documentation says 23 but the trunk documentation says 21, it appears that our version support has regressed, when I don't think it really has. Completely agree, but I think the fix should be for the trunk documentation to claim support for Java 25, since that's what CI runs now, as far as I know. I think maybe listing that version was just missed? - chia7712 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3550008696 > Guess someone else thought the same thing, 25 is being added to the docs over here Yes, our team is updating trunk to ensure that users don't assume there is a regression. > Adding support for Java 23+ was hard-fought for by users of Kafka, and this documentation should reflect the reality. Even if 23 and 3.9 go out of support, the documentation can still reflect the reality of the situation. Yes, I was one of the voters in that thread. > I think the current documentation is accurate and helpful as-is How about just adding a description to remind users that Java 23 is EOL? That would be more accurate and helpful. ```java Java 8, Java 11, Java 17, Java 21 and Java 23 are fully supported, but users should be aware that Java 23 has reached End-of-Life (EOL). ``` - gharris1727 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3550035053 > How about just adding a description to remind users that Java 23 is EOL? That would be more accurate and helpful. I don't think that information is necessary to convey here. That's something between the user and the provider of the JRE, it doesn't involve Apache Kafka at all. Some providers may offer longer support contracts out of sight. - chia7712 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3550146178 > I don't think that information is necessary to convey here. That's something between the user and the provider of the JRE, it doesn't involve Apache Kafka at all. Some providers may offer longer support contracts out of sight. Sounds good. Another way to offer helpful guidance to users would be to backport the following description from trunk to 3.9. Users should be aware that java 23 is supported by Kafka 3.9. However, they must also understand the importance of using an LTS or extended support version for production environments. WDYT? ```java We generally recommend running Apache Kafka with the most recent LTS release (Java 21 at the time of writing) for performance, efficiency and support reasons. From a security perspective, we recommend the latest released patch version as older versions typically have disclosed security vulnerabilities. ``` - ijuma commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3550757067 The readme description has Apache Kafka developers as the audience (and hence why it's detailed and covers the compilation aspects, etc.). On the other hand, the page being updated is for users of Apache Kafka and hence it should be simpler. Running unit and integration tests is the minimum required to claim support, but it doesn't actually exercise the system fully. You'd want to run complex workloads for long periods of time, performance tests, stress tests, system tests, etc. Many vendors do this, but they typically do this with one or more of the LTS Java versions. From that perspective and from the perspective of having no known CVEs, it's actually a very good idea to encourage users to use one of the LTS versions. I think it would be reasonable to backport the Java 25 patches and add that instead of Java 23. That is the cleanest option and gives users who have not migrated to KRaft an opportunity to use the latest Java version. - ijuma commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3550779826 > Guess someone else thought the same thing, 25 is being added to the docs over here [apache/kafka#20917](https://github.com/apache/kafka/pull/20917) Ah, yes. That text is actually much more helpful. I am also ok leaving 3.9 as it is since the text for newer versions is fine. - chia7712 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3551441828 > I think it would be reasonable to backport the Java 25 patches and add that instead of Java 23. That is the cleanest option and gives users who have not migrated to KRaft an opportunity to use the latest Java I love this suggestion. The primary reason I want to have the next patch release is to make non-Kraft users happy. Support for Java 25 is definitely a welcome Christmas gift @gharris1727 @FrankYang0529 What do you think? We may need some time to address this, as supporting Java 25 will require us to update Gradle and its related plugins. - FrankYang0529 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3551561343 > What do you think? We may need some time to address this, as supporting Java 25 will require us to update Gradle and its related plugins. Agree, some users still use ZK mode and may need more time to migrate to KRaft. It's good to include latest LTS (Java 25 at this moment) in 3.9 branch. - chia7712 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3551593835 I have opened KAFKA-19897 for java 25, and it's marked as a blocker for 3.9.2. We should hold this PR until KAFKA-19897 is resolved - FrankYang0529 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3553121372 Based on current discussion, I changed the content to `Build and test Apache Kafka with Java 8, 11, 17, 21, and 25.`. We can merge this PR after https://issues.apache.org/jira/browse/KAFKA-19897 is finished. - mingyen066 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3571651225 Bad news: According to the [Gradle compatibility matrix](https://docs.gradle.org/current/userguide/compatibility.html), Since Kafka 3.9 supports Java 8, we're stuck with Gradle 8.14 max. But Java 25 requires Gradle 9.10+. So If we need to keep Java 8 support, we can only go up to Java 23 or 24. - srdo commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3571731571 @mingyen066 This may be a bigger lift, but I believe Gradle supports decoupling the JDK used for invoking Gradle from the JDK used for compiling the project code, via the \"toolchains\" concept. https://docs.gradle.org/current/userguide/toolchains.html#toolchains Maybe it would be worth looking at that for trunk too, it could enable Kafka to upgrade the JDK faster in the future, even before Gradle gains support? Anyway, just an idea, not sure if it's worth doing? - chia7712 commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3571947215 > Anyway, just an idea, not sure if it's worth doing? That is good idea to me. To avoid introducing major changes to 3.9, let's focus on using JDK 25 just to run tests. We could implement this by creating a specific test task that uses the JDK 25 toolchain for example: ``` task test21(type: Test, dependsOn: compileJava) { maxParallelForks = maxTestForks ignoreFailures = userIgnoreFailures maxHeapSize = defaultMaxHeapSize jvmArgs = defaultJvmArgs javaLauncher = javaToolchains.launcherFor { languageVersion = JavaLanguageVersion.of(25) } testLogging { events = userTestLoggingEvents ?: testLoggingEvents showStandardStreams = userShowStandardStreams ?: testShowStandardStreams exceptionFormat = testExceptionFormat displayGranularity = 0 } logTestStdout.rehydrate(delegate, owner, this)() exclude testsToExclude useJUnitPlatform { includeEngines 'junit-jupiter' } retry { maxRetries = userMaxTestRetries maxFailures = userMaxTestRetryFailures } testClassesDirs = sourceSets.test.output.classesDirs classpath = sourceSets.test.runtimeClasspath } ``` > Maybe it would be worth looking at that for trunk too, it could enable Kafka to upgrade the JDK faster in the future, even before Gradle gains support? It might also make branch switching easier, if Gradle were configured to download a JDK instead of running with the one that happens to be installed locally? I suggest keeping this solution specific to 3.9 for now, unless faster JDK updates become a priority on trunk - srdo commented on PR #748: URL: https://github.com/apache/kafka-site/pull/748#issuecomment-3572091687 > I suggest keeping this solution specific to 3.9 for now, unless faster JDK updates become a priority on trunk We'll see with the next couple of upgrades I guess. I feel like Gradle definitely delayed testing around the Java 24 upgrade though, so it's been a problem in the past.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19750", "url": "https://issues.apache.org/jira/browse/KAFKA-19750"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Address the compileClasspath resolution warnings for the `releaseTarGz` task\nDescription: {code:java} [warn] Resolution of the configuration :tools:tools-api:runtimeClasspath was attempted from a context different than the project context. Have a look at the documentation to understand why this is a problem and how it can be resolved. This behavior has been deprecated. (1) - [warn] Resolution of the configuration :tools:tools-api:runtimeClasspath was attempted from a context different than the project context. Have a look at the documentation to understand why this is a problem and how it can be resolved. This behavior has been deprecated.This will fail with an error in Gradle 9.0. - Locations- `<undefined>`- `:core:releaseTarGz` {code} The issue was introduced by [https://github.com/apache/kafka/pull/13454] `tools-api` is already in core module runtime path, so adding it to `releaseTarGz` causes the resolution conflicts, which will be a fatal error in gradle 9\nComments: - this could be resolved by gradle upgrade (see https://github.com/apache/kafka/pull/19513#issuecomment-3355314891) - [~chia7712], can I take over on this issue? - go ahead - Hi [~brandboat] You may find this comment useful: https://github.com/apache/kafka/pull/19513#issuecomment-3358664101", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19749", "url": "https://issues.apache.org/jira/browse/KAFKA-19749"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: The `version-mapping` of kafka-features.sh should work without requiring the bootstrap\nDescription: It shows the following error. {code:java} chia7712@fedora:~/project/kafka$ ./bin/kafka-features.sh version-mapping usage: kafka-features [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER) {describe,upgrade,downgrade,disable,version-mapping,feature-dependencies} ... kafka-features: error: one of the arguments --bootstrap-server --bootstrap-controller is required {code} By contrast, storage tool works well {code:java} chia7712@fedora:~/project/kafka$ ./bin/kafka-storage.sh version-mapping metadata.version=27 (4.1-IV1) kraft.version=1 transaction.version=2 group.version=1 eligible.leader.replicas.version=1 share.version=0 streams.version=0 {code}\nComments: - related discussion: https://github.com/apache/kafka/pull/20248#discussion_r2395108942", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19748", "url": "https://issues.apache.org/jira/browse/KAFKA-19748"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Memory leak using range scans\nDescription: We introduced a regression bug in AK 4.1.0 release, when range scans are use. We are leaking some metrics/sensors. Thanks to [~eduwerc] and [~hermankj] for reporting this issue. Anybody who is using iterator, ie, any type of all/rang/prefix scan etc is affected. Note that certain DSL operators like session-windows, sliding-windows, stream-stream joins, and FK-join all use range scans internally, so the number of affected users is expected to be quite high.\nComments: - > also update the Kafka web page with a note that people should be careful and maybe not even upgrade to Kafka Streams 4.1.0. Hi [~mjsax], I can make a PR for the documentation changes. Would [upgrade-guide.html|https://github.com/shashankhs11/kafka/blob/trunk/docs/streams/upgrade-guide.html] be an ideal spot for this note? - Yes, the upgrade-guide would be the right place. I would also add a note to the top-level upgrade note in addition: [https://github.com/apache/kafka/blob/trunk/docs/upgrade.html#L194] - Done! Made a PR for this - [#20639|https://github.com/apache/kafka/pull/20639] - shashankhs11 opened a new pull request, #729: URL: https://github.com/apache/kafka-site/pull/729 Added a note regarding the memory leak bug in the documentation. ### Preview of `/41/documentation/streams/upgrade-guide` <img width=\"1452\" height=\"830\" alt=\"upgrade-guide\" src=\"https://github.com/user-attachments/assets/fcb52d08-1351-4f03-ad89-baf1b9e8eac0\" /> ### Preview of `/41/documentation/#upgrade` <img width=\"1452\" height=\"830\" alt=\"upgrade\" src=\"https://github.com/user-attachments/assets/1114d3f1-f6ca-4a00-885f-f677d18d4095\" /> - mjsax merged PR #729: URL: https://github.com/apache/kafka-site/pull/729 - mjsax commented on PR #729: URL: https://github.com/apache/kafka-site/pull/729#issuecomment-3374531674 Thanks for the PR!", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19747", "url": "https://issues.apache.org/jira/browse/KAFKA-19747"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improve handling of failed push telemetry request\nDescription: When a failure occurs with a push telemetry request, any exception is treated as fatal, causing telemetry to be turned off. We can enhance this error handling to check if the exception is a transient one with expected recovery, and not turn off telemetry in those cases.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19746", "url": "https://issues.apache.org/jira/browse/KAFKA-19746"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Tools | Fix order of arguments to assertEquals in unit test\nDescription: This sub-task is intended to fix the order of arguments passed in assertions in test cases within the tools package.\nComments: No comments.", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19745", "url": "https://issues.apache.org/jira/browse/KAFKA-19745"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add support for podman when running system tests\nDescription: Add support for using podman within system tests.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19744", "url": "https://issues.apache.org/jira/browse/KAFKA-19744"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Restore metrics are calculated incorrectly\nDescription: Restore metrics are calculated by measuring the time of the init method: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java#L135] But the restoration process has been moved to the poll loop: [https://github.com/apache/kafka/commit/b78d7ba5d58fa77b831244444eef005a62883f9b]\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19743", "url": "https://issues.apache.org/jira/browse/KAFKA-19743"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: [DISCUSS] Exploring Compile-time AST Generation as an Alternative Approach for Kafka Protocol Classes\nDescription: h2. 1. Background & Motivation Currently, Kafka uses a JSON template-based pre-generation approach to create protocol classes under {{{}org.apache.kafka.common.message{}}}. This solution is mature, stable, and provides excellent multi-language support. However, I've been considering whether we could provide an alternative *compile-time code generation* approach for the Java ecosystem, similar to the AST modification technology used by Lombok. This approach might offer better development experience in certain scenarios. h2. 2. Proposal Overview {*}Core Idea{*}: Use annotation processors to directly modify the AST during compilation, dynamically generating complete implementations of protocol classes instead of pre-generating Java source files. {*}Example Implementation Concept{*}: {code:java} @KafkaMessage(apiKey = 1, version = \"0.1.0\") public class FetchRequestSpec { @ProtocolField(type = \"string\", order = 1) private String groupId; @ProtocolField(type = \"int16\", order = 2) private short acks; // More fields... }{code} h2. 3. Comparative Analysis ||Aspect||Current Template Approach||Compile-time AST Approach||Hybrid Possibility|| |*Development Experience*|Requires viewing generated source files|Cleaner source code, Lombok-like|AST during development, pre-gen for release| |*Build Process*|Explicit pre-generation step|Integrated into standard compilation|Configurable generation strategy| |*Debugging Support*|\u2705 Full source-level debugging|\u26a0\ufe0f Requires IDE plugin support|Source output on demand| |*Multi-language Support*|\u2705 Single definition, multiple outputs|\u274c Java-only|Preserve existing multi-language capabilities| |*Performance Impact*|One-time generation at build time|Regeneration on each compilation|Smart caching mechanism| h2. 4. Potential Value * {*}Faster Development Iteration{*}: Immediate compilation after protocol definition changes, no pre-generation step required * {*}Source Code Cleanliness{*}: Avoid polluting codebase with\u5927\u91cf generated boilerplate code * {*}Type Safety{*}: Stricter protocol constraint checking at compile time * {*}Alignment with Modern Java Patterns{*}: Similar to Quarkus, Micronaut's compile-time processing philosophy h2. 5. Feasibility Considerations I understand the potential challenges this approach might face: * Compatibility with existing multi-language ecosystem * Ensuring debugging experience * Increased build system complexity * Community learning curve Therefore, I suggest we could: # {*}Offer as Optional Approach{*}: Doesn't disrupt existing workflow, serves as alternative during development # {*}Incremental Implementation{*}: Start with small scope or new modules for experimentation # {*}Toolchain Support{*}: Provide source output mode to ensure debugging capability h2. 6. Next Steps & Recommendations If the community finds this direction interesting, I'm willing to: * Create a Proof of Concept (POC) implementation to demonstrate concrete results * Participate in relevant discussions and design work * Assist with performance benchmarking and compatibility validation Thank you for taking the time to review this proposal. I look forward to further discussing this idea with the community, and whether it is ultimately adopted or not, I appreciate the opportunity to share my thoughts with the community.\nComments: No comments.", "output": "Status: Open\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19742", "url": "https://issues.apache.org/jira/browse/KAFKA-19742"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Acquisition lock timeout renewal in share consumer explicit mode\nDescription: Ticket for KIP-1222. The KIP proposes adding a new AcknowledgementType to be used in ShareFetch and ShareAcknowledge RPCs which will allow renewal of acquisition lock timeout with causing a state transition of the record in the share partition.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19741", "url": "https://issues.apache.org/jira/browse/KAFKA-19741"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove deprecated metrics in Rename AssignmentsManager and RemoteStorageThreadPool\nDescription: Please see the KIP: https://cwiki.apache.org/confluence/x/6oqMEw\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19740", "url": "https://issues.apache.org/jira/browse/KAFKA-19740"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Depreacte broker-level config group.coordinator.rebalance.protocols\nDescription: https://github.com/apache/kafka/pull/20466#discussion_r2379695582\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19739", "url": "https://issues.apache.org/jira/browse/KAFKA-19739"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade commons-validator to 1.10.0\nDescription: In KAFKA-19359, the commons-beanutils transitive dependency was force bumped in the project to avoid related CVEs. The commons-validator already has a new release, which solves this problem: [https://github.com/apache/commons-validator/tags] The workaround could be deleted as part of the version bump.\nComments: - After bumping the commons-validator, the upgraded commons-beanutils is used: {code:java} | +--- commons-validator:commons-validator:1.10.0 | | +--- commons-beanutils:commons-beanutils:1.11.0{code} There is still old commons-beanutils in the project, but that comes from checkstyle which is not production related: {code:java} \\--- com.puppycrawl.tools:checkstyle:10.20.2 +--- commons-beanutils:commons-beanutils:1.9.4{code} Upgrading the checkstyle should be done in another ticket.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19738", "url": "https://issues.apache.org/jira/browse/KAFKA-19738"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Improve testing of OffsetsRequestManager to better match OffsetFetcher\nDescription: OffsetsRequestManager lacks the same level of testing compared to the code it eventually intends to replace, OffsetFetcher. The addition of new unit tests could potentially surface issues with its implementation and assist with the resolution of KAFKA-18216 and KAFKA-18217\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19737", "url": "https://issues.apache.org/jira/browse/KAFKA-19737"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix PluginUtils#pluginLocations warning log not being printed\nDescription: This fix addresses an issue where no warning log was displayed when the `plugin.path` configuration contained empty string elements.\nComments: - After these two PRs\uff1a[https://github.com/apache/kafka/pull/20638] and [https://github.com/apache/kafka/pull/20612] , the problem no longer exists.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19736", "url": "https://issues.apache.org/jira/browse/KAFKA-19736"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: KAFKA-15665 prevents safe reassignments from completing\nDescription: The implementation for KAFKA-15665 is a little too strict - it requires that all replicas in the target replica set be in ISR. This means if we have a reassignment like [1, 2, 3] -> [2, 3, 4] where broker 2 was unhealthy and lagging in replication, reassignment would not complete. I believe it should work to instead just check the following: - Added replicas are in ISR (newTargetISR) - Resulting ISR (newTargetISR) is not under-min-isr\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19735", "url": "https://issues.apache.org/jira/browse/KAFKA-19735"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add automatic commit offset caching in subscribe mode\nDescription: Suppose that no new messages have been written to the target topic for a long time, for example, the producer has not generated new data for nearly an hour. On the consumer side, when the user sets enable.auto.commit=true, the OFFSET_COMMIT request will still be repeatedly submitted every 5 seconds. This not only wastes network resources but also increases the burden on __consumer_offset. Therefore, maybe we can add a cache for committed offsets to check if the currently committed offset is consistent with the most recently successfully committed offset. If they are the same, this offset commit can be ignored. Of course, all the above applies to the subscribe mode. Advantages: * Reduce unnecessary network requests * Alleviate the processing pressure on the broker side * Relieve the pressure of log cleaning for __consumer_offset Disadvantage: * There may be hidden bugs that need to be discussed and identified\nComments: - Hey, in case it helps: * I filed something related a while back here https://issues.apache.org/jira/browse/KAFKA-16233 * why subscribe mode only? I expect that whatever improvement we want to consider in this area would be for consumers with auto.commit enabled + group Id (regardless of using subscribe or not). A consumer using assign can auto commit too (if it has group ID + auto.commit enabled) - [~lianetm] Sorry for the late reply. I think in assign mode, apart from the current consumer being able to modify the offset of the corresponding TopicPartition, the Admin can also make modifications. Let\u2019s assume such a scenario: the offset cached by the consumer is 10, and all subsequent requests to commit offset 10 will return successfully quickly without being sent to the broker. At this point, if the Admin is used to set the offset to 11, the consumer will not be aware of this change. As a result, the consumer caches an invalid offset, which is inconsistent with expectations. WDYT ?", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19734", "url": "https://issues.apache.org/jira/browse/KAFKA-19734"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add application-id as a tag to the ClientState JMX metric\nDescription: As a follow-on to the improvements introduced in [KIP-1091|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1091%3A+Improved+Kafka+Streams+operator+metrics] it would be useful to add the application-id as a tag to the `client-state` metric. This allows Kafka Streams developers and operators to connect metrics containing a `thread-id` (which embeds the `application-id`) across separate deployments of Kafka Streams instances, which are members of the same logical application. KIP-1221 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1221%3A+Add+application-id+tag+to+Kafka+Streams+state+metric]\nComments: No comments.", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19733", "url": "https://issues.apache.org/jira/browse/KAFKA-19733"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Clients | Fix order of arguments to assertEquals()\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19732", "url": "https://issues.apache.org/jira/browse/KAFKA-19732"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Backport KAFKA-19716 fix to 4.0 and 4.1\nDescription: KAFKA-19716\nComments: - Does the bug occur in 3.9 as well? If so, it would be goo to backport it to 3.9 since we will have a patch release for that branch", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19731", "url": "https://issues.apache.org/jira/browse/KAFKA-19731"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove AppInfoParser deprecated metrics\nDescription: Ref: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1120%3A+AppInfo+metrics+don%27t+contain+the+client-id] We should remove the metrics whose tags do not contain a client-id in Kafka 5.0.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19730", "url": "https://issues.apache.org/jira/browse/KAFKA-19730"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: StreamsGroupDescribe result is missing topology when topology not configured\nDescription: StreamsGroupDescribe does not include a topology if the topology is not configured. The problem is that it relies on `ConfiguredTopology` which may not be present or empty. Instead, we should use the unconfigured topology when the configured topology is not present\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19729", "url": "https://issues.apache.org/jira/browse/KAFKA-19729"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove addMetric(MetricName metricName, Measurable measurable) method.\nDescription: see\uff1ahttps://github.com/apache/kafka/pull/20543#discussion_r2371009016\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19728", "url": "https://issues.apache.org/jira/browse/KAFKA-19728"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Disk space is not being freed\nDescription: I found that the remoteFetch thread is holding the file, preventing the disk space from being freed. !image-2025-09-24-10-38-25-161.png! However, I didn't encounter this problem with the earlier 0.0.1-SNAPSHOT build, which used Kafka 3.8.1. The current issue appears in version 1.0.0 together with Kafka 3.9.1. !image-2025-09-24-10-38-36-572.png! I'm not sure whether this issue is caused by the tiered storage plugin or by Kafka itself. Could someone help take a look?\nComments: - Could someone please take a look at this issue? It is causing disk space to be freed extremely slowly.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19727", "url": "https://issues.apache.org/jira/browse/KAFKA-19727"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make open telemetry object instantiation configurable\nDescription: After the 3.7 release of Kafka, the java client will emit many warning messages and more objects deployed due to an open telemetry object instantiation (possibly due to KIP-714). It appears the open telemetry feature was enabled by default and there's no way for a user to disable it. Users receive the INFO message defined [here |https://github.com/apache/kafka/blob/3.7.2/clients/src/main/java/org/apache/kafka/common/telemetry/internals/ClientTelemetryReporter.java#L479] after activating INFO log level for \"org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter\" class. This log line appears lots of times. It appears that the occurrence of this info messages shows ClientTelemetry objects are being created if the feature is not explicitly disabled. Users would like this feature to be opt-in and not enabled by default as it is.\nComments: - [~lacey] Thanks for raising it but it's not a bug rather the feature is per design in KIP-714, i.e. feature default behaviour as `opt-in`. - The user has the ability to disable this feature - enable.metrics.push=false - [~apoorvmittal10] You are entirely correct that KIP-714 tells us that client metrics are enabled by default on the clients and can be disabled by config. Of course, metrics push will only occur for a modern client, connected to a cluster with client telemetry enabled, and a valid metrics subscription. I wonder whether there is scope to adjust the logging.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19726", "url": "https://issues.apache.org/jira/browse/KAFKA-19726"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Throw an exception if client tries to override topic replication factor with a value that it < min.insync.replicas\nDescription: The goal of this ticket is to start a KIP with the following proposed changes: Currently, the broker overrides settings upon a client initial request. I.e. there's some logic to consolidate/merge settings from the broker and from the client. During that step, it should be doable to throw an exception if there's an invalid configuration. This ticket is specifically for throwing an exception if the client overrides `replication.factor` with a value that is less than `min.insync.replicas` specified by the broker. This check should happen where the \u201cmerging\u201d logic happens, on the broker side.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19725", "url": "https://issues.apache.org/jira/browse/KAFKA-19725"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Deprecate --property and replace with --reader-property in console producer\nDescription: The --property argument in kafka-console-producer.sh should be deprecated and replaced with --reader-property.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19724", "url": "https://issues.apache.org/jira/browse/KAFKA-19724"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Global stream thread ignores all exceptions\nDescription: {{globalStreamThread}} in {{KafkaStreams}} class ignores all exceptions and fails to apply the user-provided {{StreamsUncaughtExceptionHandler}} in: {code:java} public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler) {code} This can lead to streams being stuck in faulty state after an exception is thrown during their initialization phase (e.g. failure to load the RocksDB native library). The exception isn't logged, so debugging such problem is difficult. From my understanding of the {{b62d8b97}} commit message, the following code is unnecessary as the {{globalStreamThread}} can't be replaced and the issue description from KAFKA-12699 doesn't apply to it. Removing it should help. {code:java} if (globalStreamThread != null) { globalStreamThread.setUncaughtExceptionHandler((t, e) -> { } ); } {code}\nComments: - Thanks for reporting the issue. \u2013 Not logging some exception is for sure a problem. This should be easy to fix. While I agree that `globalStreamThread.setUncaughtExceptionHandler((t, e) -> \\{ });` is not useful any longer, I don't think removing this code solves the problem. \u2013 In older version of KS, we use the Java uncaught exception handler, but we did move off it, in favor of a custom implementation. However, this custom implementation which allows to restart dying thread, only makes sense for `StreamsThreads`, but not for the `GlobalThread` \u2013 of the `GlobalThread` dies, we can only close KafkaStreams client with an ERROR state (what we do). [In AK 4.0, we remove the old code, and it seems we did not do a proper cleanup; that's all.] But yes, we should fix the logging issue for sure. - You're right, now I see that the first call to {{setUncaughtExceptionHandler}} on {{globalStreamThread}} is using the new implementation and sets it right. I might have incorrectly pinpointed my issue. I've verified that the problem I've encountered happens when {{initialize()}} call in {{GlobalStreamThread:276}} throws an {{ExceptionInInitializerError}} due to RocksDB failing to load the library in static initializer of {{{}DBOptions{}}}. This exception was only caught inside the {{java.lang.Thread}} exception handler of the {{{}globalStreamThread{}}}. - [~mikkolaj] [~mjsax] I want to help Kafka development for the first time. I have the ticket but I don't know what to do exactly. Can you guide me on what to do about it? - Hi [~fatihcelik], thanks for your interest in solving the issue. The problem I see is that {{GlobalStreamThread}} lacks handling for {{Error}} instances (e.g. {{ExceptionInInitializerError}} or {{{}UnsatisfiedLinkError{}}}). They aren't caught in {{streamsUncaughtExceptionHandler}} and propagate to {{{}Thread.UncaughtExceptionHandler{}}}, which might be a no-op. As a result the global thread can die silently. Let's wait for [~mjsax]'s opinion, but I think it would be possible to handle it by implementing a {{Thread.UncaughtExceptionHandler}} for {{{}globalStreamThread{}}}, which logs the error and calls {{{}closeToError(){}}}, so that the stream can transition to {{ERROR}} state. - Thank you for the explanation [~mikkolaj]. So, for now we're waiting to hear from [~mjsax]. - While this might work, I am wondering if it's really how we want/need to do it? Where does \"UnsatisfiedLinkError\" happen, and how could we test it properly? [~mikkolaj] can you clarify if the problem is only missing logging, or also that KafkaStreams does not go into ERROR state? Looking into the code, `run()` first call `initialize()` which has a try-catch that covers the whole method, and would set `startupException`, and return `null`, what should actually transit the state into PENDING_SHUTDOWN and DEAD, and log an error: {code:java} log.error(\"Error happened during initialization of the global state store; this thread has shutdown.\"); {code} Of course, if the error does not happen inside `initialize()` or we don't handle it properly (we catch `Exception` but not `Throwable`) this code won't work as expected... Could it be, that catching Throwable instead of Exception would be enough to fix the issue? We also catch Throwable in `StreamsThread#run()`... - It is both missing logging and stream not going into {{ERROR}} state. Replication of {{UnsatisfiedLinkError}} hitting the no-op error handler: # Include {{kafka-streams-scala}} dependency in {{4.1.0}} version # Create a Scala app with a KafkaStreams topology which relies on RocksDB global state store, e.g: {code:scala} val builder = new StreamsBuilder() val input: KStream[String, String] = builder.stream[String, String](\"input-topic\") val globalTable: GlobalKTable[String, Int] = builder.globalTable[String, Int](\"global-table-topic\") val joinedStream: KStream[String, Int] = input.join(globalTable)((key, _) => key, (_, joinedValue) => joinedValue) joinedStream.to(\"output-topic\") val streams: KafkaStreams = new KafkaStreams(builder.build(), config) streams.setUncaughtExceptionHandler { _ => // currently I won't be called SHUTDOWN_APPLICATION } streams.start() {code} # Set up required topics # Run the app on Linux with {{/tmp}} directory mounted with {{noexec}} option (or set {{ROCKSDB_SHAREDLIB_DIR}} env variable to point to such directory) # No error logging present and {{globalStreamThread}} dies Catching {{Throwable}} as you suggested will fix my issue but I'm not sure if it's a good solution in general. Will it work in more serious cases, e.g. {{OutOfMemoryError}} being thrown? As per [Java docs|https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html] in general {{Error}} shouldn't be caught (and in Scala I've usually seen [NonFatal|https://www.scala-lang.org/api/2.13.3/scala/util/control/NonFatal$.html] extractor being used instead of catching {{{}Throwable{}}}). - OutOfMemoryError is also a Throwable. \u2013 Yes, in general, Error / Throwable should not be caught. But if you just try to log, it should be ok? The problem with the Java uncaught exception handler is, that the thread is already dead. So I think you can do even less? \u2013 I am also not objecting to use the Java uncaught exception handler as a last line of defense (but I am not sure if we need to do this, when we can also get it done by catching Throwable), but if we go this route, we should do it for all threads, including `StreamThread` and `StateUpdaterThread`? Maybe [~lucasbru] or [~cadonna] have an opinion? - I was wondering if maybe going a similar way to {{NonFatal}} would be a good approach \u2013 catch only these exceptions / errors that we know won't interrupt executing the regular cleanup code (e.g. {{UnsatisfiedLinkError}} in {{initialize()}} method) and propagate other ones to some {{UncaughtExceptionHandler}} defined in the user code, giving user the ability to forcefully shut down the application if something really bad happens. - We specified the Streams uncaught exception handler to handle {{Throwable}}. If we want to keep that, we need to catch {{Throwable}}s. And we actually already do that for stream threads. The Java uncaught exception handler also handles {{Throwable}}s and we wanted to have a custom uncaught exception handler for Streams. To be fair, the global stream thread is a bit special since it cannot be replaced as the stream threads. So, I would either: * log the failure in the Java uncaught exception of the global stream thread and let the Kafka Streams instance just fail with the throwable, or * catch the {{Throwable}} and log (and let the Kafka Streams instance fail). - I don't fully understand the situations mentioned. I might be misunderstanding, and I'd appreciate it if you could correct me. Currently, if a handler isn't set with \\{{setUncaughtExceptionHandler}}, I can already see the error in the logs. The error occurs when \\{{setUncaughtExceptionHandler()}} is called. Is the change here really to catch and log a \\{{Throwable}} object on the relevant line? It does this when setUncaughtExceptionHandler isn't called. (Also, the application doesn't shutdown despite receiving the error.) - Not sure if I can follow [~mikkolaj]? All exception that go into the StreamsUncaughtExceptionHandler are fatal exception. Or course, there is still different \"classes\" of fatal ones, but not sure if it would really matter much? KS tries to call the handler in any case and let the user react to it. {quote}The error occurs when {{setUncaughtExceptionHandler()}} is called {quote} Yes, that's \"just\" a bug. By default, when you call `new KafkaStreams()` there is a default exception handler set (also on the GlobalThreadThread; for this case, via the constructor). When you call `KafkaStreams#setUncaughExceptionHandler()` the issue happens because we first set the user provided handler correctly ([https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L467-L471)] but later overwrite it incorrectly ([https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L475-L478)|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L475-L478] Nevertheless, the initialization phase is not covered: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L276-L292] and I would assume that `UnsatisfiedLinkError` would be thrown here, not invoking any handler. The regular processing-loop is covered though, and we call the handler (however, we don't catch Throwable): [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L295-L343] - My case can be fixed by catching the error [here|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L434] and it will get re-thrown by the thread calling {{streams.start()}} [here|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L464]. I was trying to advocate against catching {{Throwable}} because I've seen posts like [this|https://stackoverflow.com/a/352793], saying that when a {{VirtualMachineError}} is thrown there are no guarantees for executing any code reliably, including error handling or passing the error to a custom exception handler. Java's {{UncaughtExceptionHandler}} seems to me like the last line of defense, where user could try to detect these problems and take action like shutting down the app. Setting these handlers to no-op [here|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L472-L478] looks like ignoring everything that's unexpected. I might be wrong though, so I'm not pushing for any specific solution. Catching {{Throwable}} should probably handle most of the cases just fine. - Thank you [~mjsax] , everything is much clearer now. However, we need to fix one issue. The {{UnsatisfiedLinkError}} does not appear in the lines specified here. [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L295-L343] It's appearing exactly here. [~mikkolaj] already mentioned this in their last comment: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L276] - [~mikkolaj], yes, this aligns to what I said earlier: https://issues.apache.org/jira/browse/KAFKA-19724?focusedCommentId=18021744&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18021744 \u2013 catching Throwable instead of Exception should(*) address the issue. (*) The concern about catching Throwable is totally valid; no disagreement. We do it in Kafka Streams basically as a \"best effort\" approach, and are well aware that it might not always work. {quote}Java's {{UncaughtExceptionHandler}} seems to me like the last line of defense, where user could try to detect these problems and take action like shutting down the app {quote} I am not deep enough into JVM details, but I am wondering if this is actually guaranteed? After a Throwable, the JVM is in \"very bad state\" from my understanding, and I am not sure what guarantees there are that the handler is invoked, and what code would be guaranteed to get executed inside the handler? That we set the handler as a no-op, is clearly a bug (as said above already): We actually set the handler first [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L467-L471] and incorrectly set it as no-op right afterwards. {quote}Catching {{Throwable}} should probably handle most of the cases just fine. {quote} Yes, that's why we do is this way. [~fatihcelik] \u2013 yes. But inside `initialize()` as also pointed out by [~mikkolaj], we would catch `Throwable` here: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L434] to address it. So easy fix: Change [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java#L434] from `Exception` to `Throwable` and remove [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L475-L478] Additionally, we could consider to also mess with `Thread#setUncaughtExceptionHandler()` (note, that both `StreamsThread` and `GlobalThread`, and neither the new `StateUpdatedThread` use this method \u2013 the methods we discussed above, are overload from KS which work with `StreamsUncaughtExceptionHandler`, not the Java `Thread.UncaughtExceptionHandler`), but I am not sure if this would buy us much? The tricky part would be, that `StreamsThreads`, `StateUpdaterThread` and `GlobalStreamThread` are background threads and are not exposed to the user, and we also would not want to add new public API to avoid confusion (having `KafkaStreams#setUncaughtExceptionHandler(StreamsUncaughtExceptionHandler)` and `KafkaStreams#setUncaughtExceptionHandler(Thread.UncaughExceptionHandler)` would be very weird). \u2013 But we could either register our own KS specific handler, or use `Thread.currentThread().getUncaughExceptionHandler` to get whatever handler is set on the main thread, and add it to background threads. But as said already, not sure if this would help much \u2013 but I also don't think it would cause any issues? \u2013 Also not sure if this is necessary as using `Thread.setDefaultUncaughtExceptionHandler()` should just do the trick? - For this issue, I created the following PR: [https://github.com/apache/kafka/pull/20668 |https://github.com/apache/kafka/pull/20668] This is my first contribution to an open source project :) Could you please let me know if there are any things I need to fix? - ??I am not sure what guarantees there are that the handler is invoked?? [~mjsax] I haven't found any specific guarantees being mentioned in the docs, so I would assume it's executed on a best efforts basis. I agree that exposing a separate API for setting the Java exception handler would be confusing and a KS specific handler seems like a difficult thing to implement (I'm not sure what would be a proper behavior that would suit all the cases). From my perspective not setting such handler and relying on user supplying his own default handler with {{Thread.setDefaultUncaughtExceptionHandler()}} seems like a reasonable solution. [~fatihcelik]'s PR looks good to me. Thanks guys for your help and cooperation here :)", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19723", "url": "https://issues.apache.org/jira/browse/KAFKA-19723"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add tests for ConsumerRebalanceMetricsManager\nDescription: While working on KAFKA-19722 I noticed we had no test coverage for the rebalance metrics on the new consumer (this class [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/metrics/ConsumerRebalanceMetricsManager.java] ) I introduced the test file and added a test related to KAFKA-19722, but we should extend coverage and test the other rebalance metrics (ex. tests like the ones for the ShareRebalanceMetrics [https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/clients/consumer/internals/metrics/ShareRebalanceMetricsManagerTest.java] )\nComments: - Hey [~goyal.arpit.91] , this is a small one that should be a nice easy way to get to work in the new consumer area , if you are interested you can take it (I remembered you asked when working on KAFKA-19259). Thanks! https://issues.apache.org/jira/browse/KAFKA-19259?focusedCommentId=18018434&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18018434 - Thanks [~lianetm] . I will start working on it - [~lianetm] PR is for review https://github.com/apache/kafka/pull/20565/files", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19722", "url": "https://issues.apache.org/jira/browse/KAFKA-19722"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Missing assigned-partitions metric in new consumer\nDescription: The metric assigned-partitions is missing in the new async consumer. Registered by the coordinator in the old consumer here [https://github.com/apache/kafka/blob/dbd2b527d0fa5eda4ec603ee449a47c69ac41852/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L1640]\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19721", "url": "https://issues.apache.org/jira/browse/KAFKA-19721"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update streams documentation with KIP-1147 changes\nDescription: docs/streams/quickstart.html and docsk/streams/developer-guide/datatypes.html both need to be updated to change the flags for kafka-console-consumer.sh to rename --property to --formatter-property.\nComments: - Hi [~schofielaj], I've assigned this ticket to myself. If someone else is already handling it, feel free to reassign. - Hi [~schofielaj] , should I update the docs from `--property` to `--reader-property` for consoleProducer as mentioned in https://github.com/apache/kafka/pull/20554#issuecomment-3308459767 , or would you like to do it in another ticket? - You can do the docs change in this ticket, but I'll get the code change for replacing --property with --reader-property in another ticket.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19720", "url": "https://issues.apache.org/jira/browse/KAFKA-19720"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Regex subscription should be empty for classic members joining mixed group\nDescription: We don't recompute the assignment on consumer -> classic member replacement when the consumer member had a regex subscription and the classic member does not. We should explicitly set regex subscription to empty in classicGroupJoinToConsumerGroup\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19719", "url": "https://issues.apache.org/jira/browse/KAFKA-19719"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: setting --no-initial-controllers flag should not validate kraft version against metadata version\nDescription: Just because a controller node sets `--no-initial-controllers` flag does not mean it is necessarily running kraft.version=1. The more precise meaning is that the controller node does not know what kraft version the cluster should be in. It is a valid configuration to run a static quorum defined by `controller.quorum.voters` but have all the controllers format with `--no-initial-controllers`.\nComments: - trunk: https://github.com/apache/kafka/commit/857b1e92cc5c75eb178e48613e5963755bc1b03b - 4.1: https://github.com/apache/kafka/commit/012e4ca6d8fcd9a76dcb60480d9ba9cb7827816e - 4.0: https://github.com/apache/kafka/commit/099e91f5fc7e0a44ffec05d60cba650ceea4109a - 3.9: https://github.com/apache/kafka/commit/6423c2c202d1e7c3e4185d252d0a92b888b1c493", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19718", "url": "https://issues.apache.org/jira/browse/KAFKA-19718"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: docker_build_test.py does not run tests when using --kafka-archive.\nDescription: When we want to run docker_sanity_test.py for a local image, we need to pass a local archive image via --kafka-archive to docker_build_test.py. But currently it fails to run the test(docker_sanity_test.py) when we pass --kafka-archive. Currently the tests run only when we pass a URL via --kafka-url. {code:java} python docker_build_test.py kafka/test --image-tag=4.2.0-SNAPSHOT --image-type=jvm --kafka-archive=/Users/shivsundarr/dev/opensource/kafka/core/build/distributions/kafka_2.13-4.2.0-SNAPSHOT.tgz{code} After building the image, got the following output when it attempted to run the tests. {code:java} Traceback (most recent call last): File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 50, in run_docker_tests execute([\"wget\", \"-nv\", \"-O\", f\"{temp_dir_path}/kafka.tgz\", kafka_url]) File \"/Users/shivsundarr/dev/opensource/kafka/docker/common.py\", line 24, in execute if subprocess.run(command).returncode != 0: ^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 548, in run with Popen(*popenargs, **kwargs) as process: ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 1026, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 1885, in _execute_child self.pid = _fork_exec( ^^^^^^^^^^^ TypeError: expected str, bytes or os.PathLike object, not NoneType During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 85, in <module> run_docker_tests(args.image, args.tag, args.kafka_url, args.image_type) File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 55, in run_docker_tests raise SystemError(\"Failed to run the tests\") SystemError: Failed to run the tests {code} This is because the code currently only allows kafka_url to be passed for running the tests. {code:java} def run_docker_tests(image, tag, kafka_url, image_type):{code} We need to add another overloaded function which would take kafka_archive parameter instead of kafka_url.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19717", "url": "https://issues.apache.org/jira/browse/KAFKA-19717"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: InsertHeader drops part of the value when header value is a number\nDescription: When I use the SMT *org.apache.kafka.connect.transforms.InsertHeader* and the header value is a number, part of the value is dropped. Example: {code:yaml} # Insert KafkaHeaders for avro serialization transforms: insertSpecHeader transforms.insertSpecHeader.type: org.apache.kafka.connect.transforms.InsertHeader transforms.insertSpecHeader.header: \"specversion\" transforms.insertSpecHeader.value.literal: \"2.0\" {code} Then, the record is produced with the header *\"specversion\": \"2\"* Is KafkaConnect doing a sort of casting and treating the value as float even though I am using literal?\nComments: - Hi, I'd like to work on this issue as my first contribution to Kafka. Could you please assign it to me? Thank you!", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19716", "url": "https://issues.apache.org/jira/browse/KAFKA-19716"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: OOM when loading large uncompacted __consumer_offsets partitions with transactional workload\nDescription: When loading a large poorly compacted __consumer_offsets partition with a transactional workload (alternating transaction offset commits and commit markers), we create and discard lots of TimelineHashMaps. These accumulate in the SnapshotRegistry's rollback mechanism. Overall, memory usage is linear in the size of the partition. We can commit the snapshot every N offsets in {{{}CoordinatorLoaderImpl{}}}, to allow the memory to be reclaimed.\nComments: - Fix for 4.0.x and 4.1.x is tracked by https://issues.apache.org/jira/browse/KAFKA-19732.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19715", "url": "https://issues.apache.org/jira/browse/KAFKA-19715"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Consider bumping 3rd party Github Actions\nDescription: In https://github.com/apache/kafka/commit/e1b7699975e787a78f5e9bca540b4aa19f0d419d we bumped the versions of many Github Actions but we did not bump the following: - gradle/actions/setup-gradle - aquasecurity/trivy-action - docker/setup-qemu-action - docker/setup-buildx-action - docker/login-action\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19714", "url": "https://issues.apache.org/jira/browse/KAFKA-19714"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Auto migrate .checkpoint offsets\nDescription: When opening a {{StateStore}} that {{{}managesOffsets{}}}, existing {{.checkpoint}} offsets should be \"migrated\" to the {{StateStore}} using {{{}StateStore#commit{}}}.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19713", "url": "https://issues.apache.org/jira/browse/KAFKA-19713"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Auto migrate .position offsets\nDescription: When opening a {{{}RocksDbStateStore{}}}, {{.position}} offsets should be migrated into the offsets column family.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "KAFKA-19712", "url": "https://issues.apache.org/jira/browse/KAFKA-19712"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Implement new behaviour in ProcessorStateManager\nDescription: Update {{ProcessorStateManager}} to start passing offsets to {{StateStore#commit}} and loading offsets using {{{}StateStore#getCommittedOffsets{}}}.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4995", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4995"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Information for zookeeper 3.9.4 stable version plan release date\nDescription: Current stable version of Apache zookeeper 3.8.5 as per the documentation, However, we see that Apache zookeeper version 3.9.4 is available as a current version. Could anyone let us know if there is any tentative timeline planned for a stable version release of the 3.9.x version? [https://zookeeper.apache.org/releases.html]\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4994", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4994"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Authentication Credential Logging\nDescription: h2. Security Vulnerability Report: Authentication Credential Logging *Code Location :* {{org.apache.zookeeper.server.auth.KeyAuthenticationProvider}} (Lines 93-113) *Description:* The authentication provider logs sensitive authentication credentials (keys and auth data) in plain text at line 99: {quote} LOG.debug(\"KeyAuthenticationProvider handleAuthentication ({}, {}) -> FAIL.\\n\", keyStr, authStr); {quote} *Impact:* # Authentication keys are exposed in debug logs during failed authentication attempts # Potential credential leakage through log files, log aggregation systems, or monitoring tools # Compliance framework violations (PCI-DSS, GDPR, etc.) *Recommendation:* Remove or redact sensitive parameters from the log statement: LOG.debug(\"KeyAuthenticationProvider handleAuthentication -> FAIL (credentials redacted)\"); Alternatively, log only non-sensitive metadata (timestamp, source IP, attempt count) without actual credential values.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4993", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4993"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Lower \"Attempting to delete candidate container...\" logging to DEBUG\nDescription: [https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/ContainerManager.java#L134] Can we get the \"Attempting to delete candidate container...\" logging statement lowered from INFO to DEBUG? We're getting about 2.0+ million of these logging statements each day (no exaggeration, actually). If we raised our logging for this class to WARN, to avoid the above statement, we'd miss the LOG.info(\"Using checkIntervalMs={} maxPerMinute={} maxNeverUsedIntervalMs={}\" statement in the Constructor that we'd like to keep. Thanks!\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4992", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4992"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Loading multiple trusted certificates with identical subject names from a PEM bundle fails\nDescription: When a PEM bundle file is read as a trust store, each certificate is added to an in-memory {{KeyStore}} using the subject name as the alias: [https://github.com/apache/zookeeper/blob/e8e141b21f3a07797958c74053762048c7a3a0bf/zookeeper-server/src/main/java/org/apache/zookeeper/util/PemReader.java#L95-L98] for (X509Certificate certificate : certificateChain) \\{ X500Principal principal = certificate.getSubjectX500Principal(); keyStore.setCertificateEntry(principal.getName(\"RFC2253\"), certificate); } If two CA certificates in the bundle share the same subject name, the first entry is overridden by the second. This behavior causes loss of trusted certificates that have identical subjects but are otherwise different certificates. Using the subject name as a unique alias is therefore not suitable. Related to ZOOKEEPER-4990\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4991", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4991"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: For environments of the same scale, zookeeper version 3.9.3 has SSL request processing blocking issues compared to version 3.8.1, which can lead to coredump\nDescription: The Apache ZooKeeper third-party component version 3.9.3 extensively refactored the SSL request processing module. After upgrading to version 3.9.3, large-scale environments (characterized by 200-1000 virtual machine nodes and a large volume of business requests) will experience a large accumulation and blocking of business requests, and the Zookeeper server will not be able to respond to requests properly. I once attempted to increase the stack memory from 160M to 1024M, but failed to solve the problem; After replacing the Zookeeper package on the environment with the 3.8.1 package, the problem was resolved.\nComments: No comments.", "output": "Status: Open\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4990", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4990"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper doesn't support multiple ca into truststore\nDescription: We have a microservice zookeeper based, For zookeeper configuration, we have configured with below parameters ssl.keyStore.location=/var/lib/zookeeper/secrets/server/zk-server-keystore.jks ssl.quorum.keyStore.password=xxxxxxxxxxxxxxxxx ssl.quorum.trustStore.password=xxxxxxxxxxxxxxxxx ssl.quorum.keyStore.location=/var/lib/zookeeper/secrets/server/zk-server-keystore.jks ssl.quorum.trustStore.location=/var/lib/zookeeper/secrets/server/zk-server-truststore.jks ssl.trustStore.password=xxxxxxxxxxxxxxxxx ssl.keyStore.password=xxxxxxxxxxxxxxxxx Where multiple CA's is being imported into trustStore as alias in jks format, so when the client tries to connect with CA signed but it's not working as expected. ----- keytool -list -keystore /var/lib/zookeeper/secrets/server/zk-server-keystore.jks Enter keystore password: Keystore type: PKCS12 Keystore provider: SUN Your keystore contains 1 entry zookeeper, Nov 6, 2025, PrivateKeyEntry, Certificate fingerprint (SHA-256): 74:30:24:28:52:09:F5:07:6F:AD:39:97:43:5A:CF:A6:53:AF:44:1C:3B:34:11:5A:B1:86:AD:A4:2F:AC:06:EA ------- bash-4.4$ keytool -list -keystore /var/lib/zookeeper/secrets/server/zk-server-truststore.jks Enter keystore password: Keystore type: PKCS12 Keystore provider: SUN Your keystore contains 3 entries zkserverca_cert, Nov 6, 2025, trustedCertEntry, Certificate fingerprint (SHA-256): 88:82:EA:2C:AD:A5:A9:DB:13:2C:B6:12:89:7A:B5:52:AF:1D:58:96:83:00:C2:7F:95:C0:C6:A1:E6:4F:45:2C zkserverca_cert1, Nov 6, 2025, trustedCertEntry, Certificate fingerprint (SHA-256): 14:95:7E:DA:07:C0:C9:08:01:A3:3D:3C:AF:FD:F8:43:06:E2:CA:D8:DC:1A:20:50:C1:0A:B4:82:5E:45:77:9C zkserverca_cert2, Nov 6, 2025, trustedCertEntry, Certificate fingerprint (SHA-256): 9D:5C:95:F6:ED:5D:67:94:96:A5:91:E4:3D:CB:65:34:DB:32:1B:52:B7:A5:28:F0:B0:A2:87:B0:B3:7E:CD:0B ------ keytool -list -keystore /var/lib/zookeeper/secrets/server/zk-client-truststore.jks Enter keystore password: Keystore type: PKCS12 Keystore provider: SUN Your keystore contains 3 entries zkclientca_cert, Nov 6, 2025, trustedCertEntry, Certificate fingerprint (SHA-256): 11:F6:BB:D1:36:0C:C0:4E:15:C6:3A:A2:9A:DF:DA:50:06:4E:50:01:B2:54:24:57:8E:DE:1D:02:8B:38:28:8D zkclientca_cert1, Nov 6, 2025, trustedCertEntry, Certificate fingerprint (SHA-256): 02:D3:BF:49:6F:38:CC:F7:7D:A7:83:64:A2:EB:5B:4B:40:15:4E:08:8A:25:4E:AC:5E:15:6A:B0:ED:6E:FF:D7 zkclientca_cert2, Nov 6, 2025, trustedCertEntry, Certificate fingerprint (SHA-256): A2:B8:5E:FB:CB:B2:C3:59:0C:BA:E0:86:D1:DB:15:2C:5C:03:6E:22:CB:2D:33:03:3E:9E:BE:FE:0E:C1:7E:C4 --------- Does zookeeper support multiple CAs in the truststore? Are there any limitations I should be aware off? When trying to connect with zkcli.sh with getting the below error - Unexpected throwable io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:500) at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:796) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:840) Caused by: javax.net.ssl.SSLHandshakeException: PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:383) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:326) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:321) at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654) at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473) at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369) at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1277) at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1264) at java.base/java.security.AccessController.doPrivileged(AccessController.java:712) at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1209) at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1695) at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1541) at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1377) at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1428) at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530) at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469) ... 17 common frames omitted Caused by: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors at java.base/sun.security.validator.PKIXValidator.doValidate(PKIXValidator.java:369) at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:275) at java.base/sun.security.validator.Validator.validate(Validator.java:264) at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:285) at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144) at org.apache.zookeeper.common.ZKTrustManager.checkServerTrusted(ZKTrustManager.java:135) at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:632) ... 31 common frames omitted Caused by: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors at java.base/sun.security.provider.certpath.PKIXCertPathValidator.validate(PKIXCertPathValidator.java:157) at java.base/sun.security.provider.certpath.PKIXCertPathValidator.engineValidate(PKIXCertPathValidator.java:83) at java.base/java.security.cert.CertPathValidator.validate(CertPathValidator.java:309) at java.base/sun.security.validator.PKIXValidator.doValidate(PKIXValidator.java:364)\nComments: - Hi [~tsaarni], Could you please provide some support? that would be helpful. Thanks - Hi, How does the client configuration for zkCli.sh look like? It seemed to me like the exception is from client and it would rather indicate that the client was not configured with the correct CA certificates, or it was not configured correctly. I tested and multiple CAs work on both the server and client sides and they can be configured either as a truststore file or as a PEM bundle: * Server configured with multiple CAs accepted clients that presented client certificates issued by two separate root CAs * Client configured with multiple CAs accepted server that presented server certificates issued by two separate root CAs The configuration for client would look like this when using keystore and truststore {{export CLIENT_JVMFLAGS=\"}} {{-Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty}} {{-Dzookeeper.client.secure=true}} {{-Dzookeeper.ssl.keyStore.location=/path/client-cert-keystore.jks}} {{-Dzookeeper.ssl.keyStore.password=my-password}} {{-Dzookeeper.ssl.trustStore.location=/path/ca-truststore.jks}} {{-Dzookeeper.ssl.trustStore.password=my-password}} {{\"}} or in case of PEM bundles {{export CLIENT_JVMFLAGS=\"}} {{-Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty}} {{-Dzookeeper.client.secure=true}} {{-Dzookeeper.ssl.keyStore.location=/path/client-bundle.pem}} {{-Dzookeeper.ssl.trustStore.location=/path/ca-bundle.pem}} {{\"}} A PEM bundle is a file that contains multiple PEM entries concatenated together, separated by the standard PEM markers. For trusted CAs, concatenate multiple CA certificates: {{cat cacert1.pem cacert2.pem > ca-bundle.pem}} For server or client credentials, concatenate the certificate and private key e.g.: {{cat client.pem client-key.pem > client-bundle.pem}}", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4989", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4989"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Compilation of client on Windows with MSVC is broken\nDescription: Fix added for ZOOKEEPER-4810 (Fix buf data race at format_endpoint_info) in PR [https://github.com/apache/zookeeper/pull/2140] broke the build of the C client on Windows using MVSC. The error reported is: {noformat} error C2143: syntax error: missing ';' before 'type'{noformat} It is reported at zookeeper-client\\zookeeper-client-c\\src\\zookeeper.c(5134)\nComments: - I created a proposal of fix in PR [https://github.com/apache/zookeeper/pull/2335] - master: fb43500ea53fa5b57b6dc549c6582ae0ac60d7bc branch-3.9: 3412d237e5a0174a28f963747c359031248b7c55", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4988", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4988"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add [-R] recursive option to getAcl cli command\nDescription: Similarly what we've implemented for setAcl command in the CLI, we could add the same [-R] recursive option to getAcl too.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4987", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4987"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: zookeeper client fails to fallback to tls1.2 when tls1.3 ciphers are not correct / zookeeper client fails to fallback to tls1.3 when tls1.2 ciphers are not correct\nDescription: Hi we have 2 microservices, 1. zookeeper based 2. kafka based For zookeeper configuration , we have configured with below parameters related to tls - ssl.protocol=TLSv1.3 ssl.quorum.protocol=TLSv1.3 ssl.enabledProtocols=TLSv1.3,TLSv1.2 ssl.quorum.enabledProtocols=TLSv1.3,TLSv1.2 Also jvm opts for zookeeper we have below opts - -Djdk.tls.client.protocols=TLSv1.3,TLSv1.2 -Dhttps.protocols=TLSv1.3,TLSv1.2 From kafka side we are setting below 2 configuration parameters in properties file which is used to start kafka server - zookeeper.ssl.protocol: \"TLSv1.3\" zookeeper.ssl.enabled.protocols: \"TLSv1.3,TLSv1.2\" for kafka , we have below opts for jvm -Djdk.tls.client.protocols=TLSv1.3,TLSv1.2 -Dhttps.protocols=TLSv1.3,TLSv1.2 Zookeeper server version - 3.8.4 Kafka server version - 3.9.0 inside kafka java class load path we are adding zookeeper server binary, because its zookeeper client component used by kafka for communicating with zookeeper server. So that zookeeper client version is 3.9.2 Now it was observed that, kafka uses TLSv1.3 to communicate with zookeeper which is okay, because zookeeper support both TLSv1.2 and TLSv1.3. But if I dont set TLS1.3 related ciphers and only set TLS1.2 related ciphers to zookeeper, ideally kafka also fallback to TLS1.2 and keep using TLSv1.2 for ssl handshake. But that doesnt happen. As we have set only TLS1.2 related ciphers to zookeeper, zookeeper server falls back to TLS1.2 and expects that kafka should use TLS1.2 only but kafka still uses TLS1.3 and below error messages are printed in zookeeper logs - and kafka pods doesnt come up - {\"message\":\"Caused by: javax.net.ssl.SSLHandshakeException: The client supported protocol versions [TLSv1.3] are not accepted by server preferences [TLSv1.2]\",\"metadata\":\\{\"container_name\":\"zookeeper\",\"namespace\":\"namespace1\",\"pod_name\":\"zookeeper-0\"},\"service_id\":\"zookeeper\",\"severity\":\"info\",\"timestamp\":\"2025-10-09T08:50:15.443+00:00\",\"version\":\"1.2.0\"} {\"message\":\"\\tat java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)\",\"metadata\":\\{\"container_name\":\"zookeeper\",\"namespace\":\"namespace1\",\"pod_name\":\"zookeeper-0\"},\"service_id\":\"zookeeper\",\"severity\":\"info\",\"timestamp\":\"2025-10-09T08:50:15.443+00:00\",\"version\":\"1.2.0\"} {\"message\":\"\\tat java.base/sun.security.ssl.Alert.createSSLException(Alert.java:117)\",\"metadata\":\\{\"container_name\":\"zookeeper\",\"namespace\":\"namespace1\",\"pod_name\":\"zookeeper-0\"},\"service_id\":\"zookeeper\",\"severity\":\"info\",\"timestamp\":\"2025-10-09T08:50:15.443+00:00\",\"version\":\"1.2.0\"}\nComments: - Hi [~kezhuw] Could you please provide some support? that would be helpful Thanks - Hi [~tsaarni] Could you please provide some support? that would be helpful Thanks in advance - The following Kafka settings work for the Zookeeper client when Zookeeper server was configured with the default enabled protocols (TLSv1.3 and TLSv1.2) but server is restricted to only TLSv1.2 ciphers: zookeeper.ssl.client.enable=true zookeeper.ssl.protocol = TLSv1.3 zookeeper.ssl.enabled.protocols = TLSv1.2, TLSv1.3 zookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4986", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4986"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Disable reverse DNS lookup in TLS client and server\nDescription: Port the property behavior from [Apache HBase|https://github.com/apache/hbase/commit/5baeacb7d65f8ec3386690cadbf5e091e20b7b23#diff-b8e69e4d42340619ee4cd63d9e45d7224727f78c05da70ff9ce080c1d33e36d6R157] which controls wether reverse DNS lookup is allowed in TLS handshake if the hostname is not available (e.g. connect via IP address, client hostname verification, etc.) Disable reverse DNS lookups by default for both quorum and client protocols to be consistent. This should be safe from backward compatibility perspective in a new major (minor?) version if we cut 4.0.0 from master soon. In a {{branch-3.9}} backport we should enable reverse lookup in the quorum protocol by default to support smooth transition.\nComments: No comments.", "output": "Status: In Progress\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4985", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4985"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: zkCleanup.sh fails for non-standard config directories\nDescription: The zkCleanup.sh script has several issues that make it difficult to use in containerized or non-standard deployments: # The script hardcodes the path to zoo.cfg and fails with confusing error messages when the config file is not in the default location # Unlike other ZooKeeper scripts, it doesn't properly support the --config flag even though zkEnv.sh supports it # When the config file is missing, it passes empty strings to PurgeTxnLog, causing confusing error messages Steps to Reproduce: * Run ZooKeeper with config in a non-standard location (e.g., /custom/path/zoo.cfg) * Try to run cleanup: {code:java} grep: /path/to/zookeeper/bin/../conf/zoo.cfg: No such file or directory grep: /path/to/zookeeper/bin/../conf/zoo.cfg: No such file or directory Path '/path/to/zookeeper/bin' does not exist. Usage: PurgeTxnLog dataLogDir [snapDir] -n count {code} * Trying to use --config flag (which works for zkServer.sh) has no effect either The reason, I believe, is that if the config can't be found empty strings will be passed to `PurgeTxnLogs` and that fails. I'll raise a PR for this.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4984", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4984"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade OWASP plugin to 12.1.6 due to breaking changes in the API\nDescription: Authentication is now required for {*}Sonatype OSS Index{*}: [https://ossindex.sonatype.org/doc/auth-required] I don't think we need this feature, so I won't add API key to the public repo, but I have to update the plugin to skip indexing with softfail.\nComments: - Issue resolved by pull request 2323 [https://github.com/apache/zookeeper/pull/2323]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4983", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4983"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add client-triggered operation count metrics\nDescription: ZooKeeper currently doesn't have client triggered op count metrics. We would need a way to track how many operations of each type are performing for the following: - Monitor client application performance and behavior effectively - Analyze client traffic load and patterns - Identify and resolve performance issues by operation type - Set up alerting and SLA monitoring for client operations\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4982", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4982"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Automatic HostProvider selection based on connection string format\nDescription: Enhance ZooKeeper Java client to automatically detect connection string format and select appropriate HostProvider, enabling zero-code-change migration from static to DNS SRV-based service discovery. Following ZOOKEEPER-4956, applications must explicitly choose between StaticHostProvider and DnsSrvHostProvider. This ticket enables automatic provider selection based on connection string format, allowing seamless migration without code changes.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4981", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4981"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: One node has neither crashed nor restarted again, but its information is inconsistent with other nodes\nDescription: The Time line of the bug triggered: # A cluster with five nodes: zk1, zk2, zk3, zk4, zk5. And zk3 is the leader. # client1 creates a znode \"/bug\" ; # client1 creates a znode \"delete\"; # then, zk5 crashes(a follower, but it likely to be the server which connects to client1, because the fuzzlog file shows that before client1 sets znode \"/bug\" to nice , client1 seems to have been briefly disconnected from the cluster); # then, client1 reconnect to the cluster, and sets znode \"/bug\" 's value to nice; # client1 reads znode \"/bug\", get the value \"nice\"; # client1 deletes znode \"/delete\"; # client1 creates ephemeral znode /eph; # after that, zk4 crashes(important, it is the server which connect with client1); # then, client2 connects to the cluster, and it can read the \"/eph\" node which should be deleted after client1 leaves. ********************************************************************************* The acutal testing scenario is as following\uff1a The cluster has five nodes: C1ZK1(172.30.0.2), C1ZK2(172.30.0.3), C1ZK3(172.30.0.4), C1ZK4(172.30.0.5), C1ZK5(172.30.0.6) # 2025-09-05 19:19:22,788 [ZK1Cli] - INFO - build connection with zookeeper (Client1 builds connection with C1ZK4 [observered by the log]) # 2025-09-05 19:19:23,025 [ZK1Cli] - INFO - created znode /bug hello # 2025-09-05 19:19:23,036 [ZK1Cli] - INFO - created znode /delete hello # 2025-09-05 19:19:22,887 prepare to crash node C1ZK5 before the IO operation did by C1ZK1 (C1ZK1 is going to create file: \"/home/zk1/evaluation/zk-3.6.3/zkData/version-2/log.100000001\") {code:java} java.io.FileOutputStream.<init>(FileOutputStream.java:213), java.io.FileOutputStream.<init>(FileOutputStream.java:162), org.apache.zookeeper.server.persistence.FileTxnLog.append$$PHOSPHORTAGGED(FileTxnLog.java:287), org.apache.zookeeper.server.persistence.FileTxnSnapLog.append$$PHOSPHORTAGGED(FileTxnSnapLog.java:582), org.apache.zookeeper.server.ZKDatabase.append$$PHOSPHORTAGGED(ZKDatabase.java:641), org.apache.zookeeper.server.SyncRequestProcessor.run$$PHOSPHORTAGGED(SyncRequestProcessor.java:181), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:0) {code} # 2025-09-05 19:19:23,724 [ZK1Cli] - INFO - set znode /bug nice # 2025-09-05 19:19:23,738 [ZK1Cli] - INFO - read znode /bug:nice # 2025-09-05 19:19:23,758 [ZK1Cli] - INFO - deleted znode /delete # 2025-09-05 19:19:23,770 [ZK1Cli] - INFO - created ephemeral znode /eph ephem # Client1 successfully create \"/eph\". # 2025-09-05 19:19:26,278 prepare to crash node C1ZK4 {code:java} org.apache.zookeeper.server.quorum.QuorumPacket.serialize$$PHOSPHORTAGGED(QuorumPacket.java:68), org.apache.jute.BinaryOutputArchive.writeRecord$$PHOSPHORTAGGED(BinaryOutputArchive.java:126), org.apache.zookeeper.server.quorum.Learner.writePacketNow$$PHOSPHORTAGGED(Learner.java:194), org.apache.zookeeper.server.quorum.Learner.writePacket$$PHOSPHORTAGGED(Learner.java:186), org.apache.zookeeper.server.quorum.Learner.request$$PHOSPHORTAGGED(Learner.java:254), org.apache.zookeeper.server.quorum.FollowerRequestProcessor.run$$PHOSPHORTAGGED(FollowerRequestProcessor.java:104), org.apache.zookeeper.server.quorum.FollowerRequestProcessor.run(FollowerRequestProcessor.java) {code} # 2025-09-05 19:19:27,124 ZK1Cli2 connect to the cluster and then incorrectly got ephemeral znode /eph created by cli1. # 2025-09-05 19:19:39,760 [ZKChecker] - INFO - server C1ZK3:11181 and server C1ZK1:11181 have different number of znodes:[/zookeeper/quota, /bug, /zookeeper] [/zookeeper/quota, /bug, /eph, /zookeeper] ************************************************************************** The following information is the logs of each server. The transaction 0x100000001 \"9/5/25 7:19:22 PM CST session 0x5005888b8400000 cxid 0x0 zxid 0x100000001 createSession 15000\" indicates that they create a connection with the client.(C1ZK3 is the leader, so there is no such log.) From the following information, C1ZK5 is the earliest server connecting to the client1. C1ZK4: 2025-09-05 19:19:22,932 [myid:4] - WARN [QuorumPeer[myid=4](plain=[0:0:0:0:0:0:0:0]:11181)(secure=disabled):Follower@170] - Got zxid 0x100000001 expected 0x1 C1ZK1:2025-09-05 19:19:22,889 [myid:1] - WARN [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:11181)(secure=disabled):Follower@170] - Got zxid 0x100000001 expected 0x1 C1ZK2:2025-09-05 19:19:22,889 [myid:2] - WARN [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:11181)(secure=disabled):Follower@170] - Got zxid 0x100000001 expected 0x1 C1ZK5:2025-09-05 19:19:22,884 [myid:5] - WARN [QuorumPeer[myid=5](plain=[0:0:0:0:0:0:0:0]:11181)(secure=disabled):Follower@170] - Got zxid 0x100000001 expected 0x1 So it is really confused that why only the C1ZK1's node list is different from other servers. C1ZK1 is not the node which is crashed and not the node which is rebooted before Client2 connects to cluster.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4980", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4980"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade netty to fix CVE-2025-58057 , CVE-2025-58056\nDescription: CVE ID: CVE-2025-58057 , CVE-2025-58056 Affected ZooKeeper Version: 3.9.4 Vulnerable Dependency: Netty 4.1.119 Impact: Netty is an asynchronous event-driven network application framework for development of maintainable high performance protocol servers and clients. In versions 4.1.124.Final, and 4.2.0.Alpha3 through 4.2.4.Final, Netty incorrectly accepts standalone newline characters (LF) as a chunk-size line terminator, regardless of a preceding carriage return (CR), instead of requiring CRLF per HTTP/1.1 standards. When combined with reverse proxies that parse LF differently (treating it as part of the chunk extension), attackers can craft requests that the proxy sees as one request but Netty processes as two, enabling request smuggling attacks. Fix : This is fixed in versions 4.1.125.Final and 4.2.5.Final.\nComments: - [~dpramod] updated by me in https://issues.apache.org/jira/browse/ZOOKEEPER-4976", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4979", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4979"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Provide Metric for Maximum Ephemeral Node Path Length per Session\nDescription: h4. Background Currently, ZooKeeper handles the deletion of ephemeral nodes on closeSession by processing a session-based delete transaction, rather than creating a separate delete transaction for each znode. If a session with a large number of ephemeral nodes is closed, the resulting transaction can exceed the jute.maxbuffer limit, causing errors. h4. Problem * When closing sessions with many ephemeral nodes, the delete transaction size may exceed {{{}jute.maxbuffer{}}}, resulting in \"unreasonable length\" errors and potential ZooKeeper failures. * ZooKeeper currently provides metrics for the number and size of ephemeral nodes, but does not provide metrics for the total path length of ephemeral nodes created by each session. * As a result, operators have difficulty estimating an appropriate value for {{jute.maxbuffer}} and cannot proactively monitor for related issues. h4. Improvement Suggestion * Provide a metric for the total path length of ephemeral nodes created by each session. * It is not necessary to provide this information for all sessions; reporting only the session with the largest total path length would be sufficient. * This metric would help operators determine a reasonable value for jute.maxbuffer and prevent related failures. h4. Reference Similar issues: https://issues.apache.org/jira/browse/ZOOKEEPER-4916, https://issues.apache.org/jira/browse/ZOOKEEPER-4306\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4978", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4978"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: C Client Compilation failure with MSVC/VS2022\nDescription: compilation works on Linux, but fails on Windows with VS2022. This is observed both locally as well as in the Conan Center Index CI C:\\workspace\\cci_prod_PR-28411\\conan-home\\p\\b\\zookec1c78f9eb76fe\\b\\src\\zookeeper-client\\zookeeper-client-c\\src\\zookeeper.c(5116): error C2143: *syntax error: missing ';' before 'type'* [https://c3i.jfrog.io/artifactory/cci-build-logs/cci/prod/PR-28411/1/package_build_logs/build_log_zookeeper-client-c_3_9_4_d572bd5e0bbc71191a65c3795736ee02_f857efc37fc7be829d6723a9b14488e5409e9dab.txt]\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4977", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4977"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: superDigest configuration found in embedded pom.xml within zookeeper-3.9.3.jar\nDescription: {{superDigest}} configuration found in embedded {{pom.xml}} within zookeeper-3.9.3.jar <zookeeper.DigestAuthenticationProvider.superDigest>super:D/InIHSb7yEEbrWz8b9l71RjZJU=</zookeeper.DigestAuthenticationProvider.superDigest> *Environment:* * ZooKeeper Version: 3.9.3 (the official binary distribution from Maven Central) * JDK Version: N/A (discovered during static analysis of the JAR file) * OS: N/A *Problem Description:* During a routine security audit of our application dependencies, we discovered that the {{zookeeper-3.9.3.jar}} file contains its own {{pom.xml}} file at the path {{{}META-INF/maven/org.apache.zookeeper/zookeeper/pom.xml{}}}. This embedded {{pom.xml}} file includes a property configuration for {{zookeeper.DigestAuthenticationProvider.superDigest}} with a pre-defined hash value. *Steps to Reproduce:* # Download the official {{org.apache.zookeeper:zookeeper:3.9.3}} JAR from Maven Central. # Extract the JAR file or use a tool ({{{}jar -tf{}}}, {{{}unzip -l{}}}, IDE) to list its contents. # Locate the file {{META-INF/maven/org.apache.zookeeper/zookeeper/pom.xml}} inside the JAR. # Inspect the content of this {{pom.xml}} file. On line 283 (or nearby), you will find: {{<zookeeper.DigestAuthenticationProvider.superDigest>super:D/InIHSb7yEEbrWz8b9l71RjZJU=</zookeeper.DigestAuthenticationProvider.superDigest>}} *Expected Behavior:* The published binary JAR artifacts should not contain any residual or testing configuration files that include sensitive properties, especially those related to security authentication like the superuser digest. The build/packaging process should strip such elements from the final release artifact. *Actual Behavior:* The released {{zookeeper-3.9.3.jar}} contains an embedded {{pom.xml}} which includes a configured {{superDigest}} property. While this is a hash and not a plaintext password, its presence in a widely distributed binary is a potential security risk. *Potential Risk:* # *Information Disclosure:* This exposes a known credential hash, which violates the principle of least surprise and could be used in conjunction with other vulnerabilities (e.g., CVE-2014-085 - information disclosure in logs). # *Increased Attack Surface:* If an attacker gains access to the JAR file (e.g., through a deployment leak), they extract this hash. Although SHA-1 hashing is used, it could potentially be targeted for brute-force attacks if the original password was weak, potentially granting superuser access to a ZooKeeper ensemble. # *Bad Practice:* The presence of this configuration, even if not activated by default, sets a poor security precedent for users who might find it and mistakenly use it without generating a new secure digest. !image-2025-09-15-16-00-33-152.png!\nComments: - How exactly is this a problem? The embedded pom.xml file is just a copy of the pom.xml file that maven used to build the jar. It is for reference only. It's not a runtime configuration file in any way and does not represent any runtime security configuration. That value is a test value used only during the maven build for testing using ZooKeeper's unit and integration tests. It should have no impact on any user deployments. This does not appear to be a problem to fix. - This is not a problem. The configuration is a test value for unit testing with maven-surefire-plugin, and has no impact on user configuration or deployments. This does not represent bad practice, but is the normal, acceptable, best practice way of setting test values for testing.", "output": "Status: Resolved\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4976", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4976"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update Netty to fix CVE-2025-58057\nDescription: Upgrade Netty version to 4.1.127.Final in order to fix CVE-2025-58057\nComments: - Issue resolved by pull request 2313 [https://github.com/apache/zookeeper/pull/2313]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4975", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4975"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: memory leak when the c client opens SASL\nDescription: zoo_sasl_make_basic_callbacks() malloc some memory, but never free it. Even if I want to free it from the outside, I can't get the struct zsasks_screen_ctx defined in the zk_sasl.c. At present, I can only create my own callbacks function to avoid this problem. zoo_sasl_make_basic_callbacks should be deleted or repaired.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4974", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4974"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove enforced JDK 17 compilation warnings\nDescription: To ensure full compatibility with JDK17 we should remove any usages of SecurityManager Because it is deprecated and causing the build to fail with extra maven parameters -Dmaven.compiler.source=17 -Dmaven.compiler.target=17 -Dmaven.compiler.release=17 the build fails because of these warning {code:java} [WARNING] COMPILATION WARNING : [INFO] ------------------------------------------------------------- [WARNING] /Users/andor/git/cdh/zookeeper/zookeeper-server/src/main/java/org/apache/zookeeper/server/WorkerService.java:[180,13] java.lang.SecurityManager in java.lang has been deprecated and marked for removal [WARNING] /Users/andor/git/cdh/zookeeper/zookeeper-server/src/main/java/org/apache/zookeeper/server/WorkerService.java:[180,39] getSecurityManager() in java.lang.System has been deprecated and marked for removal{code}\nComments: - Issue resolved by pull request 2312 [https://github.com/apache/zookeeper/pull/2312]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4973", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4973"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: fips-mode parameter in the configuration doesn't work as expected\nDescription: When using Zookeeper 3.9.4, I was trying to disable FIPS by setting the \"fips-mode\" parameter to false in the configuration as described in [the documentation|https://zookeeper.apache.org/doc/r3.9.4/zookeeperAdmin.html]. {code} $ cat /path/to/zoo.cfg ... fips-mode=false ... {code} However, that doesn't disable FIPS and I get errors when trying to configure authentication {code} zookeeper-1 | 2025-09-03 12:00:10,219 [myid:localhost:2181] - INFO [main-SendThread(localhost:2181):o.a.z.Login@332] - Client successfully logged in. zookeeper-1 | 2025-09-03 12:00:10,220 [myid:localhost:2181] - WARN [main-SendThread(localhost:2181):o.a.z.u.SecurityUtils@75] - Client will not use DIGEST-MD5 as SASL mechanism, because FIPS mode is enabled. zookeeper-1 | 2025-09-03 12:00:10,220 [myid:localhost:2181] - INFO [main-SendThread(localhost:2181):o.a.z.ClientCnxn$SendThread@1170] - Opening socket connection to server localhost/127.0.0.1:2181. {code} The issue is solved when using the flag \"-Dzookeeper.fips-mode=false\" (as mentioned in the documentation too).\nComments: - This is the client side. *zoo.cfg* is not being used on the client side, you can configure the client via system properties only.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4972", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4972"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky tests in PrometheusMetricsProviderConfigTest\nDescription: {noformat} org.apache.zookeeper.metrics.MetricsProviderLifeCycleException: java.io.IOException: Failed to bind to /0.0.0.0:50512 at org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider.start(PrometheusMetricsProvider.java:213) at org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProviderConfigTest.testValidHttpsAndHttpConfig(PrometheusMetricsProviderConfigTest.java:88) {noformat}\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4971", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4971"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Provide canonical way to parse quorum members from `getConfig`\nDescription: We have {{ZooKeeper::getConfig}} and {{ZooKeeperAdmin::reconfigure}}, but we don't have client side way to parse quorum members from {{getConfig}}. We do have {{QuorumPeerConfig::parseDynamicConfig}} but {{QuorumPeerConfig}} is tied with server side code.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4970", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4970"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Deprecate methods of ZKConfig which throw QuorumPeerConfig.ConfigException\nDescription: In pursue of ZOOKEEPER-233, I found some methods of {{ZKConfig}} throw {{ConfigException}} from {{QuorumPeerConfig}} which tied strictly with server side code. This is the only breaking change I found during draft https://github.com/apache/zookeeper/pull/2307. So, I suggest we can deprecate theses methods first, both in master and old releases.\nComments: - Issue resolved by pull request 2309 [https://github.com/apache/zookeeper/pull/2309] - Backported to branch-3.9 in commit f534e7135269d27f78380291b6582f674021239a to deprecate usages of {{QuorumPeerConfig.ConfigException}} from {{ZKClientConfig}}.", "output": "Status: Resolved\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4969", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4969"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Cleanup zookeeper-contrib module\nDescription: 1. There are still old for ivy, ant. We are using maven now. 2. Some of them are exectuable, but does not depend on concret log framework. 3. Some of shell scripts are designed for development usages, which is hard to follow in maven age. 4. Better to bundle exectuable module to executable jar. 5. Probably more.\nComments: - I think in the modern era, where projects can have small, self-contained git repos, there is no longer a need to follow the conventions of the past, where a \"contrib/\" directory contained contributed related optional projects. Now, users can acquire those optional projects separately. I suggest moving those projects to separate git repos (one per contrib project), and periodically building them separately from the main project, and voting on releases independently of the main ZK project. Decoupling these from the main ZK build might make it easier to simplify and update their build mechanism.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4968", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4968"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce interface to cover ZooKeeper client operations\nDescription: I think it is feasible even in 3.x with helper from {{ZooKeeperBuilder}}. But that prabably be abi incompatible.\nComments: - For my application, I created a wrapper ZooSession object that wraps ZooKeeper and handles replacing the ZooKeeper object internally when it experiences a session expiration, since the ZooKeeper client object is typically useless after a session expiration. However, I only implemented a subset of ZooKeeper's methods, to proxy, because I only used a few. It would be great to have an interface for ZooKeeper that defined the client API. Then I could have my own wrapper object implement that interface, so my automatic-reconnecting client could be used anywhere the usual ZooKeeper client object is used, as a drop-in replacement.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4967", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4967"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Builder to construct ZKConfig\nDescription: * We can make the population from system properties optional. * {{build}} and {{buildClient}}, though I see no much reason to have separated {{ZKClientConfig}}, but it is the way now. * Deprecate old constructors if possible, say {{ZKClientConfig(File configFile)}}.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4966", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4966"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: QuorumPeerConfig.ConfigException leaked to client side code through constructor of ZKConfig\nDescription: {{QuorumPeerConfig}} is tied with server side code. Client side code should not touch {{QuorumPeerConfig}} in any form.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4965", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4965"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Drop unnecessary {{@SuppressWarnings(\"deprecation\")}}\nDescription: \nComments: - Issue resolved by pull request 2304 [https://github.com/apache/zookeeper/pull/2304]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4964", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4964"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Check permissions individually during admin server auth\nDescription: \nComments: No comments.", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4963", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4963"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce ZooKeeper::builder for ZooKeeperBuilder\nDescription: \nComments: - Issue resolved by pull request 2301 [https://github.com/apache/zookeeper/pull/2301]", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4962", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4962"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add getPort and getSecurePort for ZooKeeperServerEmbedded\nDescription: Since ZOOKEEPER-4303, {{ZooKeeperServerEmbedded}} supports bind to port 0 to get unused port. It would be good to export {{getPort}} and {{getSecurePort}} for both inspection and custom connection string construction.\nComments: - Issue resolved by pull request 2302 [https://github.com/apache/zookeeper/pull/2302]", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4961", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4961"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Client side hostname verification failed due to reverse dns lookup in sasl client\nDescription: Sasl client and {{ClientCnxnSocket}} use same {{{color}{color:#000000}InetSocketAddress{color}}} object. If sasl is enabled(which is the default), it will do {{getHostName}} which will change the result of {{getHostString}} which is used by {{ClientCnxnSocket}}. Normally this doesn't matter. But in fips-mode (which is enabled by default in 3.9), this could fail client side hostname verification. This could happen in following situation: 1. server cert is signed with san ip address \"127.0.0.1\" 2. connection string is \"127.0.0.1\" 3. zookeeper.fips-mode is enabled in client 4. zookeeper.sasl.client is enabled 5. zookeeper.ssl.hostnameVerification is enabled Then the ssl connection could rejected by client as sasl client reverse dns lookup will turn {{getHostString}} to \"localhost\" which does not match server cert.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4960", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4960"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade OWASP plugin to 12.1.3 due to recent parsing errors\nDescription: Looks like our Owasp version 8.3.1 is outdated, because recently started to throw the following errors: {noformat} 12:06:36 [ERROR] org.owasp.dependencycheck.data.nvdcve.DatabaseException: Unable to parse CPE: cpe:2.3:a:f5:nginx unit:*:*:*:*:*:*:*:* 12:06:36 org.owasp.dependencycheck.data.update.exception.UpdateException: org.owasp.dependencycheck.data.nvdcve.DatabaseException: Unable to parse CPE: cpe:2.3:a:f5:nginx unit:*:*:*:*:*:*:*:* 12:06:36 at org.owasp.dependencycheck.data.update.nvd.ProcessTask.processFiles (ProcessTask.java:157) 12:06:36 at org.owasp.dependencycheck.data.update.nvd.ProcessTask.call (ProcessTask.java:114) 12:06:36 at org.owasp.dependencycheck.data.update.nvd.ProcessTask.call (ProcessTask.java:41) 12:06:36 at java.util.concurrent.FutureTask.run (FutureTask.java:266) 12:06:36 at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149) 12:06:36 at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624) 12:06:36 at java.lang.Thread.run (Thread.java:750){noformat} -I'll try to upgrade to a more recent version which still has Java 8 support.- We have to upgrade to 12.1.3, because the fix hasn't been backported to Java 8 versions.\nComments: No comments.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4959", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4959"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix license files after logback/slf4j upgrade\nDescription: {{Andor, I notice a number of license files are inaccurate:}} {code:java} rw-rr-@ 1 phunt staff 11359 Aug 8 12:21 commons-io-2.11.0.LICENSE.txt rw-rr-@ 1 phunt staff 515978 Aug 8 12:21 commons-io-2.17.0.jar rw-rr-@ 1 phunt staff 36274 Aug 8 12:21 logback-classic-1.2.13.LICENSE.txt rw-rr-@ 1 phunt staff 274470 Aug 8 12:21 logback-classic-1.3.15.jar rw-rr-@ 1 phunt staff 36274 Aug 8 12:21 logback-core-1.2.13.LICENSE.txt rw-rr-@ 1 phunt staff 571734 Aug 8 12:21 logback-core-1.3.15.jar rw-rr-@ 1 phunt staff 1133 Aug 8 12:21 slf4j-1.7.30.LICENSE.txt rw-rr-@ 1 phunt staff 68605 Aug 8 12:21 slf4j-api-2.0.13.jar{code} {{Might be more than this (if new deps added?) but these are the obvious ones}} {{I noticed. I think they need to be addressed/new RC.}}\nComments: - Issue resolved by pull request 2296 [https://github.com/apache/zookeeper/pull/2296]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4958", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4958"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: \"ssl.clientHostnameVerification\" is ignored if \"ssl.authProvider\" is configured to \"x509\"\nDescription: {{NettyServerCnxnFactory}} uses {{TrustManager}} from {{X509AuthenticationProvider}} if {{ssl.authProvider}} is configured. But the {{clientHostnameVerificationEnabled}} is explicitly set to {{false}} in construction. I confirmed this locally with test. Server configs: * zookeeper.ssl.hostnameVerification: true * zookeeper.ssl.clientHostnameVerification: true * zookeeper.fips-mode: false * zookeeper.ssl.authProvider: x509 Related codes: * https://github.com/apache/zookeeper/blob/770804bef861bbfc9e150b63774f8557f1f8d995/zookeeper-server/src/main/java/org/apache/zookeeper/server/NettyServerCnxnFactory.java#L572 * https://github.com/apache/zookeeper/blob/770804bef861bbfc9e150b63774f8557f1f8d995/zookeeper-server/src/main/java/org/apache/zookeeper/server/auth/X509AuthenticationProvider.java#L123\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4957", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4957"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Replace logback with slf4j-simple on branch-3.8 due to upgrading issues\nDescription: Logback versions 1.2.x have some CVEs which can only be resolved by upgrading to at least logback 1.3.x. Unfortunately it also involves upgrading SLF4j to 2.0.x which is not acceptable on branch-3.8. Instead we should try to replace logback with slf4j-simple, but that implies some test code changes as well.\nComments: - We ended up upgrading logging libraries on *branch-3.8* too, so we won't do this for now.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4956", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4956"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Provide a HostProvider that uses DNS SRV record for dynamic server discovery\nDescription: Currently, ZooKeeper clients use static configuration of server endpoints through connection strings. This creates operational challenges in dynamic environments like cloud deployments, container orchestration platforms, and auto-scaling scenarios. For example, server topology change requires updating all client configurations and potentially restarting al client applications. This ticket provides a `DnsSrvHostProvider` that leverages DNS SRV records for dynamic service discovery, allowing clients to automatically discover ZooKeeper servers without hardcoded configuration.\nComments: - This ticket is for Java implementation.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4955", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4955"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix intererence with jvm ssl properties for ssl.crl and ssl.ocsp\nDescription: EDIT: The original proposal was rejected, and a different solution is implemented which mimics the JVM internal logic. Zookeeper currenlty automatically calls PKIXBuilderParameters#setRevocationEnabled() based on the values of the *ssl.(quorum.)ocsp* and ssl(.quorum).crl config options. This means that if we don't set the above options, then ZK will explicitly disable revocation checks. As those options are also setting global System/Security properties, we do not have a way to enable revocation checks without clobbering the revocation related global properties. Adding a new property will let ZK enable/disable revocation checks without clobbering the JVM global properties.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4954", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4954"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Use FIPS style hostname verification when no custom truststore is specified\nDescription: Zookeeper has two ways to enable hostname verification: The traditional one is set in ZKTrustManager when a custome truststore is used. The FIPS style one is used when when FIPs mode is set. However, there is currently no way to specify hostname verification when no custom truststore is used. The FIPS style hostname verification does not depend on having a truststore defined, and can be used as a fallback when no custom trustore and no FIPS mode is configured.\nComments: - Issue resolved by pull request 2283 [https://github.com/apache/zookeeper/pull/2283]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4953", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4953"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fixing Typo In ZooKeeper Programmer's Guide\nDescription: There are typo errors in Programmers Guide in the documentation of zookeeper 3.9.3. Need to fix it.\nComments: - https://github.com/apache/zookeeper/pull/2281 - Issue resolved by pull request 2281 [https://github.com/apache/zookeeper/pull/2281]", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4952", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4952"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Reduce the GC overhead of Prometheus reject exception handling\nDescription: As part of ZOOKEEPER-4289, Prometheus summary reporting is handled as an async operation using ThreadPoolExecutor. The default RejectedExecutionHandler used by ThreadPoolExecutor is AbortPolicy, which performs toString() on both the Runnable object and exception object. This can significantly impact performance when too many exceptions occur after the max queue size exceeds. {code:java} public static class AbortPolicy implements RejectedExecutionHandler { /** * Creates an {@code AbortPolicy}. */ public AbortPolicy() { } /** * Always throws RejectedExecutionException. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task * @throws RejectedExecutionException always */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); } } {code} We can reduce the GC overhead and improve performance by implementing a PrometheusRejectExceptionHandler and overriding the rejectedExecution() API.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4951", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4951"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Optimize the default PrometheusMetricsProvider max queue size\nDescription: As part of ZOOKEEPER-4289, Prometheus summary reporting is handled as an async operation using a queue. The default max queue size is 1M. We have done perf tests with different queue sizes and found the optimal size is 10k.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4950", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4950"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Do not set jdk.tls.rejectClientInitiatedRenegotiation in ZK client\nDescription: The ZK client should not set random JRE system properties, as that can have unforseen effects in up in the stack / application. This is not a major issue, as this property seems to be only relevant fro TLS 1.0 and lower, but this is part of making the ZK client a well behaved Java library that does not randomly override potentially problematic JVM system properties. It is fine to keep this for the ZK server process.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4949", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4949"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Clean up TLS CRL/OCSP configuration\nDescription: This is an umbrella ticket for the CRL / OCSP issues identified in ZK.\nComments: - This is done.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4948", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4948"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: When ZooKeeper client enters AuthFailed state (e.g., due to SASL authentication failure), calling `close()` method does not terminate internal SendThread and EventThread.\nDescription: Problem When ZooKeeper client enters AuthFailed state (e.g., due to SASL authentication failure), calling `close()` method does not terminate internal SendThread and EventThread. Reproduction Steps 1. Configure SASL authentication with invalid credentials 2. Connect to ZooKeeper server 3. Watch receives `KeeperState.AuthFailed` event 4. Call `zookeeper.close()` 5. Observe SendThread/EventThread still running via thread dump Expected Behavior All client resources including background threads should be released. Proposed Fix Remove state check in close method: public synchronized void close() throws InterruptedException { if (cnxn != null) { cnxn.close(); } }\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4947", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4947"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: When ZooKeeper client enters AuthFailed state (e.g., due to SASL authentication failure), calling `close()` method does not terminate internal SendThread and EventThread.\nDescription: Problem When ZooKeeper client enters AuthFailed state (e.g., due to SASL authentication failure), calling `close()` method does not terminate internal SendThread and EventThread. Reproduction Steps 1. Configure SASL authentication with invalid credentials 2. Connect to ZooKeeper server 3. Watch receives `KeeperState.AuthFailed` event 4. Call `zookeeper.close()` 5. Observe SendThread/EventThread still running via thread dump Expected Behavior All client resources including background threads should be released. Proposed Fix Remove state check in close method: public synchronized void close() throws InterruptedException { if (cnxn != null) { cnxn.close(); } }\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4946", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4946"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Login thread failed to shutdown successfully, causing SendThead to be blocked\nDescription: Call org.apache.zookeeper.client.ZooKeeperSaslClient#shutdown method in sendThread to close the zooKeeperSaslClient, that is, shutdown Login thread. Although the t.interrupt method was called, the run method of the Login thread did not detect the thread being interrupted. For example, when the Login thread enters the while loop of reLogin, there may be a situation where the Login thread cannot interrupt. This will cause the t.join method to remain blocked, resulting in the sendThread thread being blocked and unabled to complete execution. This may result in many zk requests being unable to be released, such as possible deadlocks, due to sendThread being blocked in lower versions of zk, such as 3.5 and 3.6. Higher versions of zk, such as 3.8 and 3.9, have leaked sendThread threads.\nComments: - Call org.apache.zookeeper.client.ZooKeeperSaslClient#shutdown method, which is the shutdown Login thread. {code:java} public void shutdown() { if ((t != null) && (t.isAlive())) { t.interrupt(); try { t.join(); } catch (InterruptedException e) { LOG.warn(\"error while waiting for Login thread to shutdown: \" + e); } } } {code} After calling t.interrupt, if the Login thread is not executing the Thread.sleep method, other code blocks cannot perceive that the thread has been interrupted and still ger stuck in a dead loop. This will cause the t.join method to consistently block the sendThread. For example, when there is a failure in kerberos, the reLogin method call throws an exception. After retry has been set to 0, the reLogin method will be repeatedly called until successful. If the t.interrupt method is called at this time, the Login thread cannot be successfully interrupted. t.join method will block the sendThread method. {code:java} try { int retry = 1; while (retry >= 0) { try { reLogin(); break; } catch (LoginException le) { if (retry > 0) { --retry; // sleep for 10 seconds. try { sleepBeforeRetryFailedRefresh(); } catch (InterruptedException e) { LOG.error(\"Interrupted during login retry after LoginException:\", le); throw le; } } else { LOG.error(\"Could not refresh TGT for principal: {}.\", principal, le); } } } } catch (LoginException le) { LOG.error(\"Failed to refresh TGT: refresh thread exiting now.\", le); break; }{code}", "output": "Status: Patch Available\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4945", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4945"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support automatic renewal of ephemeral nodes via client heartbeats\nDescription: *Motivation* Currently, ZooKeeper clients must maintain an active session to retain ephemeral nodes. If a session timeout occurs due to a network glitch or GC pause, these nodes are deleted even if the client recovers shortly after. *Proposed Feature* Introduce a configurable *{*}auto-renewal heartbeat mechanism{*}* where the ZooKeeper client can *{*}extend the lifetime of ephemeral nodes{*}* for a grace period after temporary session disconnections \u2014 essentially a soft-reconnect buffer. *This feature would:* - Reduce unintended ephemeral node deletion due to transient network failures. - Improve stability for clients with flaky connections. - Help cloud-native workloads where short-lived network interruptions are common. *Implementation Ideas* - Introduce a `znode.ephemeral.gracePeriod` config on the server/client. - Allow clients to reattach to their ephemeral nodes within this window. - Maintain consistency and fencing semantics using a version hash or ephemeral token. *Benefits* This change would improve ZooKeeper's resilience in distributed environments without breaking the ephemeral node contract, as the node would still expire if the client doesn't reconnect within the grace period. *Impact* - Fully backward-compatible - Opt-in via configuration - May require slight changes to session expiration logic Let me know if this is a direction you'd consider. Happy to discuss design or help contribute a patch.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4944", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4944"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Cache zookeeper dists for end to end compatibility tests\nDescription: Our github action's end to end test fails quite often in downloading zookeeper dists from archive.apache.org. I think we could use github action's cache to eliminate this as much as possible.\nComments: No comments.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4943", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4943"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Use Duration instead of int timeout for ZooKeeperBuilder\nDescription: Some of zookeeper codes predate even java 8. It would be nice to use {{Duration}} for newly introduced {{ZooKeeperBuilder}}.\nComments: - Issue resolved by pull request 2274 [https://github.com/apache/zookeeper/pull/2274]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4942", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4942"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add option to preserve JVM TLS certification revocation properties\nDescription: The current behaviour is to overwrite the JVM global TLS certification revocation properties based on ZK configuration. This is a bad idea, as ZK is unlikely to be the only thing running in the JVM (unless it is the ZK server process), and the ZK settings (Even the OCSP/CRL disabled default settings) will apply to ALL connections in the JVM. Ideally, ZK would just leave these properties alone, but that would break backwards compatibility. I propose the following fixes: A. Change the default behaviour, so that if the _zookeeper.ssl.ocsp_ _zookeeper.ssl.crl_ properties are not defined then ZK skips changing the certificate revocation related global settings, and relies on the JVM System/Security properties. B. Change the default behaviour, so that if the _zookeeper.ssl.ocsp_ _zookeeper.ssl.crl_ properties are set to a specific value (i.e. \"system\") ZK skips changing the certificate revocation related global settings, and relies on the JVM System/Security properties. Option A. would be the more secure, as it doesn't overwrite the system settings, but it does break backwards compatibiliry with existing ZK versions. Option B. is less secure, as it requires explicit configuration not to clobber the JVM global settings, but it is fully backwards compatible with existing configurations.\nComments: - FYI [~andor] - As noted by [~andor] , this can be achieved by simply setting the revocation related config properties to their default (false) setting, so this is not really necessary. The new per-trustore revocation checking property makes this redundant.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4941", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4941"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Serveral SSL properties ignored when custom trustore is not specified\nDescription: CRL, OCSP, Hostname verification and fips are all ignored if there is no custom trustore specified. https://github.com/apache/zookeeper/blob/e5dd60bf0512ccc1e090d99410a8da48623219da/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java#L402 These properties are all meaningful for the default (cacerts) JVM certificates.\nComments: - TBH some of these properties are fishy. They are setting JVM global system and security properties, so they change the JVM global security settings, and apply to *ALL* TLS traffic of the *JVM*. It would probably be better to leave them alone and rely on the JVM system properties. - Thanks [~stoty] for bringing these up. I always thought that revocation properties are poorly handled in ZooKeeper. Which parameters are we talking about exactly? * boolean sslCrlEnabled = config.getBoolean(this.sslCrlEnabledProperty); * boolean sslOcspEnabled = config.getBoolean(this.sslOcspEnabledProperty); * boolean sslServerHostnameVerificationEnabled = isServerHostnameVerificationEnabled(config); * boolean sslClientHostnameVerificationEnabled = isClientHostnameVerificationEnabled(config); * boolean fipsMode = getFipsMode(config); *Hostname verification* on both client and server side relies on the custom truststore and the custom ZKTrustManager that we create for it. If we don't have these, how to set up hostname verification? Is that even possible? If yes, should we care about that instead of relying on JVM defaults (since we already use default trust manager)? *fipsMode* is a ZooKeeper specific setting, if we don't create TrustStore and ZKTrustManager, we can ignore it. *Revocation flags* (CRL, OCSP enabled) could be a TriState value as you propose in your PR to maintain backward compatibility. I'm not sure if disabling them in ZooKeeper makes too much, so we could have just ignored them if config value is false. Btw, what is the use case exactly? If truststore location is null, ZK won't touch any of the above default settings in JVM. Why do you want to do that from ZooKeeper? - We have discuessed this offline, and possibly in other tickets, but I will give quick summary here as well. - The listed options not having an effect if truststore is not set is acceptable, but that should be documented and maybe we should emit warnings when they have no effect. - You have also added a different hostname verification mechanism in ZOOKEEPER-4622 , that can work even without a custom truststore.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4940", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4940"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enabling zookeeper.ssl.ocsp with JRE TLS provider errors out\nDescription: The problem is that ZK uncoditionally calls *io.netty.handler.ssl.SslContextBuilder.enableOcsp(boolean)* when _zookeeper.ssl.ocsp_ is set to true, even though Netty explicitly does not support that for the JRE provider. For JRE OCSP is set in the javax.net.ssl.TrustManager object. I did not dig deep, but I presume that the OpenSSL provider ignores that, hence it needs another property. -To make this even more intersting, this setting doesn't actually do anything at all in Zookeeper.- -Zookeeper use netty-tcnative-boringssl-static , but this method is a NoOP for boringSSL, it is only supported by tcnative- -for OpenSSL.- -(I guess in theory the consumer could replace the tcnative implementation, in which case it would work as intended)- {noformat} [zk: ccycloud-1.nightly7310-og.root.comops.site:2182(CONNECTING) 0] 2025-06-18 04:06:01,013 [myid:] - WARN [zkNetty-EpollEventLoopGroup-1-1:o.a.z.c.ClientX509Util@72] - zookeeper.ssl.keyStore.location not specified 2025-06-18 04:06:01,074 [myid:] - WARN [zkNetty-EpollEventLoopGroup-1-1:i.n.c.ChannelInitializer@97] - Failed to initialize a channel. Closing: [id: 0x1fac3cf9] java.lang.IllegalArgumentException: OCSP is not supported with this SslProvider: JDK at io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:837) at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:648) at org.apache.zookeeper.common.ClientX509Util.createNettySslContextForClient(ClientX509Util.java:93) at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initSSL(ClientCnxnSocketNetty.java:449) at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initChannel(ClientCnxnSocketNetty.java:438) at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initChannel(ClientCnxnSocketNetty.java:424) at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:129) at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:112) at io.netty.channel.AbstractChannelHandlerContext.callHandlerAdded(AbstractChannelHandlerContext.java:1130) at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:558) at io.netty.channel.DefaultChannelPipeline.access$100(DefaultChannelPipeline.java:45) at io.netty.channel.DefaultChannelPipeline$PendingHandlerAddedTask.execute(DefaultChannelPipeline.java:1410) at io.netty.channel.DefaultChannelPipeline.callHandlerAddedForAllHandlers(DefaultChannelPipeline.java:1064) at io.netty.channel.DefaultChannelPipeline.invokeHandlerAddedIfNeeded(DefaultChannelPipeline.java:599) at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:513) at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428) at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485) at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173) at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:408) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:750) {noformat}\nComments: - FYI [~andor] - {quote}when _zookeeper.ssl.ocsp_ is set to true {quote} This sounds like a user error to me. Why do you want to set it if it's not supported by your runtime? - No. This happens if you set ocsp to true and use the JRE provider , which is totally supported by the runtime. Setting the parameter causes the Tcnative provider to request a stapled OCSP response from the server. The JRE provider has a system property for the same purpose, and no way to configure this on an Connection/SSLContext basis. So Netty is kind of right by not accepting the parameter for JRE, but this is still a horrible API. (And this still doesn't do anything unless you replace the default tcnative implementation used by ZK) - Could you please share both the client and server configuration (config files and command line parameters) which is broken? What if we just put this setting behind an SslProvider check and don't set it if SslProvider == JDK? - {quote}Setting the parameter causes the Tcnative provider to request a stapled OCSP response from the server. {quote} Really? We've never added OCSP stapling support to ZooKeeper. Based on the JDK documentation, it can be enabled on both client and server side by setting {noformat} jdk.tls.client.enableStatusRequestExtension=true jdk.tls.server.enableStatusRequestExtension=true{noformat} system properties, but ZooKeeper doesn't touch these. Therefore we only do CRL and client-side OCSP if revocation is enabled. In which case we might not need to set it in SslContext at all. Have you found Netty's documentation of this property? - [Even better|https://netty.io/4.1/api/io/netty/handler/ssl/OpenSsl.html#isOcspSupported--] {code:java} if (OpenSsl.isOcspSupported()) { sslContextBuilder.enableOcsp(config.getBoolean(getSslOcspEnabledProperty())); } {code} which will enable Ocsp stapling if set in the ZooKeeper configuration _and_ the provider supports it. For the JRE provider one must set it via the above mentioned properties. I'm not sure if we want to integrate that in ZooKeeper, though we already set some other system properties on the server side. - I've just replied to the same issue on the PR. jdk.tls.client.enableStatusRequestExtension=true jdk.tls.server.enableStatusRequestExtension=true works for the JRE TLS provider, while sslContextBuilder.enableOcsp() works for tcnative. (if supported) The problem is that sslContextBuilder.enableOcsp() when the provider is JRE will just throw an exception, hence we need to check for the provider before calling it. OpenSsl.isOcspSupported() only tells us if the native library loaded by tcnative supports it, but that's unrelated to whether we're using tcnative or the JRE provider. i.e tcnative could have loaded an OpenSSL native library that supports OCSP, but if we set the JRE TLS provider for Netty, it still won't work. - Issue resolved by pull request 2282 [https://github.com/apache/zookeeper/pull/2282]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4939", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4939"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: c client side support skip cert verification\nDescription: Based on ZOOKEEPER-4929\uff0c I think we need make c client side support skip cert verification\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4938", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4938"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Allow specifying LoginContext directly\nDescription: Currently ZK allows setting the Application name, but always uses the global javax.security.auth.login.Configuration object to retirieve it. Some applications (notably anything using the Hadoop UserGroupInformation) create and set their own LoginContext object. For these cases, the user somehow needs to duplicate the required jaas Configuration, and install it globally (either creating a _jaas.conf_ file and setting the_java.security.auth.login.config_ system property, or by programmatically installing a Configuration object) Life would be MUCH easier if ZK allowed directly setting the LoginContext object for Connections, so that applications can set the SASLconfig without having to override the global Configuration.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4937", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4937"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support commit message edit in zk-merge-pr.py\nDescription: {{zk-merge-pr.py}} supports only commit message squash. I would like it to support: 1. commit message preview 2. commit message edition\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4936", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4936"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make ZookeeperServer.shutdown(fullyShutdown) Non-final fix curator's TestingZookeeperMain\nDescription: Curator's [TestingZookeeperMain|https://github.com/apache/curator/blob/master/curator-test/src/main/java/org/apache/curator/test/TestingZooKeeperMain.java#L259] is used for integration testing in many projects. It overrides {{Zookeeper#shutdown(fullyShutdown)}} to add additional logic. But recent change in this [commit|https://github.com/apache/zookeeper/commit/bc9afbf8ef1bc6156643d3d05c87fcf8411e9d8f] has made {{Zookeeper#shutdown(fullyShutdown)}} final. Due to this 3.9.3 Zookeeper is unable to be used in TestingZooKeeperMain. Resulting in {quote}java.lang.VerifyError: class org.apache.curator.test.TestingZooKeeperMain$TestZooKeeperServer overrides final method org.apache.zookeeper.server.ZooKeeperServer.shutdown(Z)V{quote}\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4935", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4935"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix potential resource leak\nDescription: zookeeper-server/src/main/java/org/apache/zookeeper/server/controller/ControllerServerConfig.java#ensureComplete Any exception occurring before both {{close()}} calls complete successfully can lead to one or both {{ServerSocket}} instances (and the system ports they hold) not being released. The best way to fix this is to use the {{try-with-resources}} statement, which guarantees that {{Closeable}} resources are closed, even if exceptions occur. Besides, it seems there are some other leaks I found later, which not methoned in github issue: leak2: org.apache.zookeeper.server.quorum.Learner#validateSession DataOutputStream dos if an exception occurs before dos.close(); is reached, dos.close() will be skipped. !image-2025-06-04-13-13-28-697.png! leak3: org.apache.zookeeper.server.quorum.Learner#request DataOutputStream oa if an exception occurs before oa.close(); is reached, oa.close() will be skipped. !image-2025-06-04-13-14-27-680.png! leak 4 org.apache.zookeeper.server.SnapshotComparer#getSnapshot The code opens snapIS to read from the snapshot file but never explicitly calls snapIS.close(). !image-2025-06-04-13-26-06-474.png! leak 5 org.apache.zookeeper.server.TraceFormatter#main FileInputStream\u3001FileChannel !image-2025-06-04-13-30-56-580.png! They needs to be explicitly released by calling their close() method.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4934", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4934"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add metrics for TTL node creation\nDescription: Add metrics for the number of TTL nodes being created\nComments: - Hi [~liwang] , I have raised a PR for this ticket. Please kindly take a look. Thanks in advance.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4933", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4933"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Connection throttle exception causing all connections to be rejected\nDescription: When there is no request for a long time, the diff may be very large, causing the refill value to become negative. !image-2025-05-29-18-01-04-561.png!\nComments: - Issue resolved by pull request 2264 [https://github.com/apache/zookeeper/pull/2264]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4932", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4932"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: The newest version of zookeeper includes Jetty versiob 9.4.57.x which has CVE-2024-6763 issue\nDescription: Hi, Can you please help me on below request. The newest version of zookeeper includes Jetty version 9.4.57.x which has CVE-2024-6763 issue. When can the Jetty version will be upgraded to 12.0.12 or greater for zookeeper 3.9.4 or greater version https://github.com/apache/zookeeper/blob/release-3.9.3-2/pom.xml#L563\nComments: - According to ZOOKEEPER-4876 ZooKeeper is not vulnerable to CVE-2024-6763, but looks like we accidentally removed it from Owasp suppressions. Let me put it back with this ticket. We cannot upgrade to the latest version of Jetty, because it doesn't support Java 8 anymore. cc [~eolivelli] [~kezhuw]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4931", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4931"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade Jetty to a non-EOL version\nDescription: Hey everyone, while scrolling through our SBOMs I noticed that Zookeeper is using a pretty \"old\" version of Jetty - it's still getting security updates it seems, but ZK is using Jetty 9 while 12 has been released. I'm sure it's not that easy and there's a thrifty reason, but could someone maybe elaborate why it's not being updated? Thanks in advance!\nComments: - Jetty 12 requires Java 17. Upgrading to Jetty 12 implies dropping support for any Java older than 17. - Is upgrading Jetty and aligning with Java 17 something that's feasible for ZooKeeper at this stage, or would we likely run into bigger issues and end up wasting effort due to unforeseen compatibility challenges?", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4930", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4930"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add metrics for TTL node deletion\nDescription: Add metric for the number of TTL nodes being deleted by the server to provide more visibility on TTL node cleanup.\nComments: - Hi [~liwang] , just noticed this is already merged to master. Do we need to resolve this? Or does it need to wait to be backported to the other branches (e.g. branch-3.9)?", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4929", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4929"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make c client side cert optional in connecting to tls server\nDescription: Created for [https://github.com/apache/zookeeper/pull/2257] Currently, client side cert is mandatory to connect to tls server.\nComments: - master commit: d7f9717adf46addb582c0e0d90bf03a6ac6666af", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4928", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4928"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Version in zookeeper_version.h is not updated\nDescription: [~ctubbsii] pointed out this in https://github.com/apache/zookeeper/pull/2241. I confirmed this in https://github.com/apache/zookeeper/blob/release-3.9.4-0/zookeeper-client/zookeeper-client-c/include/zookeeper_version.h cc [~Tison]\nComments: - I am not sure how release manager update versions in c client, but seems that [replace-cclient-files-during-release|https://github.com/apache/zookeeper/blob/2aaeff840e8e5b61a30530b261b8c05c181d078f/zookeeper-client/zookeeper-client-c/pom.xml#L119] was skipped long time ago in ZOOKEEPER-3226([pr-758|https://github.com/apache/zookeeper/pull/758/files#diff-c0941f37053212308258c9f1382b7ab811ecf40ad05a3c4d683b3d5e0829f4e6L33]). - Found https://cwiki.apache.org/confluence/display/ZOOKEEPER/HowToRelease+using+maven+release+plugin. Profile {{full-build}} is needed to generate versions for c client. - Issue resolved by pull request 2259 [https://github.com/apache/zookeeper/pull/2259]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4927", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4927"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support more fine tunable purgeInterval\nDescription: A zookeeper instance went down in prod because it ran out of disk space. It turned out that the purge task was not able to keep up with the rate of snapshot taken. A new snapshot was taken every a couple of mins. Too snapshots were generate during autoPurge.purgeInterval. Since the unit is hour, so the min internal is 1 hour. To support writes heavy use case, we would need to support more fine tuned purge interval. For example, in minutes.\nComments: - {code:java} autopurge.purgeInterval : (No Java system property) New in 3.4.0: The time interval in hours for which the purge task has to be triggered. Set to a positive integer (1 and above) to enable the auto purging. Defaults to 0. {code} - Duplicate for ZOOKEEPER-4829 ? - Yes, that's right. Looks like it's been fix in 3.10.0 Can we backport this to 3.9? - I found two mails about backport: # [https://lists.apache.org/thread/q1wfmc1bomswjvolmkwv9bfxpqgmhlxg] # [https://lists.apache.org/thread/tdoc8wco2606qcb4myl5fttnhwl9dmwf] Currently, 3.9 is the \"current\" release so it can accept improvement commits(a.k.a. not bug fix). I think ZOOKEEPER-4829 is small and could be considered as an improvement, but [the commit|https://github.com/apache/zookeeper/pull/2178/files#diff-c184aadd789e20edde3f3f4a95c8d6f2ae0802c90fae2ed11df8878062fd0a1fL872] itself renamed a public class [QuorumPeerConfig|https://github.com/apache/zookeeper/blob/ec0aa27b1053c839333038efbada98b6a3504728/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java#L63]. I think this is not allowed according to our [Backward Compatibility|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=25200595#ReleaseManagement-BackwardCompatibility]. So we may have to fix that before backport. - Thanks for looking into this [~kezhuw] {quote}but the commit itself renamed a public class QuorumPeerConfig.{quote} Do you mean the getPurgeInterval() API is renamed to getPurgeIntervalInMs()? - {quote}we may have to fix that before backport.{quote} How about we keep the original getPurgeInterval() API but add documentation to indicate it's in milliseconds? I can submit a fix to make it backward compatible. WDYT? - {quote} How about we keep the original getPurgeInterval() API but add documentation to indicate it's in milliseconds? {quote} Hmm, I don't think people want this. I authored https://github.com/apache/zookeeper/pull/2258 - Yes, the solution in the above PR is a better solution for backward compatibility. Let me review it.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4926", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4926"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CVE-2025-24970,CVE-2025-25193 and CVE-2024-47535 reported by black duck on zookeeper 3.9.3\nDescription: \nComments: - Can anyone confirm if zookeeper 3.9.3 is vulnerable to the CVE's mentioned in the title.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4925", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4925"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Diff sync introduce hole in stale follower's committedLog which cause data loss in leading\nDescription: There are two variants of {{ZooKeeperServer::processTxn}}. Those two variants diverge in behavior since ZOOKEEPER-3484. {{processTxn(Request request)}} pops outstanding change from {{outstandingChanges}} and adds txn to {{committedLog}} for follower to sync in addition to what {{processTxn(TxnHeader hdr, Record txn)}} does. The {{Learner}} uses {{processTxn(TxnHeader hdr, Record txn)}} to commit txn to memory after ZOOKEEPER-4394, which means it leaves {{committedLog}} untouched in {{SYNCHRONIZATION}} phase. In above case, a stale follower will have hole in its {{committedLog}} after joining cluster. The stale follower will propagate the in memory hole to other stale nodes after becoming leader. This causes data loss.\nComments: - Added a test case for this: https://github.com/kezhuw/zookeeper/commit/c9e00b87bd0c952d382250d7b911896f52e92407. Only 3.9.3 is affected - Just in case, I found this in writing test cases for ZOOKEEPER-4882.", "output": "Status: Closed\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4924", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4924"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix double \"the\" word typos\nDescription: Resolved in https://github.com/apache/zookeeper/commit/bf71704249f447601926d96592075809dcb1e8e9\nComments: - make new Issue for this (PR)[https://github.com/apache/zookeeper/pull/2251/files] - I have fixed this", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4923", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4923"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support individual timeout to establish a brand-new session\nDescription: See dev mail: https://lists.apache.org/thread/nfb9z7rhgglbjzfxvg4z2m3pks53b3c1\nComments: No comments.", "output": "Status: Open\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4922", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4922"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add Audit Logging Support for Login Events in ZooKeeper\nDescription: ZooKeeper implemented audit logging in ZOOKEEPER-1260, but currently lacks audit records for authentication events. h3. Problem Statement After analyzing the {{org.apache.zookeeper.Login}} class, we identified the following gaps: # {*}No audit records for login events{*}: Authentication success/failure events are not logged # {*}Missing Kerberos authentication auditing{*}: Kerberos ticket operations are not recorded # {*}No tracking of TGT renewal events{*}: Ticket renewal operations have no audit trail h3. Proposed Solution # {*}Add login event auditing{*}: {{// In Login.login() method}} {{ZKAuditProvider.log(principal, \"login\", Result.SUCCESS);}} {{// For login failures}} {{ZKAuditProvider.log(principal, \"login\", Result.FAILURE);}} # {*}Add ticket renewal auditing{*}: {{// In reLogin() methodZKAuditProvider.log(principal, \"ticketRenewal\", Result.SUCCESS);}} # {*}Add logout event auditing{*}: {{// In logout() method}} {{ZKAuditProvider.log(principal, \"logout\", Result.SUCCESS);}} {{}}\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4921", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4921"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper Client 3.9.3 Fails to Reconnect After Network Failures\nDescription: After upgrading the Java Zookeeper client to version 3.9.3, we observed that it is not resilient to brief network disruptions, such as a short VPN blip. In such cases, the client attempts to reconnect only once, and if unsuccessful, the session expires. {quote}Apr 23, 2025 10:19:23 AM com.twitter.finagle.common.zookeeper.ZooKeeperClient$3 process INFO: Zookeeper session expired. Event: WatchedEvent state:Expired type:None path:null zxid: -1 {quote} In contrast, the previous version (3.9.2) would continuously retry until the network connection was restored, maintaining the session more reliably. I believe it's a new issue with this change: https://issues.apache.org/jira/browse/ZOOKEEPER-4508 Step to repro: # Open VPN. # Start the application which connects to the Zookeeper server with the VPN. # Disable VPN for a couple of minutes. # Observe the application. # Enable the VPN again. {quote}3.9.3: \"message\" : \"Session 0x0 for server XXX, Closing socket connection. Attempting reconnect except it is a SessionExpiredException or SessionTimeoutException.\", \"stackTrace\" : \"o.a.z.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 5590ms for session id 0x0 at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1253) {quote} 3.9.2: Application will be reconnected successfully.\nComments: - cc [~andor] as I see your comment on the previous issue. - The Accumulo team has noticed this and has developed a solution, a \"ZooSession\" object that mostly mimics the API of ZooKeeper client object, and automatically reconnects. However, it needs additional work to be fully resilient to exceptions thrown due to transient failures. Some of that is encapsulated in a helper \"ZooReader\" and \"ZooReaderWriter\" objects which retry operations on transient errors, and plan to merge those into the ZooSession feature. I think if we can make it resilient enough to be generally useful, it is something that we could contribute directly to the ZooKeeper project. However, that requires some time to polish up the API and behavior, and we have a lot of other development tasks we're working on. But, I think the idea is sound. It's very weird, after all, that the ZK client is useless after network failures. It's a bit like having to restart your browser every time you navigate to website with an error. That shouldn't be necessary. You may want to take a look at the workarounds that the Accumulo project has come up with for handling these transient issues. - Hi [~tranvanchuong1995], did you have more logs during repro steps ? > Disable VPN for a couple of minutes. What is the ZooKeeper session timeout ? ZOOKEEPER-4508 enforced client side check to expire zookeeper session in case of unreachable of cluster to avoid endless reconnection(a.k.a. no {{Expired}} after lose connection to cluster). The timeout is {{sessionTimeout * 4 /3}}. The default max session timeout in zookeeper is 1 minute, \"a couple of minutes\" usually enough for a zookeeper session to expire. - {noformat} \"stackTrace\" : \"o.a.z.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 5590ms for session id 0x0 {noformat} This is the log when client decide to give up retry. 1. This log comes from an unestablished zookeeper session as it prints \"session id 0x0\". 2. The server is configured with a small session timeout as the default min expiration timeout in client is {{8000ms}} (it is {{4/3}} of the default min session timeout in server which is {{6000ms}}). - [~kezhuw] This is the full log of 3.9.3: { \"time\" : \"2025-04-23T18:51:35.611-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZkAnnouncer Mutator-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Session 0x100707c2dc261da for server XXX/YYYY:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException or SessionTimeoutException.\", \"stackTrace\" : \"o.a.z.ClientCnxn$ConnectionTimeoutException: Client connection timed out, have not heard from server in 2667ms for session id 0x100707c2dc261da at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1259) \" } { \"time\" : \"2025-04-23T18:51:37.218-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZkAnnouncer Mutator-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Opening socket connection to server XXX/YYYY:2181.\" } { \"time\" : \"2025-04-23T18:51:37.219-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZkAnnouncer Mutator-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"SASL config status: Will not attempt to authenticate using SASL (unknown error)\" } { \"time\" : \"2025-04-23T18:51:41.355-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZkAnnouncer Mutator-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Client session timed out, have not heard from server in 8393ms for session id 0x100707c2dc261da\" } { \"time\" : \"2025-04-23T18:51:41.360-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZkAnnouncer Mutator-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Session 0x100707c2dc261da for server XXX/YYYY:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException or SessionTimeoutException.\", \"stackTrace\" : \"o.a.z.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 8393ms for session id 0x100707c2dc261da at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1253) \" } Apr 23, 2025 6:51:41 PM com.twitter.finagle.common.zookeeper.ZooKeeperClient$3 process INFO: Zookeeper session expired. Event: WatchedEvent state:Expired type:None path:null zxid: -1 { \"time\" : \"2025-04-23T18:51:41.588-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZkAnnouncer Mutator-EventThread\", \"level\" : \"INFO\", \"message\" : \"EventThread shut down for session: 0x100707c2dc261da\" } { \"time\" : \"2025-04-23T18:51:41.595-07:00\", \"logger_name\" : \"org.apache.zookeeper.ZooKeeper\", \"thread_name\" : \"ZookeeperClient-watcherProcessor\", \"level\" : \"INFO\", \"message\" : \"Initiating client connection, connectString=XXX:2181 sessionTimeout=4000 watcher=com.twitter.finagle.common.zookeeper.ZooKeeperClient$3@6ecbbfdf\" } { \"time\" : \"2025-04-23T18:51:41.596-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxnSocket\", \"thread_name\" : \"ZookeeperClient-watcherProcessor\", \"level\" : \"INFO\", \"message\" : \"jute.maxbuffer value is 1048575 Bytes\" } { \"time\" : \"2025-04-23T18:51:41.597-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor\", \"level\" : \"INFO\", \"message\" : \"zookeeper.request.timeout value is 0. feature enabled=false\" } { \"time\" : \"2025-04-23T18:51:41.600-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Opening socket connection to server XXX/YYYY:2181.\" } { \"time\" : \"2025-04-23T18:51:41.601-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"SASL config status: Will not attempt to authenticate using SASL (unknown error)\" } { \"time\" : \"2025-04-23T18:51:45.613-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Session 0x0 for server XXX/YYYY:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException or SessionTimeoutException.\", \"stackTrace\" : \"o.a.z.ClientCnxn$ConnectionTimeoutException: Client connection timed out, have not heard from server in 4005ms for session id 0x0 at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1259) \" } { \"time\" : \"2025-04-23T18:51:46.730-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Opening socket connection to server XXX/YYYY:2181.\" } { \"time\" : \"2025-04-23T18:51:46.730-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"SASL config status: Will not attempt to authenticate using SASL (unknown error)\" } { \"time\" : \"2025-04-23T18:51:50.732-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Client session timed out, have not heard from server in 9134ms for session id 0x0\" } { \"time\" : \"2025-04-23T18:51:50.732-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Session 0x0 for server XXX/YYYY:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException or SessionTimeoutException.\", \"stackTrace\" : \"o.a.z.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 9134ms for session id 0x0 at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1253) \" } Apr 23, 2025 6:51:50 PM com.twitter.finagle.common.zookeeper.ZooKeeperClient$3 process INFO: Zookeeper session expired. Event: WatchedEvent state:Expired type:None path:null zxid: -1 - With 3.9.2, it will retry really hard until we connect to the VPN: { \"time\" : \"2025-04-23T19:42:23.638-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Session 0x100707c2dc26281 for server XXX:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.\", \"stackTrace\" : \"j.l.IllegalArgumentException: Unable to canonicalize address XXX:2181 because it's not resolvable at o.a.z.SaslServerPrincipal.getServerPrincipal(SaslServerPrincipal.java:78) at o.a.z.SaslServerPrincipal.getServerPrincipal(SaslServerPrincipal.java:41) at o.a.z.ClientCnxn$SendThread.startConnect(ClientCnxn.java:1142) at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1192) \" } { \"time\" : \"2025-04-23T19:42:25.506-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Opening socket connection to server XXX/YYY:2181.\" } { \"time\" : \"2025-04-23T19:42:25.507-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"SASL config status: Will not attempt to authenticate using SASL (unknown error)\" } { \"time\" : \"2025-04-23T19:42:25.519-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Socket connection established, initiating session, client: /10.4.9.178:61916, server: XXX/YYY:2181\" } { \"time\" : \"2025-04-23T19:42:25.536-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Unable to reconnect to ZooKeeper service, session 0x100707c2dc26281 has expired\" } { \"time\" : \"2025-04-23T19:42:25.537-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"WARN\", \"myid\" : \"XXX:2181\", \"message\" : \"Session 0x100707c2dc26281 for server XXX/YYY:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.\", \"stackTrace\" : \"o.a.z.ClientCnxn$SessionExpiredException: Unable to reconnect to ZooKeeper service, session 0x100707c2dc26281 has expired at o.a.z.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1418) at o.a.z.ClientCnxnSocket.readConnectResult(ClientCnxnSocket.java:148) at o.a.z.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:86) at o.a.z.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at o.a.z.ClientCnxn$SendThread.run(ClientCnxn.java:1274) \" } { \"time\" : \"2025-04-23T19:42:25.542-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-EventThread\", \"level\" : \"INFO\", \"message\" : \"EventThread shut down for session: 0x100707c2dc26281\" } { \"time\" : \"2025-04-23T19:42:25.543-07:00\", \"logger_name\" : \"org.apache.zookeeper.ZooKeeper\", \"thread_name\" : \"ZookeeperClient-watcherProcessor\", \"level\" : \"INFO\", \"message\" : \"Initiating client connection, connectString=XXX:2181 sessionTimeout=4000 watcher=com.twitter.finagle.common.zookeeper.ZooKeeperClient$3@38b9cd1e\" } { \"time\" : \"2025-04-23T19:42:25.544-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxnSocket\", \"thread_name\" : \"ZookeeperClient-watcherProcessor\", \"level\" : \"INFO\", \"message\" : \"jute.maxbuffer value is 1048575 Bytes\" } { \"time\" : \"2025-04-23T19:42:25.545-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor\", \"level\" : \"INFO\", \"message\" : \"zookeeper.request.timeout value is 0. feature enabled=false\" } { \"time\" : \"2025-04-23T19:42:25.546-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Opening socket connection to server XXX/YYY:2181.\" } { \"time\" : \"2025-04-23T19:42:25.547-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"SASL config status: Will not attempt to authenticate using SASL (unknown error)\" } Apr 23, 2025 7:42:25 PM com.twitter.finagle.common.zookeeper.ZooKeeperClient$3 process INFO: Zookeeper session expired. Event: WatchedEvent state:Expired type:None path:null zxid: -1 { \"time\" : \"2025-04-23T19:42:25.557-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Socket connection established, initiating session, client: /10.4.9.178:61917, server: XXX/YYY:2181\" } { \"time\" : \"2025-04-23T19:42:25.571-07:00\", \"logger_name\" : \"org.apache.zookeeper.ClientCnxn\", \"thread_name\" : \"ZookeeperClient-watcherProcessor-SendThread(XXX:2181)\", \"level\" : \"INFO\", \"myid\" : \"XXX:2181\", \"message\" : \"Session establishment complete on server XXX/YYY:2181, session id = 0x100707c2dc26286, negotiated timeout = 4000\" } Apr 23, 2025 7:42:25 PM com.twitter.finagle.common.zookeeper.Group$ActiveMembership join INFO: Membership path is : /env-q/finagle/service/example-server/thrift/member_0000000003 Apr 23, 2025 7:42:25 PM com.twitter.finagle.common.zookeeper.Group$ActiveMembership join INFO: Set group member ID to member_0000000003 - [~tranvanchuong1995] Thank you for your sharing! You could upload log file directly to jira. 3.9.3: {noformat} o.a.z.ClientCnxn$ConnectionTimeoutException: Client connection timed out, have not heard from server in 2667ms for session id 0x100707c2dc261da {noformat} 3.9.2: {noformat} Session 0x100707c2dc26281 for server XXX:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException. {noformat} Sessions will expired finally in your cases. In case of 3.9.3, it is *considered expired* by client, while in 3.9.2, it is *confirmed expired* by server. There is no much differences in these two cases from client's perspective except *the later will be delayed substantially if cluster is unreachable*, just like this case. {noformat} With 3.9.2, it will retry really hard until we connect to the VPN: {\"2025-04-23T19:42:25.543-07:00\",\"Initiating client connection, connectString=XXX:2181 sessionTimeout=4000 \" } {\"2025-04-23T19:42:25.557-07:00\", \"Socket connection established, initiating session, client: /10.4.9.178:61917, server: XXX/YYY:2181\" } {\"2025-04-23T19:42:25.571-07:00\", \"Session establishment complete on server XXX/YYY:2181, session id = 0x100707c2dc26286, negotiated timeout = 4000\" } {noformat} It will retry hard(*actually endless*) to get reconnected and to know its expiration. The next client will success immediately in your case as network has been repaired. 3.9.3: {noformat} 2025-04-23T18:51:41.595-07:00, Initiating client connection, connectString=XXX:2181 sessionTimeout=4000 2025-04-23T18:51:41.600-07:00, Opening socket connection to server XXX/YYYY:2181 2025-04-23T18:51:45.613-07:00, Client connection timed out, have not heard from server in 4005ms for session id 0x0 2025-04-23T18:51:46.730-07:00, Opening socket connection to server XXX/YYYY:2181 2025-04-23T18:51:50.732-07:00, Client session timed out, have not heard from server in 9134ms for session id 0x0 {noformat} From the log, I can tell the it try twice before exhausting expiration timeout(a.k.a. 4/3 * 4000ms). 3.9.2 {noformat} Session establishment complete on server XXX/YYY:2181, session id = 0x100707c2dc26286, negotiated timeout = 4000 {noformat} In 3.9.2, the client will keeping retry(*actuall endless*) until a brand new session established while 3.9.3 does not. Those endless retry is what ZOOKEEPER-4508 try to fix so client can get prompt notification according to session timeout and react somehow. I think you could mimic 3.9.2 behavior by looping new session establishment so you could get a brand new session after network repaired. - [~kezhuw] Are you saying that starting with version 3.9.3, the application is now responsible for managing session establishment itself? In our case, the Zookeeper library is embedded within the Twitter Finagle library, creating multiple layers of abstraction that make it nearly impossible for us to override this behavior. - > Are you saying that starting with version 3.9.3, the application is now responsible for managing session establishment itself? {{org.apache.zookeeper.ZooKeeper}} manages a single session which tolerate network downtime within session timeout. Beyond that, applications(Finagle, Curator, etc.) have to manage themselves across sessions. > In our case, the Zookeeper library is embedded within the Twitter Finagle library, creating multiple layers of abstraction that make it nearly impossible for us to override this behavior. Seems that finagle's [ZooKeeperClient.get|https://github.com/twitter/finagle/blob/develop/finagle-serversets/src/main/java/com/twitter/finagle/common/zookeeper/ZooKeeperClient.java#L357] relies on the endless retrying during estalishing a brand new session. We could fix it there. Alternative, we could provide a option for zookeeper to loop until a customized timeout in new session establishment. I think it might be a common pattern to get {{Expired}} and then loop until connected. - > Alternative, we could provide a option for zookeeper to loop until a customized timeout in new session establishment. I think it might be a common pattern to get {{Expired}} and then loop until connected. I sent a dev mail to discuss this. https://lists.apache.org/thread/nfb9z7rhgglbjzfxvg4z2m3pks53b3c1 - [~kezhuw] this dev mail discussion looks really good. I am looking forward to hearing about the final decision. Thank you! - Issue resolved by pull request 2252 [https://github.com/apache/zookeeper/pull/2252]", "output": "Status: Closed\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4920", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4920"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: ZooKeeperServerMaxCnxnsTest is flaky\nDescription: https://github.com/apache/zookeeper/actions/runs/14560136021/job/40842324999?pr=2211#step:6:494 {noformat} [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.06 s <<< FAILURE! - in org.apache.zookeeper.server.ZooKeeperServerMaxCnxnsTest [ERROR] testMaxZooKeeperClientsWithNIOServerCnxnFactory Time elapsed: 2.687 s <<< FAILURE! org.opentest4j.AssertionFailedError: Client is not supposed to get connected as max connection already reached. at org.apache.zookeeper.server.ZooKeeperServerMaxCnxnsTest.testMaxZooKeeperClients(ZooKeeperServerMaxCnxnsTest.java:124) at org.apache.zookeeper.server.ZooKeeperServerMaxCnxnsTest.testMaxZooKeeperClientsWithNIOServerCnxnFactory(ZooKeeperServerMaxCnxnsTest.java:58) {noformat}\nComments: - Issue resolved by pull request 2249 [https://github.com/apache/zookeeper/pull/2249]", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4919", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4919"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: ResponseCache supposed to be a LRU cache\nDescription: Created for https://github.com/apache/zookeeper/pull/2243\nComments: - Issue resolved by pull request 2243 [https://github.com/apache/zookeeper/pull/2243] - Thank you for backporting this also to older releases!", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4918", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4918"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bump com.fasterxml.jackson.core:jackson-core from 2.15.2 to 2.18.1\nDescription: The 2.15.2 has an affecting CVE-2023-35116 that might result in DoS.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4917", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4917"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper encountered an abnormal startup issue with jute.maxbuffer\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4916", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4916"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper's jute.maxbuffer issue causes cluster exceptions, how to raise alerts through monitoring\nDescription: From the implementation of ZooKeeper, deleting temporary nodes during closeSession is a deletion transaction operation based on the session, rather than creating a separate deletion transaction for each znode node. That's why there is an unreasonable length issue. But how to monitor the size of transactions? Does the official have any corresponding monitoring items or solutions to solve this problem\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4915", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4915"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Change default value of znode.container.maxNeverUsedIntervalMs\nDescription: Usually, {{CONTAINER}} node with no children will be deleted. But if that node is never used(a.k.a. no child created before), it will not be deleted unless {{znode.container.maxNeverUsedIntervalMs}} configured. Currently, {{znode.container.maxNeverUsedIntervalMs}} defaults to {{0}} which means container nodes that never had any children are never deleted. I think it would make more sense to give it a non {{0}} value, say 5 minutes, to make it work out of box.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4914", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4914"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: [QUESTION] Strategies for monitoring and preventing \"unreasonable length\" errors in session closure\nDescription: Description: =========== I'm encountering potential risks with \"unreasonable length\" errors during session closure, particularly when sessions have numerous ephemeral nodes. I'd like to discuss possible monitoring and prevention strategies. Current Situation: ----------------- 1. When sessions with many ephemeral nodes are closed, all node paths are collected into a single transaction 2. If the combined size exceeds jute.maxbuffer, it results in \"unreasonable length\" errors 3. Currently lacking effective ways to predict or prevent this issue Questions: --------- 1. Monitoring Strategy: * What metrics should we monitor to predict potential issues? * Are there existing metrics for tracking session transaction sizes? * How can we monitor the growth of ephemeral nodes per session? 2. Prevention Approaches: * What are the recommended approaches to prevent this issue? * Is there a way to estimate transaction size before session closure? * Are there best practices for managing large numbers of ephemeral nodes? 3. Configuration Guidelines: * What's the recommended jute.maxbuffer setting for different scenarios? * Are there other relevant configuration parameters? --------------- Would appreciate insights on: 1. Additional metrics to monitor 2. Early warning indicators 3. Prevention strategies 4. Best practices for large-scale deployments Thank you for any guidance or suggestions.\nComments: - I discovered an abnormality in the operation of the Zookeeper cluster, which resulted in a failure. Additionally, attempts to restart the cluster were unsuccessful.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4913", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4913"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Query on Apache zookeeper next stable version\nDescription: Currently we are using Apache zookeeper latest stable version 3.8.4. The current released version in the community is 3.9.3. Could you please let us know when Apache zookeeper 3.9.x new stable version will be released?\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4912", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4912"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove default TLS cipher overrides\nDescription: This is a follow-up on the discussion on the ZOOKEEPER-4415 [PR|https://github.com/apache/zookeeper/pull/1919] . ZK currently hardcodes the list of ciphers, and needs to add code to handle all new ciphers and Java TLS changes. This was originally added as a performance optimization, which is not very relevant today, and interferes with normal TLS operation. I propose removing the default cipher logic from X509Util. Ciphers could still be specified either by the existing config properties, or via the standard java properties / security config, but would otherwise default to the JVM defaults, and pick up any changes from new JDKs or security settings. This could cause performance problems for very old JDK8 JVMs, where the current behaviour can be restored by explicitly specifying the CBC cipher list, which should be added to the documentation.\nComments: - Issue resolved by pull request 2239 [https://github.com/apache/zookeeper/pull/2239]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4911", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4911"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support IPv6 prefixes in ACLs\nDescription: Currently we can define ACLS for IPv4 networks, but there is no similar support for IPv6 networks / prefixes. This would probably be best done by adding a new IPv6AuthenticationProvider similar to the existing IPv4 AuthenticationProvider.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4910", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4910"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: java.lang.NoClassDefFoundError: com/codahale/metrics/Reservoir\nDescription: when no dep dropwizard will appear java.lang.NoClassDefFoundError: com/codahale/metrics/Reservoir {code:java} Exception in thread \"ZooKeeper Server Starter\" java.lang.NoClassDefFoundError: com/codahale/metrics/Reservoir at org.apache.zookeeper.metrics.impl.DefaultMetricsProvider$DefaultMetricsContext.lambda$getSummary$2(DefaultMetricsProvider.java:126) at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1713) at org.apache.zookeeper.metrics.impl.DefaultMetricsProvider$DefaultMetricsContext.getSummary(DefaultMetricsProvider.java:122) at org.apache.zookeeper.server.ServerMetrics.<init>(ServerMetrics.java:74) at org.apache.zookeeper.server.ServerMetrics.<clinit>(ServerMetrics.java:44) at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:133) at org.apache.dubbo.springboot.demo.provider.EmbeddedZooKeeper$ServerRunnable.run(EmbeddedZooKeeper.java:249) at java.base/java.lang.Thread.run(Thread.java:1575) Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.Reservoir at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:528) ... 8 {code}\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4909", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4909"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: When a spurious wakeup occurs, the client\u2019s waiting time may exceed requestTimeout.\nDescription: I noticed that a while loop is used here to avoid spurious wakeups. I think we should calculate the total sleep time to prevent it from exceeding requestTimeout too much in case of spurious wakeups and the server not responding for a long time.\nComments: No comments.", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4908", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4908"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper encountered an abnormal startup issue with jute.maxbuffer\nDescription: ZooKeeper encountered a jute.maxbuffer restriction issue while loading the database. The default size of the current jute.maxbuffer is 1MB (1048575 bytes), but the length displayed in the log is 2429535, which far exceeds this value, resulting in an Unreasonable leLength error. Does the official stance not recommend modifying the jute.maxbuffer parameter, or can the configuration of jute.maxbuffer be moderately increased? If adjustable, what is the recommended value\nComments: - Hello [~alphachb520]. For more discussion of {{jute.maxbuffer}}, refer to the documentation here: https://zookeeper.apache.org/doc/r3.9.3/zookeeperAdmin.html The general guidance is that ZooKeeper is designed to store relatively small pieces of information in each znode. Storing large data may introduce latency spikes or quorum instability. Because of that, the community guidance is to stick with the default of 1 MB. You are free to adjust it higher, but be aware of the trade-offs, and consider if there are ways to make design changes to shrink the amount of data you need to store in a znode. If you need follow-up discussion, I recommend moving to the user@zookeeper.apache.org mailing list.", "output": "Status: Resolved\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4907", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4907"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Shouldn't throw \"Len error\" when server closing cause confusion\nDescription: We got the error: {code:java} 2024-11-07 19:03:01,414 [myid:14] - WARN [nioEventLoopGroup-7-25:NettyServerCnxn@537] - Closing connection to /135.224.186.250:47051 java.io.IOException: Len error 794913900 at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:521) at org.apache.zookeeper.server.NettyServerCnxn.processMessage(NettyServerCnxn.java:374) at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.channelRead(NettyServerCnxnFactory.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead {code} It cause us very confused about it : if we write some big data into zookeeper. Thus. we found, in actual. it is just the log/issue when closing the server when reelecting the leader sometime. We don't send any big data. So I think we can do one tiny code change to avoid throw the error which causing confuse with big size's data to reduce trouble shooting effort.\nComments: - [https://github.com/apache/zookeeper/pull/2236] attach the PR. - Issue resolved by pull request 2236 [https://github.com/apache/zookeeper/pull/2236]", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4906", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4906"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Log full exception details for server JAAS config failure\nDescription: For a server configured with SASL authentication, startup can fail due to failures loading the JAAS configuration file. ZooKeeper doesn't log the details of the failure though, which makes it difficult to troubleshoot. We can improve this by logging the full root cause exception.\nComments: No comments.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4905", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4905"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Use create2 only when required\nDescription: This could improve compatiblity to old server or ClickHouse Keeper.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4904", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4904"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: SessionTrackerImpl generates negative session IDs when server ID is larger than 127\nDescription: This issue was discovered during a [discussion|https://lists.apache.org/thread/0pwxw1rzdffmbxctdzv2rmplzgwt6lpl] about negative Solr Overseer IDs Setting the server ID to a value greater than 127 in the myid file causes the SessionTrackerImpl.initializeNextSessionId function to generate a negative session ID. {code:java} public static long initializeNextSessionId(long id) { long nextSid; nextSid = (Time.currentElapsedTime() << 24) >>> 8; nextSid = nextSid | (id << 56); <------------------------ if (nextSid == EphemeralType.CONTAINER_EPHEMERAL_OWNER) { ++nextSid; // this is an unlikely edge case, but check it just in case } return nextSid; } {code}\nComments: - @[~kezhuw] In this description, the server Id never change ,the initializeNextSessionId method paramter id is always 1 , I find all the all this fuction, it is not from myid file value\u3002 so it not a bug can we close this issue?", "output": "Status: Open\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4903", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4903"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: An Inconsistency of `lastProcessed` variable in Spec and Impl\nDescription: I have noticed that in the specification of ZKV3_7_0, it appears that the `SyncFollower` action uses an incorrect `lastProcessed`. In `SyncFollower`, the following code is used to determine which sync mode should be employed: {code:java} lastProcessedZxid == lastProcessed[i].zxid minCommittedIdx == lastSnapshot[i].index + 1 maxCommittedIdx == IF zabState[i] = BROADCAST THEN lastCommitted[i].index ELSE Len(history[i]) committedLogEmpty == minCommittedIdx > maxCommittedIdx minCommittedLog == IF committedLogEmpty THEN lastProcessedZxid ELSE history[i][minCommittedIdx].zxid maxCommittedLog == IF committedLogEmpty THEN lastProcessedZxid ELSE IF maxCommittedIdx = 0 THEN << 0, 0>> ELSE history[i][maxCommittedIdx].zxid {code} We can observe that `lastProcessedZxid` is assigned the value of `lastProcessed[i].zxid`, where `i` is the leader node ID and `lastProcessed` is a global variable in this specification. According to the comment in the specification, the `lastProcessed` variable means \"The index and zxid of the last processed transaction in history\". And we can see that it will be updated in the `StartZkServer\\(i\\)` Action with the following code: {code:java} LastProposed(i) == IF Len(history[i]) = 0 THEN [ index |-> 0, zxid |-> <<0, 0>> ] ELSE LET lastIndex == Len(history[i]) entry == history[i][lastIndex] IN [ index |-> lastIndex, zxid |-> entry.zxid ] StartZkServer(i) == LET latest == LastProposed(i) IN /\\ lastCommitted' = [lastCommitted EXCEPT ![i] = latest] /\\ lastProcessed' = [lastProcessed EXCEPT ![i] = latest] /\\ lastSnapshot' = [lastSnapshot EXCEPT ![i] = latest] /\\ UpdateElectionVote(i, acceptedEpoch[i]) {code} We know that when a cluster finishes an election but before receiving any user requests, according to the specification, it will contain no history. And in `StartZKServer\\(i\\)`, `lastSnapshot[i]` will be assigned `[ index \\|\\-> 0, zxid \\|\\-> <<0, 0>> ]`. However, in the implementation code, I found that the `lastProcessed` used in the `LearnHanler.syncFollower()` function is actually the `dataTree.lastProcessedZxid` variable. And this variable is updated with the following code when `ZkServer` is starting: {code:java} Leader.java : Line 657: zk.setZxid(ZxidUtils.makeZxid(epoch, 0)); Leader.java : Line 1609: zk.getZKDatabase().setlastProcessedZxid(zk.getZxid()); {code} Therefore, when a cluster finishes an election but before receiving any user requests, its value is indeed `<<1, 0>>`. This causes the following `LeaderSyncFollower` action to choose an incorrect sync mode in the specification. I found that the specification already seems to use a variable, namely `tempMaxEpoch`, to record `zk.hzxid` (which `zk.getZxid()` actually retrieves). It seems that we should use `tempMaxEpoch[i]` in the `StartZkServer` action. And the correct code should be: {code:java} LastProposed(i) == IF Len(history[i]) = 0 THEN [ index |-> 0, zxid |-> <<Maximum({tempMaxEpoch[i], 0}), 0>> ] ...... StartZkServer(i) == LET latest == LastProposed(i) IN ...... /\\ lastProcessed' = [lastProcessed EXCEPT ![i] = latest] ...... {code}\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4902", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4902"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Document that read-only mode also enables isro 4lw\nDescription: ZooKeeper admin docs state that the 4lw white list only contains \"srvr\" by default. However, if read-only mode is enabled, then this implicitly also adds the \"isro\" command to the whitelist. Update documentation to mention this.\nComments: No comments.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4901", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4901"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Correct logback license information on master branch.\nDescription: The last few Logback upgrades on the master branch missed updating the license files. Update them to match the current dependency version: 1.3.15.\nComments: - master via https://github.com/apache/zookeeper/pull/2229", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4900", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4900"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bump patch release of jetty to include CVE fix for CVE-2024-6763\nDescription: Jetty 9.4.57.v20241219 contains the fix for CVE-2024-6763. We can update the ZooKeeper dependency.\nComments: - Backport to branch-3.9: [https://github.com/apache/zookeeper/pull/2242] - Issue resolved by pull request 2220 [https://github.com/apache/zookeeper/pull/2220]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4899", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4899"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: We hope ZooKeeper is expected to support specifying a customized decryption class to parse certificate authentication keys to prevent plaintext passwords in process environment variables\nDescription: We hope ZooKeeper is expected to support specifying a customized decryption class to parse certificate authentication keys to prevent plaintext passwords in process environment variables\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4898", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4898"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: The incorrect usage of the subclass of ZooKeeperThread leads to the failure of exception handling\nDescription: WorkerSender inherits from ZooKeeperThread. When using WorkerSender, a WorkerSender thread is created first, followed by the creation of a new thread where WorkerSender is passed as a task. At this point, the ExceptionHandler in ZooKeeperThread does not take effect. !image-2025-02-26-09-48-32-165.png!!image-2025-02-26-09-43-37-299.png!\nComments: - I have submitted a Pull Request #2228 , and I hope it will be helpful. - master via https://github.com/apache/zookeeper/pull/2228", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4897", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4897"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Upgrade Netty to fix CVE-2025-24970 in ZooKeeper 3.9.3\nDescription: h3. *Details of the Issue* * {*}CVE ID{*}: [CVE-2025-24970|https://nvd.nist.gov/vuln/detail/CVE-2025-24970] * {*}Affected ZooKeeper Version{*}: 3.9.3 * {*}Vulnerable Dependency{*}: Netty 4.1.113 * {*}Impact{*}: When a special crafted packet is received via SslHandler it doesn't correctly handle validation of such a packet in all cases which can lead to a native crash. * {*}Fix{*}: Upgrade Netty to *4.1.118.Final* (or the version addressing this CVE).\nComments: - The issue has been resolved, and waiting for the new version - [~jimqin] Could you please let me know the expected release date for ZooKeeper 3.9.4? - [~dpramod] I have asked the main zookeeper contributor for this, no expected release date for 3.9.4 for now", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4896", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4896"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Inconsistency for a ZK Path in the cluster\nDescription: We are using Zookeeper in cluster mode. For one of the paths, we notice a data inconsistency between a follower node and the other nodes in the system. We observe that the version for the path increases alongside content updates. However, there are four additional entries in that path in the follower where the issue is present. The version consistently lags four updates behind both the leader and the other followers. If we try to delete one of the extra entries from that follower node, it fails with the error: Node does not exist. *Leader:* {{{}vmanage@60f9084accc9:/var/lib/zookeeper$ /var/lib/zookeeper/bin/zkServer.sh status{}}}{{{}ZooKeeper JMX enabled by default{}}}{{{}Using config: /var/lib/zookeeper/bin/../conf/zoo.cfg{}}}{{{}Client port found: 2181. Client address: localhost. Client SSL: false.{}}}{{{}Mode: leader{}}}{{{}vmanage@60f9084accc9:/var/lib/zookeeper${}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 1] stat -w /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}cZxid = 0x13000611b0{}}}{{{}ctime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}mZxid = 0x13000611b0{}}}{{{}mtime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}pZxid = 0x9109c98fb2{}}}{{{}cversion = 2332172{}}}{{{}dataVersion = 0{}}}{{{}aclVersion = 0{}}}{{{}ephemeralOwner = 0x0{}}}{{{}dataLength = 0{}}}{{{}numChildren = 22{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 2] ls /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}[20250206_0_685_142, 20250206_0_690_143, 20250206_0_695_144, 20250206_0_700_145, 20250206_686_686_0, 20250206_687_687_0, 20250206_688_688_0, 20250206_689_689_0, 20250206_690_690_0, 20250206_691_691_0, 20250206_692_692_0, 20250206_693_693_0, 20250206_694_694_0, 20250206_695_695_0, 20250206_696_696_0, 20250206_697_697_0, 20250206_698_698_0, 20250206_699_699_0, 20250206_700_700_0, 20250206_701_701_0, 20250206_702_702_0, 20250206_703_703_0]{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 3] stat -w /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}cZxid = 0x13000611b0{}}}{{{}ctime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}mZxid = 0x13000611b0{}}}{{{}mtime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}pZxid = 0x9109c9b371{}}}{{{}cversion = 2332178{}}}{{{}dataVersion = 0{}}}{{{}aclVersion = 0{}}}{{{}ephemeralOwner = 0x0{}}}{{{}dataLength = 0{}}}{{{}numChildren = 26{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 4] ls /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}[20250206_0_685_142, 20250206_0_690_143, 20250206_0_695_144, 20250206_0_700_145, 20250206_0_705_146, 20250206_686_686_0, 20250206_687_687_0, 20250206_688_688_0, 20250206_689_689_0, 20250206_690_690_0, 20250206_691_691_0, 20250206_692_692_0, 20250206_693_693_0, 20250206_694_694_0, 20250206_695_695_0, 20250206_696_696_0, 20250206_697_697_0, 20250206_698_698_0, 20250206_699_699_0, 20250206_700_700_0, 20250206_701_701_0, 20250206_702_702_0, 20250206_703_703_0, 20250206_704_704_0, 20250206_705_705_0, 20250206_706_706_0]{}}} *Follower-1 (No issue seen):* {{{}vmanage@aacf5ef6555b:/var/lib/zookeeper$ /var/lib/zookeeper/bin/zkServer.sh status{}}}{{{}ZooKeeper JMX enabled by default{}}}{{{}Using config: /var/lib/zookeeper/bin/../conf/zoo.cfg{}}}{{{}Client port found: 2181. Client address: localhost. Client SSL: false.{}}}{{{}Mode: follower{}}}{{{}vmanage@aacf5ef6555b:/var/lib/zookeeper${}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 1] stat -w /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}cZxid = 0x13000611b0{}}}{{{}ctime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}mZxid = 0x13000611b0{}}}{{{}mtime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}pZxid = 0x9109c98fb2{}}}{{{}cversion = 2332172{}}}{{{}dataVersion = 0{}}}{{{}aclVersion = 0{}}}{{{}ephemeralOwner = 0x0{}}}{{{}dataLength = 0{}}}{{{}numChildren = 22{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 2] ls /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}[20250206_0_685_142, 20250206_0_690_143, 20250206_0_695_144, 20250206_0_700_145, 20250206_686_686_0, 20250206_687_687_0, 20250206_688_688_0, 20250206_689_689_0, 20250206_690_690_0, 20250206_691_691_0, 20250206_692_692_0, 20250206_693_693_0, 20250206_694_694_0, 20250206_695_695_0, 20250206_696_696_0, 20250206_697_697_0, 20250206_698_698_0, 20250206_699_699_0, 20250206_700_700_0, 20250206_701_701_0, 20250206_702_702_0, 20250206_703_703_0]{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 3] stat -w /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}cZxid = 0x13000611b0{}}}{{{}ctime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}mZxid = 0x13000611b0{}}}{{{}mtime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}pZxid = 0x9109c9b371{}}}{{{}cversion = 2332178{}}}{{{}dataVersion = 0{}}}{{{}aclVersion = 0{}}}{{{}ephemeralOwner = 0x0{}}}{{{}dataLength = 0{}}}{{{}numChildren = 26{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 4] ls /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}[20250206_0_685_142, 20250206_0_690_143, 20250206_0_695_144, 20250206_0_700_145, 20250206_0_705_146, 20250206_686_686_0, 20250206_687_687_0, 20250206_688_688_0, 20250206_689_689_0, 20250206_690_690_0, 20250206_691_691_0, 20250206_692_692_0, 20250206_693_693_0, 20250206_694_694_0, 20250206_695_695_0, 20250206_696_696_0, 20250206_697_697_0, 20250206_698_698_0, 20250206_699_699_0, 20250206_700_700_0, 20250206_701_701_0, 20250206_702_702_0, 20250206_703_703_0, 20250206_704_704_0, 20250206_705_705_0, 20250206_706_706_0]{}}} *Follower-2 (Issue seen):* {{{}vmanage@fc890e903696:/var/lib/zookeeper$ /var/lib/zookeeper/bin/zkServer.sh status{}}}{{{}ZooKeeper JMX enabled by default{}}}{{{}Using config: /var/lib/zookeeper/bin/../conf/zoo.cfg{}}}{{{}Client port found: 2181. Client address: localhost. Client SSL: false.{}}}{{{}Mode: follower{}}}{{{}vmanage@fc890e903696:/var/lib/zookeeper${}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 1] stat -w /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}cZxid = 0x13000611b0{}}}{{{}ctime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}mZxid = 0x13000611b0{}}}{{{}mtime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}pZxid = 0x9109c98fb2{}}}{{{}cversion = 2332168{}}}{{{}dataVersion = 0{}}}{{{}aclVersion = 0{}}}{{{}ephemeralOwner = 0x0{}}}{{{}dataLength = 0{}}}{{{}numChildren = 26{}}}{{{}[zk: 127.0.0.1:2181(CONNECTED) 2]{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 2] ls /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}[20250130_0_4587_333, 20250130_0_4592_334, 20250130_4443_4565_11, 20250130_4560_4560_0, 20250206_0_685_142, 20250206_0_690_143, 20250206_0_695_144, 20250206_0_700_145, 20250206_686_686_0, 20250206_687_687_0, 20250206_688_688_0, 20250206_689_689_0, 20250206_690_690_0, 20250206_691_691_0, 20250206_692_692_0, 20250206_693_693_0, 20250206_694_694_0, 20250206_695_695_0, 20250206_696_696_0, 20250206_697_697_0, 20250206_698_698_0, 20250206_699_699_0, 20250206_700_700_0, 20250206_701_701_0, 20250206_702_702_0, 20250206_703_703_0]{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 3] stat -w /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}cZxid = 0x13000611b0{}}}{{{}ctime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}mZxid = 0x13000611b0{}}}{{{}mtime = Thu Jun 06 13:04:52 UTC 2024{}}}{{{}pZxid = 0x9109c9b371{}}}{{{}cversion = 2332174{}}}{{{}dataVersion = 0{}}}{{{}aclVersion = 0{}}}{{{}ephemeralOwner = 0x0{}}}{{{}dataLength = 0{}}}{{{}numChildren = 30{}}}{{{}[zk: 127.0.0.1:2181(CONNECTED) 4]{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 4] ls /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts{}}}{{{}[20250130_0_4587_333, 20250130_0_4592_334, 20250130_4443_4565_11, 20250130_4560_4560_0, 20250206_0_685_142, 20250206_0_690_143, 20250206_0_695_144, 20250206_0_700_145, 20250206_0_705_146, 20250206_686_686_0, 20250206_687_687_0, 20250206_688_688_0, 20250206_689_689_0, 20250206_690_690_0, 20250206_691_691_0, 20250206_692_692_0, 20250206_693_693_0, 20250206_694_694_0, 20250206_695_695_0, 20250206_696_696_0, 20250206_697_697_0, 20250206_698_698_0, 20250206_699_699_0, 20250206_700_700_0, 20250206_701_701_0, 20250206_702_702_0, 20250206_703_703_0, 20250206_704_704_0, 20250206_705_705_0, 20250206_706_706_0]{}}} {{{}[zk: 127.0.0.1:2181(CONNECTED) 5] delete /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts/20250130_0_4587_333{}}}{{{}Node does not exist: /clickhouse/tables/shard_2/aggregated_apps_dpi_app_summary_7f1396c5_a4d8_48c9_a3f3_90271305c114/replicas/replica_2/parts/20250130_0_4587_333{}}}\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4895", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4895"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce a helper function for C client to generate password for SASL authentication\nDescription: C client has provided zoo_sasl_make_basic_callbacks() function to help users initialize SASL callbacks with the password in the specified file. The client would use this password directly for SASL authentication. However, considering the security of the production environment, the password in a file is usually encrypted. Also, security software also scans files in the system and will issue an alert if it detects an unencrypted password. Therefore, we need a mechanism to read the encrypted text from the file and decrypt it to obtain the real password. To achieve this, a helper function is introduced to decrypt the encrypted text in the file using a user-provided custom callback. A handback object is also introduced to provide necessary information for the decryption.\nComments: - For the pull request please see https://github.com/apache/zookeeper/pull/2223. - Issue resolved by pull request 2223 [https://github.com/apache/zookeeper/pull/2223]", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4894", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4894"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update logback-core-1.2.13.jar to fix CVE-2024-12798\nDescription: zookeeper v3.9.3 has security vulnerability for logback-core-1.2.13.jar due to CVE-2024-12798 Awaiting analysis: [https://nvd.nist.gov/vuln/detail/CVE-2024-12798] {*}Fix Required{*}: upgrade to *logback-core:1.3.15* in latest zookeeper\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4893", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4893"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Excessive reconection delays due to hardcoded sleep intervals\nDescription: *Description* I'll try to explain our issue as clearly as I can. Some clients take too long to reconnect to a ZooKeeper cluster after a minor downtime. We have identified two hardcoded sleep intervals in the client connection logic that contribute to this issue, but they cannot be configured. Spending several seconds in this disconnected state, even though the cluster is up and healthy is an issue in our setup. *These are the two Thread.sleep() which I am referring to* 1. Random sleep (0-1000ms) before attempting a new connection: * [ClientCnxn.java#L1138 (release-3.9.3)|https://github.com/apache/zookeeper/blob/release-3.9.3/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L1138] 2. Fixed 1000ms sleep before reconnecting to the last known server: * [StaticHostProvider#L363 (release-3.9.3)|https://github.com/apache/zookeeper/blob/release-3.9.3/zookeeper-server/src/main/java/org/apache/zookeeper/client/StaticHostProvider.java#L362] *Example Scenario* Consider a three-node ZooKeeper cluster (node01, node02, node03) where node01 is currently the leader. 1. Event: Node01 is temporarily taken down for short maintenance (e.g. for security patching). 2. Result: The remaining nodes (node02 and node03) elect a new leader, completing within ~1000ms. 3. Client (that was connected to node01) behavior (worst-case scenario): * Connection to node01 is lost \u2192 client enters a suspended state. * Waits 500ms, attempts connection to node02 \u2192 fails (cluster not ready). * Waits 499ms, attempts connection to node03 \u2192 fails (cluster still not ready). * Waits 1000ms as we are now back to original node01 (sleep #2 in the list above) * Waits 1000ms before connecting to node01 -> fails (this node is down for maintenance) * Waits 1000ms before retrying node02 \u2192 finally succeeds. 4. Total reconnection time: ~4 seconds, despite the cluster being available after just 1 second. *Impact* * Clients remain in a suspended state longer than necessary, leading to degraded service availability. * The reconnection delay is artificially inflated due to hardcoded sleeps. *Suggested improvement* * Give the user an option to provide our own logic for how long we should sleep before retry. It could be making these sleep intervals configurable, but even better would be to be able to provide our own implementation of the waiting logic. *Offer to contribute* We would be happy to submit a pull request to address this issue if that would be helpful. Please let us know if a contribution would be welcomed and if you have any guidance on the preferred approach. Would appreciate any insights from the maintainers. Thanks!\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4892", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4892"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Inconsistency of the quorum rule of the leader election process between the Zookeeper implmentation and specification.\nDescription: It seems the quorum rule of the leader election process between the Zookeeper implmentation and specification is inconsistent. In {*}implementation{*}, according to the code in the _FastLeaderElection_ class, a node maintains the votes of all nodes in the cluster it considers through a `recvset` (which are modeled into _receiveVotes_ variable in the specification). At the same time, it maintains its own vote through the variables {_}proposedLeader{_}, {_}proposedZxid{_}, and _proposedEpoch_ (which are modeled into _currentVote_ variable in the specification). The quorum rule is that when there are votes from a majority of the nodes in the _recvset_ that match its own vote ({_}proposedLeader{_}, {_}proposedZxid{_}, and _proposedEpoch_) , the node considers the quorum rule is satisfied and moves to the next stage of the election. Related codes: {code:java} // Line 761-780 in FastLeaderElection.java protected SyncedLearnerTracker getVoteTracker(Map<Long, Vote> votes, Vote vote) { .... for (Map.Entry<Long, Vote> entry : votes.entrySet()) { if (vote.equals(entry.getValue())) { voteSet.addAck(entry.getKey()); } } return voteSet; } // Line 1045 in FastLeaderElection.java voteSet = getVoteTracker(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch)); {code} In {*}specification{*}, according to the action _FLEHandleNotmsg_ and _HandleNotmsg_, the node itself is initially included in the set of nodes that have reached consensus. Then, the specification checks whether the votes from other nodes in the _rreceiveVotes_ match the node's _currentVote_. If the number of nodes whose votes match the node's _currentVote_ reaches a majority with the node itself, the quorum is considered reached. Related codes: {code:java} \\* Line 216-222 in FastLeaderElection.tla. VoteSet(i, msource, rcvset, thisvote, thisround) == \\{msource} \\union \\{s \\in (Server \\ {msource}): VoteEqual(rcvset[s].vote, rcvset[s].round, thisvote, thisround)} HasQuorums(i, msource, rcvset, thisvote, thisround) == LET Q == VoteSet(i, msource, rcvset, thisvote, thisround) IN IF Q \\in Quorums THEN TRUE ELSE FALSE \\* Line 433 in HandleNotmsg action of FastLeaderElection.tla. /\\ LET hasQuorums == HasQuorums(i, i, receiveVotes'[i], currentVote'[i], n.mround) {code} We can see that these two mechanisms are not entirely consistent. Consider the following three-node scenario: 1. The cluster starts, and at this point, node 1's _recvset_ contains votes from nodes 1, 2, and 3, all of which are initial votes (denoted as `InitialVote`). 2. Node 3 sends a _Notification-1_ (leader=N3, zxid=1, electionEpoch=1) to nodes 1, 2, and 3. 3. Node 1 receives _Notification-1_ from node 3. 4. Node 1 updates its vote to _Notification-1_ and sends _Notification-1_ to nodes 1, 2, and 3 again. 5. Node 1 updates its _recvset_. At this point, node 1's `recvset` is \\{N1: _InitialVote_, N2: _InitialVote_, N3: _Notification-1_}. 6. According to the specification, node 1 now believes that quorom nodes have reached consensus. However, according to the implementation, node 1 needs to receive the _Notification-1_ sent by itself in step 4 and update its _recvset_ (i.e., node 1's vote in the `recvset` must also be set to _Notification-1_) before it can consider the quorum reached. This figure \"Inconsistency in quorom rule.png\" in attachment shows the system state after step 5 more detailedly. I noticed that in the current version of the TLA+ specification, developers are attempting to avoid modeling the process of a node sending a message to itself and processing its own message. However, this simplification seems to lead to: 1. Inconsistencies between key algorithms in the specification and the implementation. In the example above, the inconsistent quorum rule caused me some confusion when I read the code. 2. Omission of some test scenarios. In the example above, There are some possible concurrent behaviors in the cluster after step 5 but before node 1 receives its own `Notification` and updates its `recvset` in step 6. These concurrent behaviors cannot be verified with the current version of the specification. I believe this issue can be fixed with the following two possible solutions: 1. *Simpler solution*: In the _HandleNotmsg\\(i\\)_ action of the specification, update _receiveVotes[i]_ after performing `UpdateProposal\\(i\\)`. Then, modify the _HasQuorums_ action to match the implementation's quorum rule. 2. *Complete solution*: Modify the _BroadcastNotmsg_ action in the specification to include _Notification_ messages sent to itself in the _recvqueue_, as is actually in the implementation (currently, the specification directly ignores such messages). Also, improve the handling logic for these types of messages in _FLEReceiveNotmsg_ and _FLEHandleNotmsg_ actions. Finally, modify the _HasQuorums_ action to match the implementation's quorum rule. If this issue is confirmed, I would be happy to provide a PR.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4891", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4891"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update logback to 1.3.15 to fix CVE-2024-12798.\nDescription: This is addressed in Logback 1.3.15: [https://nvd.nist.gov/vuln/detail/CVE-2024-12798]\nComments: - I have committed this to master.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4890", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4890"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update Netty to fix CVE-2024-47535\nDescription: Please, upgrade Jetty version to 4.1.115.Final in order to fix CVE-2024-47535. Pull request: [https://github.com/apache/zookeeper/pull/2212]\nComments: No comments.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4889", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4889"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fallback to DIGEST-MD5 auth mech should be disabled in Fips mode\nDescription: FIPS doesn't allow using MD5 algorithm, so it should be disabled at all times. When we create SASL client there's a fallback code path: if Kerberos doesn't work for some reason, we try to use DIGEST-MD5 mech instead. We already have a fips-mode property, so let's disable this code patch if the property is enabled.\nComments: - Issue resolved by pull request 2215 [https://github.com/apache/zookeeper/pull/2215]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4888", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4888"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Issues with TLS post upgrade from 3.9.1 to 3.9.2\nDescription: We upgraded Zookeeper ensemble from 3.9.1 to 3.9.2. TLS (node-node, client-node) is enabled before upgrade. Everything was working fine before upgrade. Post upgrade -> # Stopped everything (all ZK nodes) # Started all ZK nodes # Checked if SSL is happening between ZK nodes is fine or not # Its confirmed that SSL is working fine between ZK nodes. # Now started just one instance of client application # Post that we see intermittent successful & unsuccessful handshake messages in ZK logs. *ZK server side, we see below messages:* 2024-11-21 13:28:15,586 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.c.X509Util@599] - FIPS mode is ON: selecting standard x509 trust manager com.ibm.jsse2.br@4362299c 2024-11-21 13:28:15,586 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.c.X509Util@644] - Using Java8 optimized cipher suites for Java version 1.8 2024-11-21 13:28:15,588 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxnFactory@596] - SSL handler added for channel: [id: 0x2443db1c, L:/10.1.10.50:2181 - R:/10.1.10.46:57272] 2024-11-21 13:28:15,620 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxnFactory$CertificateVerifier@415] - *Successful handshake with session 0x0* 2024-11-21 13:28:15,620 [myid:] - DEBUG [epollEventLoopGroup-4-9:i.n.h.s.SslHandler@1934] - [id: 0x2443db1c, L:/10.1.10.50:2181 - R:/10.1.10.46:57272] HANDSHAKEN: protocol:TLSv1.3 cipher suite:TLS_AES_256_GCM_SHA384 2024-11-21 13:28:15,622 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxnFactory$CnxnChannelHandler@350] - New message PooledUnsafeDirectByteBuf(ridx: 0, widx: 4, cap: 42) from [id: 0x2443db1c, L:/10.1.10.50:2181 - R:/10.1.10.46:57272] 2024-11-21 13:28:15,622 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@368] - 0x0 queuedBuffer: null 2024-11-21 13:28:15,622 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@386] - not throttled 2024-11-21 13:28:15,623 [myid:] - INFO [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@311] - Processing mntr command from /10.1.10.46:57272 2024-11-21 13:28:15,642 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@113] - close called for session id: 0x0 2024-11-21 13:28:15,642 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@131] - close in progress for session id: 0x0 2024-11-21 13:28:15,644 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@113] - close called for session id: 0x0 2024-11-21 13:28:15,644 [myid:] - DEBUG [epollEventLoopGroup-4-9:o.a.z.s.NettyServerCnxn@124] - cnxns size:0 2024-11-21 13:28:17,155 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.c.X509Util@599] - FIPS mode is ON: selecting standard x509 trust manager com.ibm.jsse2.br@a5cca67c 2024-11-21 13:28:17,156 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.c.X509Util@644] - Using Java8 optimized cipher suites for Java version 1.8 2024-11-21 13:28:17,158 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.s.NettyServerCnxnFactory@596] - SSL handler added for channel: [id: 0xb818882d, L:/10.1.10.50:2181 - R:/10.1.10.46:57276] 2024-11-21 13:28:17,161 [myid:] - ERROR [epollEventLoopGroup-4-10:o.a.z.s.NettyServerCnxnFactory$CertificateVerifier@466] - *Unsuccessful handshake with session 0x0* 2024-11-21 13:28:17,161 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.s.NettyServerCnxn@113] - close called for session id: 0x0 2024-11-21 13:28:17,162 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.s.NettyServerCnxn@124] - cnxns size:0 2024-11-21 13:28:17,163 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.s.NettyServerCnxn@113] - close called for session id: 0x0 2024-11-21 13:28:17,163 [myid:] - DEBUG [epollEventLoopGroup-4-10:o.a.z.s.NettyServerCnxn@124] - cnxns size:0 *At client side, we see below message intermittently.* 17:37:43.878 [pool-7-thread-1-SendThread(10.1.10.50:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for server bdc-dev1807.in.syncsort.dev/10.1.10.50:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException. org.apache.zookeeper.ClientCnxn$EndOfStreamException: channel for sessionid 0x0 is lost at org.apache.zookeeper.ClientCnxnSocketNetty.doTransport(ClientCnxnSocketNetty.java:287) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1274) *We also see successful SSL connections from client side as well* INFO: Connected via SSL to server : 10.1.10.50 @ port : 2181 Nov 21, 2024 5:46:04 PM com.ibm.mailbox.zkwatchdog.ZKCommandClient connect INFO: Connected via SSL to server : 10.1.10.46 @ port : 2181 Nov 21, 2024 5:46:09 PM com.ibm.mailbox.zkwatchdog.ZKCommandClient connect INFO: Connected via SSL to server : 10.1.10.46 @ port : 2181 Nov 21, 2024 5:46:09 PM com.ibm.mailbox.zkwatchdog.ZKCommandClient connect INFO: Connected via SSL to server : 10.1.10.50 @ port : 2181 Nov 21, 2024 5:46:14 PM com.ibm.mailbox.zkwatchdog.ZKCommandClient connect INFO: Connected via SSL to server : 10.1.10.46 @ port : 2181 Nov 21, 2024 5:46:14 PM com.ibm.mailbox.zkwatchdog.ZKCommandClient connect INFO: Connected via SSL to server : 10.1.10.50 @ port : 2181 *We have not set any TLS protocol version or Ciphers at client or server side.* *We are using IBM JDK 8.* Please help troubleshoot this issue\nComments: - We have one more observation. Configured client to use TLS 1.3 and no issues observed. zkclient.ssl.protocol=TLSv1.3 zkclient.ssl.ciphers=TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_AES_128_GCM_SHA256 No changes made at ZK server side. This seems a behavior change from ZK 3.9.1 to 3.9.2 (In our environment, we used same IBM JDK 8 for 3.9.1 and 3.9.2). With 3.9.1, it was working without any of the changes. With 3.9.2, does it only allows only TLS 1.3 out of the box and need to be configured explicitly to use other TLS versions? - Please find the attached zookeeper.out logs. There is no error printing around these logs. The unsuccessful handshake doesn't print any error logs around it. Unsuccessful handshake error is coming intermittently. At line #130135 : you can see the Unsuccessful handshake At line #130143 : You can see a successful handshake. What's the client's version? -> We are using the same zookeeper version at the client side (3.9.2). We didn't change the configuration in the zoo.cfg file during upgrade. You can find the below logs as well in the log file attached to this email at line #7 . 2024-11-28 13:59:45,908 [myid:] - INFO [main:o.a.z.c.X509Util@110] - Default TLS protocol is TLSv1.3, supported TLS protocols are [TLSv1.3, TLSv1.2, TLSv1.1, TLSv1, SSLv2Hello] - [~andor] I work with Aayush and Jeetendra. I investigated this a bit more and have some ideas on the cause of this issue. The IBM JDK uses different cipher names internally compared to other JDK implementations. You can see that information here [https://www.ibm.com/docs/en/sdk-java-technology/8?topic=suites-cipher] Specifically some cipher names start with SSL_ instead of TLS_. If you call SSLServerSocketFactory.getDefault()).getSupportedCipherSuites(), these ciphers will be returned with SSL_. Why is this a problem for ZooKeeper? In X509Util, there are several methods that return lists of ciphers that ZK wants to use (getTLSv13Ciphers, getGCMCiphers, getCBCCiphers). These ciphers start with TLS_. Later in the getSupportedCiphers method, ZK filters out unsupported ciphers by comparing the hardcoded lists to what the JDK's SSLServerSocketFactory.getDefault()).getSupportedCipherSuites() method returns. For the IBM JDK, there's little overlap due to the naming differences, and this results in no TLS 1.2 ciphers in the list. This makes it impossible for any clients to connect with TLS 1.2. The solution is to add the SSL_ ciphers in the getTLSv13Ciphers (maybe not needed), getGCMCiphers, getCBCCiphers methods, to properly support the IBM JDK. Or an even better would be to not hardcode any ciphers. This will be helpful for longer term maintenance of this code (new ciphers added, new TLS versions added, etc).", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4887", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4887"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper quorum formation fails when TLS is enabled in k8s env\nDescription: We have three(3) node zookeeper cluster running as a pod on Kubernetes cluster, zookeeper quorum formation fails with TLS handshake error, as the server name in the https request does not match with any of the SANs in the certificate configured for zookeeper server. Server name in the request is of the form \"x-x-x-x.kubernetes.default.svc.cluster.local\" (where x-x-x-x is the IP address of the POD), and I am unable to understand the reason behind pre-pending FQDN with a IP address. Please find below the extract of the error logs from the zookeeper POD {code:java} [myid:] - ERROR [LearnerHandler-/192.168.220.10:46516:o.a.z.c.ZKTrustManager@191] - Failed to verify host address: 192.168.220.10 javax.net.ssl.SSLPeerUnverifiedException: Certificate for <192.168.220.10> doesn't match any of the subject alternative names: [eric-data-coordinator-zk, eric-data-coordinator-zk.zdhagxx1, eric-data-coordinator-zk.zdhagxx1.svc, eric-data-coordinator-zk.zdhagxx1.svc.cluster.local, *.eric-data-coordinator-zk-ensemble-service.zdhagxx1.svc.cluster.local, certified-scrape-target] org.apache.zookeeper.common.ZKHostnameVerifier.matchIPAddress(ZKHostnameVerifier.java:197) org.apache.zookeeper.common.ZKHostnameVerifier.verify(ZKHostnameVerifier.java:165) org.apache.zookeeper.common.ZKTrustManager.performHostVerification(ZKTrustManager.java:180) org.apache.zookeeper.common.ZKTrustManager.checkClientTrusted(ZKTrustManager.java:93) java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkClientCerts(CertificateMessage.java:1285) java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1204) java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1181) java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392) java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443) java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:421) java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:183) java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1511) java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1421) java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:456) java.base/sun.security.ssl.SSLSocketImpl.ensureNegotiated(SSLSocketImpl.java:926) java.base/sun.security.ssl.SSLSocketImpl.getSession(SSLSocketImpl.java:372) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedSocket.detectMode(UnifiedServerSocket.java:269) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedSocket.getSocket(UnifiedServerSocket.java:298) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedSocket.access$400(UnifiedServerSocket.java:172) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedInputStream.getRealInputStream(UnifiedServerSocket.java:699) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedInputStream.read(UnifiedServerSocket.java:693) java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252) java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271) java.base/java.io.DataInputStream.readInt(DataInputStream.java:392) org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96) org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:86) org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:134) org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:472)[myid:] - ERROR [LearnerHandler-/192.168.220.10:46516:o.a.z.c.ZKTrustManager@192] - Failed to verify hostname: 192-168-220-10.eric-data-coordinator-zk.zdhagxx1.svc.cluster.local javax.net.ssl.SSLPeerUnverifiedException: Certificate for <192-168-220-10.eric-data-coordinator-zk.zdhagxx1.svc.cluster.local> doesn't match any of the subject alternative names: [eric-data-coordinator-zk, eric-data-coordinator-zk.zdhagxx1, eric-data-coordinator-zk.zdhagxx1.svc, eric-data-coordinator-zk.zdhagxx1.svc.cluster.local, *.eric-data-coordinator-zk-ensemble-service.zdhagxx1.svc.cluster.local, certified-scrape-target] org.apache.zookeeper.common.ZKHostnameVerifier.matchDNSName(ZKHostnameVerifier.java:230) org.apache.zookeeper.common.ZKHostnameVerifier.verify(ZKHostnameVerifier.java:171) org.apache.zookeeper.common.ZKTrustManager.performHostVerification(ZKTrustManager.java:189) org.apache.zookeeper.common.ZKTrustManager.checkClientTrusted(ZKTrustManager.java:93) java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkClientCerts(CertificateMessage.java:1285) java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1204) java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1181) java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392) java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443) java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:421) java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:183) java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1511) java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1421) java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:456) java.base/sun.security.ssl.SSLSocketImpl.ensureNegotiated(SSLSocketImpl.java:926) java.base/sun.security.ssl.SSLSocketImpl.getSession(SSLSocketImpl.java:372) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedSocket.detectMode(UnifiedServerSocket.java:269) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedSocket.getSocket(UnifiedServerSocket.java:298) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedSocket.access$400(UnifiedServerSocket.java:172) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedInputStream.getRealInputStream(UnifiedServerSocket.java:699) org.apache.zookeeper.server.quorum.UnifiedServerSocket$UnifiedInputStream.read(UnifiedServerSocket.java:693) java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252) java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271) java.base/java.io.DataInputStream.readInt(DataInputStream.java:392) org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96) org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:86) org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:134) org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:472) {code}\nComments: - You are probably running into ZOOKEEPER-4790 here. When we encountered this [back in the day|https://github.com/stackabletech/zookeeper-operator/issues/760] we figured out that enabling FIPS mode bypasses all the ZK specific TLS checks and makes it work. In the ZK version you are on it is not yet enabled by default, you could either update or set zookeeper.fips-mode and this error _should_ go away. - Just noticed that this is just copy paste from ZOOKEEPER-4536 I think this one can be closed as duplicate. - Hi [~sliebau] , We are using Apache Zookeeper version 3.8.3. If we change the default configuration of fips-mode to true, will there be any other impacts? Thanks, Dharani", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4886", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4886"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: observer with small myid can't join SASL quorum\nDescription: When SASL Quorum like: server.11=localhost:11223:11224:participant server.21=localhost:11226:11227:participant server.1=localhost:11229:11230:observer The server.1 can't join quorum.\nComments: - Issue resolved by pull request 2211 [https://github.com/apache/zookeeper/pull/2211]", "output": "Status: Closed\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4885", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4885"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Can Non-SASL-Clients automatically recover with the recovery of kerberos communication\uff1f\nDescription: About ZOOKEEPER-2139 & ZOOKEEPER-2323, it just avoids ZooKeeper clients into infinite AuthFailedException. Noauth Exception still exists! LoginException was thrown through each login, but at this point, a zkclient without Kerberos SASL authentication was created. Non SASL Znodes can be operated on in the future. However, when Kerberos recovers from network disconnections and other anomalies, the previously created zkclient without SASL authentication is still being used without rebuilding the login or recreating a saslclient. If it is used to operate on ACL Znodes at this time, an error will always be reported: {code:java} KeeperErrorCode = NoAuth for /zookeeper or KeeperErrorCode = AuthFailed for /zookeeper or KeeperErrorCode = InvalidACL for /zookeeper{code} Isn't this a question that should be considered? And I also met this issue in ZK-3.6.4\uff0cIt seems that this issue has not been considered in the updated version.\nComments: - LoginException was thrown through each login, about code\uff1a {code:java} if (ZooKeeperSaslClient.isEnabled()) { try { if (zooKeeperSaslClient != null) { zooKeeperSaslClient.shutdown(); } zooKeeperSaslClient = new ZooKeeperSaslClient(SaslServerPrincipal.getServerPrincipal(addr)); } catch (LoginException e) { // An authentication error occurred when the SASL client tried to initialize: // for Kerberos this means that the client failed to authenticate with the KDC. // This is different from an authentication error that occurs during communication // with the Zookeeper server, which is handled below. LOG.warn(\"SASL configuration failed: \" + e + \" Will continue connection to Zookeeper server without \" + \"SASL authentication, if Zookeeper server allows it.\"); eventThread.queueEvent(new WatchedEvent( Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed, null)); saslLoginFailed = true; } } {code} Clients enter AuthFailed state from the following code\uff1a {code:java} if (zooKeeperSaslClient != null) { boolean sendAuthEvent = false; if (zooKeeperSaslClient.getSaslState() == ZooKeeperSaslClient.SaslState.INITIAL) { try { zooKeeperSaslClient.initialize(ClientCnxn.this); } catch (SaslException e) { LOG.error(\"SASL authentication with Zookeeper Quorum member failed: \" + e); state = States.AUTH_FAILED; sendAuthEvent = true; } } KeeperState authState = zooKeeperSaslClient.getKeeperState(); if (authState != null) { if (authState == KeeperState.AuthFailed) { // An authentication error occurred during authentication with the Zookeeper Server. state = States.AUTH_FAILED; sendAuthEvent = true; } else { if (authState == KeeperState.SaslAuthenticated) { sendAuthEvent = true; } } } if (sendAuthEvent == true) { eventThread.queueEvent(new WatchedEvent( Watcher.Event.EventType.None, authState,null)); if (state == States.AUTH_FAILED) { eventThread.queueEventOfDeath(); } } }{code} I want to know if the community thinks that this kind of issue needs to be considered by client-users, and the server only needs to ensure that in the event of a failure to authenticate with kerberos, a client that can operate on no sasl znode can be created normally. Is that correct\uff1fThis problem is equivalent to another scenario: *When creating a zkClient, the Kerberos is no longer online, and the created client can only operate on the Znode of no SASL. After the subsequent recovery of the Kerberos, it still remains the same, and cannot operate on the Znode of SASL, nor does it trigger the reconstruction of the client.* Isn't this a question that should be considered? And I also met this issue in ZK-3.6.4\uff0cIt seems that this issue has not been considered in the updated version. - 1. So {*}there is a real scenario in the production environment{*}: When a *Curator Client* used to create a EphemeralNode\uff0clike code in HiveServer2\uff1a {code:java} zooKeeperClient = CuratorFrameworkFactory.builder().connectString(zooKeeperEnsemble).sessionTimeoutMs(sessionTimeout).aclProvider(zooKeeperAclProvider).retryPolicy(new ExponentialBackoffRetry(baseSleepTime, maxRetries)).build(); zooKeeperClient.start(); PersistentEphemeralNode znode = new PersistentEphemeralNode(zooKeeperClient, PersistentEphemeralNode.Mode.EPHEMERAL_SEQUENTIAL, pathPrefix, znodeDataUTF8); znode.start();{code} There is a callback function in PersistentEphemeralNode\uff1a {code:java} backgroundCallback = new BackgroundCallback() { @Override public void processResult(CuratorFramework client, CuratorEvent event) throws Exception { String path = null; if ( event.getResultCode() == KeeperException.Code.NODEEXISTS.intValue() ) { path = event.getPath(); } else if ( event.getResultCode() == KeeperException.Code.OK.intValue() ) { path = event.getName(); } if ( path != null ) { nodePath.set(path); watchNode(); CountDownLatch localLatch = initialCreateLatch.getAndSet(null); if ( localLatch != null ) { localLatch.countDown(); } } else { createNode(); } } }; // createNode() registers this backgroundCallback again\uff1a createMethod.withMode(mode.getCreateMode(existingPath != null)).inBackground(backgroundCallback).forPath(createPath, data.get());{code} When HiveServer2 is disconnected from the Zookeeper and then network reconnected, Curator automatically rebuilds the client. Coincidentally, the login to kerberos fails at this time, and a non SASL authenticated client is created. The Curator uses this client to rebuild PersistentEphemeralNode under \u201c/hive\u201d. There will be an error message in creating: {code:java} KeeperErrorCode = InvalidACL for /hive/hiveserver2-0... {code} and only the judgment for NODEEXISTS and OK will be made in {*}backgroundCallback#processResult{*}. In this case, it will received InvaldACL as 'event.getResultCode()' and repeatedly call creatNode() and register backgroundcallback. After the error callback, it will call creatNode() and register backgroundcallback again. So it enters a dead loop, due to the callback processing within the Curator, the zk client of hiveserver2 is completely unaware, while the server will frantically brush InvalidACL logs due to the fast call of creatNode(). The CPU pressure on the zk server will also increase dramatically. *From here, it can be seen that neither the Curator nor Zookeeper wants to automatically rebuild the client for scenarios where Kerberos ACL authentication fails.* If we don't modify the Curator code, a solution is proposed for this scenario, which involves monitoring the event on the Hiveserver2 side. When the zk client sends an Authfailed event, the current Curator client is closed and rebuilt, and the ephemeral node is recreated until it is successfully created. However, there is another issue with this modification. When the authfailed event occurs, rebuilding the Curator client. If the zk server is disconnected for a period of time and the reconnection may generates an expire event, the expire event will occur after the authfailed event and trigger automatic reconstruction within the Curator. At this point, there will be an additional connection between hiveserver2 and the zk server, as well as an additional zk client, which is considered a {*}client leak{*}. {code:java} // Expired event trigger Curator to Reset() org.apache.curator.ConnectionState#checkState case Expired: { isConnected = false; checkNewConnectionString = false; handleExpiredSession(); break; } org.apache.curator.ConnectionState#handleExpiredSession private void handleExpiredSession() { log.warn(\"Session expired event received\"); tracer.get().addCount(\"session-expired\", 1); try { reset(); } catch ( Exception e ) { queueBackgroundException(e); } } //========== reset() will rebuild Zookeeper Client ================= org.apache.curator.ConnectionState#reset private synchronized void reset() throws Exception { log.debug(\"reset\"); instanceIndex.incrementAndGet(); isConnected.set(false); connectionStartMs = System.currentTimeMillis(); zooKeeper.closeAndReset(); zooKeeper.getZooKeeper(); // initiate connection }{code} {*}In order to solve the problem of client leak in this scheme{*}, it is proposed to synchronously modify the Zookeeper client code(ClientCnxn.java). The purpose of the modification is to avoid generating expired events after authfailed events, so that the Curator side will not trigger handleExpirdSession() and reset(). This is customized for some clients that need to set SASL permission znode. The client process needs to carry an environment variable (newly added)\uff1a {code:java} zookeeper.sasl.kerberos.client=true{code} When creating such a client, after the authfailed event generated during login, Zookeeper. state can be set to AuthFailed, so exit the retry connection with the zk server, and close the event thread in the same time. In this way, because the zk client has disconnected from the server internally, the expired event will not be triggered again. We can actively initiate the reconstruction of the client and znode in HiveServer2, the leakage problem should not occur again. The modification is as follows: {code:java} // org.apache.zookeeper.ClientCnxn.SendThread#startConnect private void startConnect(InetSocketAddress addr) throws IOException { // initializing it for new connection saslLoginFailed = false; state = States.CONNECTING; setName(getName().replaceAll(\"\\\\(.*\\\\)\", \"(\" + addr.getHostName() + \":\" + addr.getPort() + \")\")); if (ZooKeeperSaslClient.isEnabled()) { try { if (zooKeeperSaslClient != null) { zooKeeperSaslClient.shutdown(); } zooKeeperSaslClient = new ZooKeeperSaslClient(SaslServerPrincipal.getServerPrincipal(addr)); } catch (LoginException e) { // An authentication error occurred when the SASL client tried to initialize: // for Kerberos this means that the client failed to authenticate with the KDC. // This is different from an authentication error that occurs during communication // with the Zookeeper server, which is handled below. eventThread.queueEvent(new WatchedEvent( Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed, null)); saslLoginFailed = true; // =====================modification========================== if (ZooKeeperSaslClient.isKerberosBind()) { state = States.AUTH_FAILED; LOG.warn(\"Kerberos is bind but LoginException occurs, Zookeeper client will go to AuthFailed state \" + \"and will not continue connection to Zookeeper server.\"); throw new SaslException(e.getMessage()); } else { LOG.warn(\"SASL configuration failed: \" + e + \" Will continue connection to Zookeeper server without \" + \"SASL authentication, if Zookeeper server allows it.\"); } } } logStartConnect(addr); clientCnxnSocket.connect(addr); } // org.apache.zookeeper.ClientCnxn.SendThread#run if (state.isConnected()) { // determine whether we need to send an AuthFailed event. if (zooKeeperSaslClient != null) { boolean sendAuthEvent = false; if (zooKeeperSaslClient.getSaslState() == ZooKeeperSaslClient.SaslState.INITIAL) { try { zooKeeperSaslClient.initialize(ClientCnxn.this); } catch (SaslException e) { LOG.error(\"SASL authentication with Zookeeper Quorum member failed: \" + e); state = States.AUTH_FAILED; sendAuthEvent = true; } } KeeperState authState = zooKeeperSaslClient.getKeeperState(); if (authState != null) { if (authState == KeeperState.AuthFailed) { // An authentication error occurred during authentication with the Zookeeper Server. state = States.AUTH_FAILED; sendAuthEvent = true; } else { if (authState == KeeperState.SaslAuthenticated) { sendAuthEvent = true; } } } if (sendAuthEvent == true) { eventThread.queueEvent(new WatchedEvent( Watcher.Event.EventType.None, authState,null)); if (state == States.AUTH_FAILED) { eventThread.queueEventOfDeath(); // =====================modification========================== if (ZooKeeperSaslClient.isKerberosBind()) { LOG.warn(\"Kerberos is bind but auth failed, Zookeeper client will go to AuthFailed state \" + \"and will not continue connection to Zookeeper server.\"); throw new SaslException(\"zooKeeperSaslClient.State is AuthFailed.\"); } } } } to = readTimeout - clientCnxnSocket.getIdleRecv(); } // org.apache.zookeeper.client.ZooKeeperSaslClient // =====================modification========================== public static final String ENABLE_CLIENT_KERBEROS_BIND_KEY = \"zookeeper.sasl.kerberos.client\"; public static final String ENABLE_CLIENT_KERBEROS_BIND_DEFAULT = \"false\"; public static boolean isKerberosBind() { return Boolean.valueOf(System.getProperty(ENABLE_CLIENT_KERBEROS_BIND_KEY, ENABLE_CLIENT_KERBEROS_BIND_DEFAULT)); }{code} - 2. Also, {*}there is another real scenario in the production environment{*}: Another zk client of Hive is responsible for establishing persistent nodes. Due to abnormal Kereros interaction during creation, a non SASL authenticated Zookeeper client was obtained, It may report an error when creating SASL Znode\uff1a {code:java} org.apache.zookeeper.KeeperException.NoAuthException org.apache.zookeeper.KeeperException.InvalidACLException org.apache.zookeeper.KeeperException.AuthFailedException{code} Similarly, after the recovery of kerberos, using this client to continuously create znodes also results in continuous error messages. So the solution provided is to *consider actively rebuilding a client in the user code every time these three exceptions are encountered.* {code:java} // user code demo try { createZNode(zookeeperClient, c); } catch (Exception e) { e.printStackTrace(); c++; if (e instanceof org.apache.zookeeper.KeeperException.AuthFailedException || e instanceof org.apache.zookeeper.KeeperException.NoAuthException || e instanceof org.apache.zookeeper.KeeperException.InvalidACLException) { System.out.println(\"Warn: zkclient need construct, state: \" + zooKeeper.getState() + \" zkException: \" + e.getClass()); zooKeeperClient.close(); Thread.sleep(5000); // Proactively rebuild client objects zooKeeper = new ZooKeeper(\"localhost:2181\", 120000, new ZkClientKerberos()); } else { throw e; // Other exceptions are thrown directly }{code}", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4884", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4884"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: FastLeaderElection WorkerSender/WorkerReceiver don't need to be Thread\nDescription: ZOOKEEPER-1810 replaced them with dedicated threads.\nComments: - This is superseded by ZOOKEEPER-4898 and fixed at master via https://github.com/apache/zookeeper/pull/2228.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4883", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4883"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Rollover leader epoch when counter part of zxid reach limit\nDescription: Currently, zxid rollover will cause re-election(ZOOKEEPER-1277) which is time consuming. ZOOKEEPER-2789 proposes to use 24 bits for epoch and 40 bits for counter. I do think it is promising as [it promotes rollover rate from 49.7 days to 34.9 years assuming 1k/s ops|https://github.com/apache/zookeeper/pull/2164#issuecomment-2368107479]. But I think it is a one-way ticket. And the change of data format may require community wide spread to upgrade third party libraries/tools if they are ever tied to this. Inside ZooKeeper, `accepetedEpoch` and `currentEpoch` are tied to `zxid`. Given a snapshot and a txn log, we need probably deduced those two epoch values to join quorum. So, I presents alternative solution to rollover leader epoch when counter part of zxid reach limit. # Treats last proposal of an epoch as rollover proposal. # Requests from next epoch are proposed normally. # Fences next epoch once rollover proposal persisted. # Proposals from next epoch will not be written to disk before rollover committed. # Leader commits rollover proposal once it get quorum ACKs. # Blocked new epoch proposals are logged once rollover proposal is committed in corresponding nodes. This results in: # No other lead cloud lead using next epoch number once rollover proposal is considered committed. # No proposals from next epoch will be written to disk before rollover proposal is considered committed. Here is the branch, I will draft a pr later. https://github.com/kezhuw/zookeeper/tree/zxid-rollover\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4882", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4882"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Data loss after restarting an node experienced temporary disk error and rejoin\nDescription: The cause is multifold: 1. Leader will commit a proposal once quorum acked. 2. Proposal is able to be committed in node's memory even if it has not been written to that node's disk. 3. In case of disk error, the txn log could lag behind memory database. The above applies to both leader and follower. I have not verified leader branch, let's consider only follower for now. f4. A follower experienced temporary disk error will have hole in txn log after re-join. f5. Restarted follower will lose the data. Worse, it is able to win election and propagate data loss to whole cluster. I authored commits in my repo to expose this. https://github.com/kezhuw/zookeeper/commits/data-loss-temporary-sync-disk-error/\nComments: - The leader branch might be a little complicated as it take a clean snapshot before leading. But once it is restarted, it has no much difference to the follower case. - {quote} But once it is restarted, it has no much difference to the follower case. {quote} This is no true. The snapshot has no data loss. But once it got elected, it could propagate data loss in txn log through DIFF sync. We need test to verify this.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4881", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4881"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support non blocking ZooKeeper::close\nDescription: {{ZooKeeper::close}} is synchronous, it waits until {{OpCode.closeSession}} returned. I think it would be useful to support background closing to avoid block caller, just like `close(2)` for socket. We probably could use {{ZooKeeperBuilder}} from ZOOKEEPER-4697 to manually enable it. This way we would not surprise anyone just like `SO_LINGER` for socket.\nComments: - {{ZooKeeper::closeAsync()}}(or whatever name) should also sound. The good of an addition constructor level option is that it play well with {{AutoCloseable}}. I am thinking whether we should provide both. - I believe synchronous {{Zookeeper::close}} would be helpful in certain cases, such as tests, to ensure that session get closed after return.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4880", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4880"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Generate comments from zookeeper.jute into code\nDescription: When I was learning protobuf, I found that the comments in the proto file would be generated into the code. I think this feature is very useful. I wonder if this feature can be added to zookeeper, because in the process of learning zookeeper, we can directly see the attribute comments of the relevant classes generated by jute, and there is no need to check the meaning of the fields in zookeeper.jute. Although I found that there are not many comments in the zookeeper.jute file, I think it would be helpful to generate the comments into the code. And I studied javacc and found that javacc supports this feature. We can get the comments in zookeeper.jute through SPECIAL_TOKEN. If possible, I hope to contribute a PR.\nComments: - +1 Look forward to this. - Issue resolved by pull request 2206 [https://github.com/apache/zookeeper/pull/2206]", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4879", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4879"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix log arguments typo\nDescription: fix log arguments typo, exception stack does not need placeholder.\nComments: - master: 5664ce11c98cfdd9348b2e32f74307582b7a1dda", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4878", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4878"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper servers not running after Chaos mesh IO fault experiment\nDescription: We are running zookeeper in kubernetes as stateful set with 3 replicas. when we performed chaos mesh IO fault experiment using [^IO_Fault.yaml], zookeeper servers are not recovering. Zookeeper config file: [^zoo.cfg] {code:java} 2024-10-24T09:43:40.896+0000 [myid:] - ERROR [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):o.a.z.s.ZooKeeperServer@552] - Severe unrecoverable error, exiting java.io.FileNotFoundException: /var/lib/zookeeper/data/version-2/snapshot.1100000859 (Input/output error) at java.base/java.io.FileOutputStream.open0(Native Method) at java.base/java.io.FileOutputStream.open(FileOutputStream.java:298) at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237) at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:187) at org.apache.zookeeper.server.persistence.SnapStream.getOutputStream(SnapStream.java:133) at org.apache.zookeeper.server.persistence.FileSnap.serialize(FileSnap.java:242) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.save(FileTxnSnapLog.java:481) at org.apache.zookeeper.server.ZooKeeperServer.takeSnapshot(ZooKeeperServer.java:550) at org.apache.zookeeper.server.ZooKeeperServer.takeSnapshot(ZooKeeperServer.java:544) at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:540) at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:597) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1552) 2024-10-24T09:43:40.898+0000 [myid:] - ERROR [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):o.a.z.u.ServiceUtils@48] - Exiting JVM with code 10 {code} Expectation: When IO_fault experiment using chaos mesh is performed for 60 sec (storage pause time), all the zookeeper servers should recover by itself without any manual intervention within the 10times the storage pause time. Is it possible to have partial traffic when PV is hanged?\nComments: - Do you have more logs so we can know the more context about how are you testing and how it fails ? Personally, I think \"unrecoverable\" means it demands human operation. If you are going fault IO operation to behave \"corrupted\", then \"unrecoverable\" is suitable. - Hi [~kezhuw] , I have attached the logs and the experiment configuration used while performing the fault IO operation using chaos mesh. After the fault IO operation on the PVC path, zookeeper servers are not recovering and unable to serve requests. Is there a way to mitigate it? - I saw the logs and found that all nodes are experiencing faulty storage. ZooKeeper is able to tolerate n faulty nodes of 2n+1 nodes. I suggest you not to sharing PVC for ZooKeeper instances, at least not majority(a.k.a. n+1 nodes) on one physical volume.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4877", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4877"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix typos\nDescription: Fix typos. 1. !image-2024-10-18-09-53-46-914.png! 2. !image-2024-10-18-09-54-06-318.png! 3. !image-2024-10-18-09-54-26-149.png! 4. !image-2024-10-18-09-54-49-783.png! 5. !image-2024-10-18-09-55-17-835.png!\nComments: - Issue resolved by pull request 2203 [https://github.com/apache/zookeeper/pull/2203]", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4876", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4876"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: jetty-http-9.4.53.v20231009.jar: CVE-2024-6763(3.7)\nDescription: \nComments: - I looked into this, and bumping the dependency to the latest release of Jetty advertised on [https://jetty.org/download.html], {{{}9.4.56.v20240826{}}}, apparently resolves a fresh {{{}CVE-2024-8184{}}}, but *not* {{{}CVE-2024-6763{}}}\u2014both having been published on 2024-10-14. Worse, still, this comment says: [https://github.com/jetty/jetty.project/pull/12012#issuecomment-2416450253] {quote}This will not be backported to an End of Community Support version of Jetty. {quote} On the other hand, it continues with: {quote}As stated in [CVE-2024-6763|https://github.com/advisories/GHSA-qh8g-58pp-2wxh], the use of Jetty server, or Jetty client, does not make you vulnerable to that CVE. {quote} The linked report itself explains: {quote}The impact of this vulnerability is limited to developers that use the Jetty HttpURI directly. Example: your project implemented a blocklist to block on some hosts based on HttpURI's handling of authority section. {quote} So: should we upgrade to 9.4.56 and suppress CVE-2024-6763? (Note that I haven't performed an analysis of the affected source code!) WDYT? - {quote}So: should we upgrade to 9.4.56 and suppress CVE-2024-6763? {quote} Yes, I think that's what we should do. I did a quick grep in source code for {{HttpURI}} and we don't use it, so I believe we can suppress that CVE. I bumped Jetty version and build is successful. Let me create a PR. - https://github.com/apache/zookeeper/pull/2202", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4875", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4875"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support pre-constructed ZKConfig in server side\nDescription: Currently, `ZKConfig` is only constructed right before its usage. It makes it hard to run multiple zookeeper servers in one JVM but with different configurations. Though, we don't officially claim support to that, but I think it should be a good to not have such a ban in our side. Also, accepting pre-constructed `ZKConfig` clould also benefit tests to not mess up properties between client and server. See also https://github.com/apache/zookeeper/pull/2200#discussion_r1800328858\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4874", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4874"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Avoid using fuzzy snapshots.\nDescription: Currently, ZooKeeper uses a fuzzy snapshot method that allows write requests to continue processing while taking a snapshot. However, this approach does not retain an instantaneous data view during the serialization snapshot, leading to some issues( ZOOKEEPER-4846, ZOOKEEPER-3145). There is a need to optimize this process in order to maintain a consistent and accurate data view., We can optimize the snapshot process by caching the change and applying it after the snapshot is finished to avoid a fuzzy snapshot.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4873", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4873"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: An exception occurring in the INITIAL state of ZooKeeper can result in resource leaks.\nDescription: Currently, org.apache.zookeeper.server.ZooKeeperServer#canShutdown only checks the *Running* and *Error* states of the ZooKeeperServer. If an exception occurs in the *INITIAL* state, org.apache.zookeeper.server.ZooKeeperServer#shutdown(boolean) may not work properly, potentially leading to a resource leak.\nComments: - !image-2024-10-11-22-59-59-190.png! If already shut down, canShutdown() should return false; otherwise, it should return true.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4872", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4872"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: SnapshotCommand should not perform fastForwardFromEdits\nDescription: \nComments: - To avoid the potential of getting into data inconsistent state, SnapshotCommand should just take a snapshot from the data tree, instead of applying the txns recorded in the txn log and then taking snapshot. - {code:java} LOG.error(\"{}(highestZxid) > {}(next log) for type {}\", highestZxid, hdr.getZxid(), hdr.getType()); {code} This is to fix the error that can happen when snapshot is taken while txns are processed and appended to the txn log. - master: 3e02328048a9c753624d44aba51ac70cab60619a branch-3.9: da1fc515611dfce7b846a4aef2b362f1b238a266", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4871", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4871"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: ZooKeeper python module (zkpython) is incompatible with Python 3.12\nDescription: I've tried to run zk-smoktest.py on latest Ubuntu 24.04 and with Python 3.12 and run into the following issue: {noformat} $ python zk-smoketest.py -h Traceback (most recent call last): File \"/home/andor/git/zk-smoketest/zk-smoketest.py\", line 22, in <module> import zkclient File \"/home/andor/git/zk-smoketest/zkclient.py\", line 17, in <module> import zookeeper, time, threading ImportError: /usr/local/lib/python3.12/dist-packages/ZooKeeper-0.4-py3.12-linux-x86_64.egg/zookeeper.cpython-312-x86_64-linux-gnu.so: undefined symbol: PyIOBase_Type {noformat} It turned out that *PyIOBase_Type* is deprecated and not available in Python 3.12. Found the following workaround in a different project which might be useful for *zkpython* too. [https://github.com/inspired-solutions/pygraphviz/pull/1/files]\nComments: - Issue resolved by pull request 2199 [https://github.com/apache/zookeeper/pull/2199]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4870", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4870"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Proactive leadership transfer\nDescription: We do have leadership transfer, but it only happen when we are removing leader in reconfiguration. It would be nice to support it with dedicated API. This way it will be really useful to reduce unavailability during rolling upgrade or leader shutdown. Also, I think it cloud also help zxid rollover. Inheriting leadership in rollover should be similar to leadership transfer in protocol. https://www.usenix.org/conference/atc12/technical-sessions/presentation/shraer {quote} we investigate the effect of reconfigurations removing the leader. Note that a server can never be added to a cluster as leader as we always prioritize the current leader. Figure 8 shows the advan- tage of designating a new leader when removing the cur- rent one, and thus avoiding leader election. It depicts the average time to recover from a leader crash versus the average time to regain system availability following the removal of the leader. The average is taken on 10 executions. We can see that designating a default leader saves up to 1sec, depending on the cluster size. As cluster size increases, leader election takes longer while using a default leader takes constant time regardless of the clus- ter size. Nevertheless, as the figure shows, cluster size always affects total leader recovery time, as it includes synchronizing state with a quorum of followers. {quote}\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4869", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4869"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update doc of Supporting for Multiple Time Units in autopurge.purgeInterval Configuration\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4868", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4868"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bump commons-io library to 2.14.0\nDescription: CVE-2024-47554 is fixed in that version of the library. Could please you confirm whether Zookeeper is affected by this vulnerability and if so, are there any plans to update the dependency? {code} Java (jar) ========== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Library \u2502 Vulnerability \u2502 Severity \u2502 Status \u2502 Installed Version \u2502 Fixed Version \u2502 Title \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 commons-io:commons-io (commons-io-2.11.0.jar) \u2502 CVE-2024-47554 \u2502 HIGH \u2502 fixed \u2502 2.11.0 \u2502 2.14.0 \u2502 apache-commons-io: Possible denial of service attack on \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 untrusted input to XmlStreamReader \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 https://avd.aquasec.com/nvd/cve-2024-47554 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 {code} h4. Steps to reproduce {code} trivy image zookeeper:3.9 {code}\nComments: - Issue resolved by pull request 2197 [https://github.com/apache/zookeeper/pull/2197] - FYI, ZK isn't affected by this CVE. But bump version to reduce friction is valuable so we bump it and make scanners happy. - Thank you for confirming the solution is not affected and for updating the dependency too.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4867", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4867"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Cannot use encrypted PEM certificate\nDescription: TLS can be enabled in ZooKeeper and configured using PEM formatted files. However, if the PEM file contains an encrypted EC (or RSA) private key, ZooKeeper is not able to find it within the PEM file. The {{PemReader.loadPrivateKey}} method's RegEx does not match such a key, for example: {code} -----BEGIN EC PRIVATE KEY----- Proc-Type: 4,ENCRYPTED DEK-Info: AES-256-CBC,e49 rdz -----END EC PRIVATE KEY----- {code} This appears to be because the RegEx does not allow {{-}} or {{,}} characters within the body of the private key. There may be other problems with using such keys beyond the RegEx matching.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4866", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4866"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Override `equals()` and `hashCode()` for `ClientCnxn.AuthData`\nDescription: In {{{}ClientCnxn.java{}}}, when managing authentication information, {{AuthData}} objects are added to {{{}authInfo{}}}, which is a set. However, the {{AuthData}} class does not override the {{equals()}} and {{hashCode()}} methods. This means that duplicate authentication data will not be automatically removed when added to the set. Why were {{equals()}} and {{hashCode()}} not overridden in the {{AuthData}} class? Is there a specific reason for this design decision?\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4865", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4865"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: User-level exception log cannot be output correctly.\nDescription: Currently, the condition check for the user-level exception log may be incorrect, and the exception log may not be output correctly. The error code is negative, so the exception log should be output when the error code is less than APIERROR. !image-2024-09-29-17-01-01-153.png! And shall we move the user-level exception log to FinalRequestProcessor? Because we can get more user-level exception there.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4864", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4864"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: TxnLogToolkit dump multi to unrecognized chars.\nDescription: !https://intranetproxy.alipay.com/skylark/lark/0/2024/png/17056839/1727342763883-20799ce6-7595-461b-bb77-bb49261cdee6.png?x-oss-process=image%2Fformat%2Cwebp!\nComments: - Currently, the TxnLogToolkit dumps the multi-txn using new String(txn.getData(),StandardCharsets.UTF_8), but the transaction bytes cannot be converted to a string directly by UTF-8 decoding. We should dump the transaction in a multi-transaction case-by-case manner. - I will submit a pr to resolve it. - Does this solved by ZOOKEEPER-4607 ? - It seems only error will be decoded. !image-2024-09-27-13-52-45-301.png! - After building zookeeper with the latest code, I found that the problem still persists with other transactions in multi-txn (such as checkVersion, setData, etc.). - Hi, [~kezhuw] I submit a pr to solve it, can you take a look? - master via https://github.com/apache/zookeeper/commit/b99714543db86ca081a5d648c565f0d9cb4c82c2", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4863", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4863"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support Client Error metrics\nDescription: Support client error statistics metrics may help users find exceptions in their applications. For example, using {{echo mntr | nc localhost 2181}} can produce the following output: ... client_error_node_exist 33 client_error_no_node 22 ... We can also add these metrics to Prometheus metrics.\nComments: - I will submit a pr to support it.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4862", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4862"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Doc for Secure prometheus support\nDescription: \nComments: - master: 1880fc0d0a092b2c4e518bc45d5c4e8a77ca4abc", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4861", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4861"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: TLS compatibility issue\nDescription: TLS is not established after the following warning message: {{2024-09-06 20:55:34,307 [myid:] - WARN [epollEventLoopGroup-4-1:o.a.z.s.NettyServerCnxnFactory$CnxnChannelHandler@302] - Exception }} {\\{caught }} {{{}io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: The client supported protocol versions [TLSv1.2] are n{}}}{{{}ot accepted by server preferences [TLS13] {{}}}} {\\{ at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:499) }} {\\{ at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290) }} {\\{ at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) }} {\\{ at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) }} {\\{ at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) }} {\\{ at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) }} {\\{ at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) }} {{ at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)}} During the startup, zookeeper logs: {{2024-09-06 20:54:49,778 [myid:] - INFO [main:o.a.z.c.X509Util@110] - Default TLS protocol is TLSv1.3, supported TLS protocols are [ }} {{TLSv1.3, TLSv1.2, TLSv1.1, TLSv1, SSLv3, SSLv2Hello] }} This was noticed on the following environment: * Zookeeper Client (3.9.0) - Zookeeper Server (3.9.1): When Zookeeper server is running with Java 21. Client java version does not matter. This works with Java 17. * Zookeeper Client (3.9.1) - Zookeeper Server (3.9.2): It happens for Java 17 and Java 21\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4860", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4860"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Disable X-Forwarded-For in IPAuthenticationProvider by default\nDescription: Disable *X-Forwarded-For* header check in *IPAuthenticationProvider* by default to improve reliability in client IP address detection. X-Forwarded-For is not a standard header, it's not required and not needed unless ZooKeeper is behind a proxy server. Relying on that when detecting client IP address should be the exception, not the standard behaviour. Therefore we should disable it by defult.\nComments: - ping [~liwang]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4859", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4859"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: C client tests hang to be cancelled quite often\nDescription: CPPUNIT tests runs sequentially. By comparing logs with successful run, I think it could be {{{}Zookeeper_readOnly::testReadOnlyWithSSL{}}}. Logs from failed run. {noformat} [exec] Zookeeper_multi::testWatch : elapsed 2005 : OK [exec] Zookeeper_multi::testSequentialNodeCreateInAsyncMulti : elapsed 2001 : OK [exec] Zookeeper_multi::testBigAsyncMulti : elapsed 3003 : OK [exec] Zookeeper_operations::testAsyncWatcher1 : elapsed 54 : OK [exec] Zookeeper_operations::testAsyncGetOperation : elapsed 54 : OK [exec] Zookeeper_operations::testOperationsAndDisconnectConcurrently1 : elapsed 382 : OK [exec] Zookeeper_operations::testOperationsAndDisconnectConcurrently2 : elapsed 0 : OK [exec] Zookeeper_operations::testConcurrentOperations1 : elapsed 21127 : OK [exec] Zookeeper_readOnly::testReadOnly ZooKeeper server started ZooKeeper server started : elapsed 12214 : OK {noformat} Logs from successful run. {noformat} [exec] Zookeeper_multi::testWatch : elapsed 2006 : OK [exec] Zookeeper_multi::testSequentialNodeCreateInAsyncMulti : elapsed 2001 : OK [exec] Zookeeper_multi::testBigAsyncMulti : elapsed 3003 : OK [exec] Zookeeper_operations::testAsyncWatcher1 : elapsed 54 : OK [exec] Zookeeper_operations::testAsyncGetOperation : elapsed 54 : OK [exec] Zookeeper_operations::testOperationsAndDisconnectConcurrently1 : elapsed 387 : OK [exec] Zookeeper_operations::testOperationsAndDisconnectConcurrently2 : elapsed 0 : OK [exec] Zookeeper_operations::testConcurrentOperations1 : elapsed 22459 : OK [exec] Zookeeper_readOnly::testReadOnly ZooKeeper server started ZooKeeper server started : elapsed 15515 : OK [exec] Zookeeper_readOnly::testReadOnlyWithSSL ZooKeeper server started ZooKeeper server started : elapsed 14865 : OK {noformat}\nComments: - Shit happens. Github is providing us hostnames with more than 64 characters quite often now. {noformat} fv-az1272-448.grsihaubamwerhiryzjrxtypna.phxx.internal.cloudapp.net fv-az1383-210.ikz3ofujitfebmhnxoyjuylfjd.ex.internal.cloudapp.net {noformat} And openssl refuses to generate certs for ssl tests when hostname length is greater than 64. I found these from {{gencerts.stderr}} (output by {{gencerts.sh}}). {noformat} Error making certificate request 404703E6897F0000:error:06800097:asn1 encoding routines:ASN1_mbstring_ncopy:string too long:../crypto/asn1/a_mbstr.c:106:maxsize=64 {noformat} - I have reported this to https://github.com/actions/runner-images/issues/10650. I was guided to there from Github Support.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4858", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4858"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Remove the lock contention between snapshotting and the sync operation\nDescription: Remove the synchronized keyword from Zookeeper.takeSnapshot() and ZookeeperServer.restoreFromSnapshot() API, as it causes lock contention on the ZookeeperServer object with the sync operation. In ZookeeperServer.java, we have the following {code:java} public synchronized File takeSnapshot(boolean syncSnap, boolean isSevere, boolean fastForwardFromEdits) throws IOException { .... } {code} In ObserverZookeeperServer.java and FollowerZookeeperServer.java, we have the following {code:java} public synchronized void sync() { ... } {code}\nComments: - Will submit a patch for it. - Issue resolved by pull request 2185 [https://github.com/apache/zookeeper/pull/2185]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4857", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4857"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enhancing Logging for Ephemeral Type Handling in EphemeralType.get() Method\nDescription: The {{EphemeralType.get(long ephemeralOwner)}} method in the ZooKeeper codebase is responsible for determining the type of ephemeral node based on the provided {{ephemeralOwner}} value. The current implementation lacks sufficient logging to diagnose and understand how ephemeral types are determined, especially when dealing with extended ephemeral types. By enhancing the method with detailed logging, we can make it easier to track the logic flow and pinpoint issues when handling different {{ephemeralOwner}} values, particularly under scenarios involving extended ephemeral types and emulation of older versions. *Expected Behavior:* The method should log the steps it takes to determine the {{EphemeralType}} based on the provided {{ephemeralOwner}} value. This includes logging whether extended ephemeral types are enabled, if specific properties like TTL emulation are set, and the final {{EphemeralType}} that is returned. For invalid {{ephemeralOwner}} values, an error should be logged before an exception is thrown. *How-to-Fix:* We propose to enhance the {{EphemeralType.get(long ephemeralOwner)}} method with logging statements that detail the decision-making process for returning an {{{}EphemeralType{}}}. These logs will include the status of feature flags, the type of {{{}ephemeralOwner{}}}, and error conditions.\nComments: - To make it clear on how to fix the issue, we upload the comparison between the original version and the log-enhanced version in the attached file `original-vs-log-enhanced.md`. The log-enhanced version is generated automatically utilizing our proposed tool. - -1 {{EphemeralType.get}} is a simple utility method. It should not log anything. {{IllegalArgumentException}} is enough for error path.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4856", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4856"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enhancement of QuorumCnxManager to Log Configuration Parameter for Connection Timeout\nDescription: The {{QuorumCnxManager}} constructor in the ZooKeeper quorum module reads the {{zookeeper.cnxTimeout}} system property to set the connection timeout. However, there is currently no logging in place to indicate whether this parameter has been set or what its value is. This enhancement adds logging to ensure that users are informed whether the parameter is set and what value it holds, or if it falls back to the default timeout. *Expected Behavior:* When the system property {{zookeeper.cnxTimeout}} is set, a log message should be generated indicating the value and the configured timeout in milliseconds. If the property is not set, a warning should be logged stating that the default timeout will be used. *How-to-Fix:* We propose adding logging statements to expose the relationship between the {{zookeeper.cnxTimeout}} system property and the connection timeout configuration. This will help in debugging and understanding how the system parameter affects the quorum manager's behavior.\nComments: - To make it clear on how to fix the issue, we upload the comparison between the original version and the log-enhanced version in the attached file `original-vs-log-enhanced.md`. The log-enhanced version is generated automatically utilizing our proposed tool.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4855", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4855"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enhance Logging in AdminServerFactory to Reflect Configuration Changes\nDescription: The {{AdminServerFactory}} class in ZooKeeper creates an admin server based on a configuration parameter ({{{}zookeeper.admin.enableServer{}}}). Currently, there is no logging to indicate whether the server was enabled or disabled based on this parameter, which makes it difficult to troubleshoot or verify behavior during testing or in production. The enhancement proposes adding logging to capture this configuration decision, aiding in better diagnostics and understanding of server behavior. *Expected Behavior:* When the {{zookeeper.admin.enableServer}} property is set to {{{}\"false\"{}}}, the {{AdminServerFactory}} should log an informational message indicating that the JettyAdminServer is disabled. When the property is set to {{{}\"true\"{}}}, the server should start without additional logging related to this property, unless an error occurs. *How-to-Fix:* It's important to introduce required logging.\nComments: - To make it clear on how to fix the issue, we upload the comparison between the original version and the log-enhanced version in the attached file `original-vs-log-enhanced.md`. The log-enhanced version is generated automatically utilizing our proposed tool.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4854", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4854"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enhance Logging for Client Connection Socket Selection in ZooKeeper\nDescription: The {{ZooKeeper}} class currently selects a client connection socket implementation ({{{}ClientCnxnSocket{}}}) based on a configuration parameter ({{{}zookeeper.clientCnxnSocket{}}}). However, the current implementation does not log the outcome of this selection, making it difficult to debug issues related to socket selection or understand the behavior of the client in different environments. The proposed enhancement adds logging to expose the decision-making process, providing clearer insights into which socket is being used and whether the configuration was set correctly. *Expected Behavior:* When the {{zookeeper.clientCnxnSocket}} property is set to a valid value (e.g., {{{}ClientCnxnSocketNetty{}}}), the log should indicate that this specific socket is being used. If the property is not set or is set to the default value (ClientCnxnSocketNIO), the log should confirm that the default socket is in use. If the property is set to an unrecognized value, the log should warn that the system is defaulting to {{{}ClientCnxnSocketNIO{}}}. *How-to-Fix:* Introduce logging within the {{getClientCnxnSocket}} method to capture and report the configuration state of the {{ZOOKEEPER_CLIENT_CNXN_SOCKET}} parameter. This enhancement will help trace which client connection socket implementation is selected\u2014whether it's the default {{{}ClientCnxnSocketNIO{}}}, a specified alternative like {{{}ClientCnxnSocketNetty{}}}, or an unrecognized value that causes a fallback to the default. The added log statements will improve transparency and aid in debugging by providing clear and actionable information in the logs.\nComments: - To make it clear on how to fix the issue, we upload the comparison between the original version and the log-enhanced version in the attached file `original-vs-log-enhanced.md`. The log-enhanced version is generated automatically utilizing our proposed tool.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4853", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4853"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: erroneous assertion in ZooKeeperQuotaTest#testQuota\nDescription: Created for https://github.com/apache/zookeeper/pull/2169. {code:java} assertNotNull(server.getZKDatabase().getDataTree().getMaxPrefixWithQuota(path) != null, \"Quota is still set\"); {code} The above equation test evaluates to a non null boolean value, so always pass.\nComments: - Issue resolved by pull request 2169 [https://github.com/apache/zookeeper/pull/2169]", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4852", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4852"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix the bad \"*uuuuu\" mark in the ASF license\nDescription: we have four java files have this issue. e.g. {code:java} /* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 *uuuuu * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"/RequuuAS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.zookeeper.server; import java.io.IOException; import java.nio.ByteBuffer; import java.util.function.Supplier; import org.apache.jute.Record; public interface RequestRecord { {code}\nComments: - master: 9d522642a9c78dafb7084952f9807beb205478c3 branch-3.9: 3ca20f8a867c63255d212035287eac1c8dbad750 branch-3.8: 758a0d2e621b38c4531c88520f9564778e547e50 (after conflict resolved)", "output": "Status: Closed\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4851", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4851"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Honor X-Forwarded-For optionally in IPAuthenticationProvider\nDescription: Introduce a feature flag in *IPAuthenticationProvider* to toggle reading of *X-Forwarded-For* header from HTTP requests, because there're some use cases when it's not needed.\nComments: - ping [~liwang]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4850", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4850"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Enhance zkCli Tool to Support Reading and Writing Binary Data\nDescription: Currently, the `get` and `set` commands in the zkCli tool only support reading and writing text data. However, znode data can be an arbitrary byte array. It would be useful to read and write binary data in base64 or hexdump format.\nComments: - I don't think this is worth it. See my comments on the PR", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4849", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4849"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Option to Provide Custom X509 Implementation of QuorumAuthServer and QuorumAuthLearner\nDescription: With the implementation of ZOOKEEPER-2123, we now have the option to implement a custom X509 AuthenticationProvider for client-server authentication. We also need a similar option for quorum authentication. In our cloud environment, we could restrict quorum members to those with specific certificates, allowing only those to join the quorum.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4848", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4848"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Possible stack overflow in setup_random\nDescription: Created for https://github.com/apache/zookeeper/pull/2097. {code:c} int seed_len = 0; /* Enter a loop to fill in seed with random data from /dev/urandom. * This is done in a loop so that we can safely handle short reads * which can happen due to signal interruptions. */ while (seed_len < sizeof(seed)) { /* Assert we either read something or we were interrupted due to a * signal (errno == EINTR) in which case we need to retry. */ int rc = read(fd, &seed + seed_len, sizeof(seed) - seed_len); assert(rc > 0 || errno == EINTR); if (rc > 0) { seed_len += rc; } } {code} Above code will overflow {{seed}} in case of a short read.\nComments: - Issue resolved by pull request 2097 [https://github.com/apache/zookeeper/pull/2097]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4847", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4847"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Found od in zookeeper/data/version-2/currentEpoch\nDescription: {code:java} 2024-07-30 16:32:48,950 [myid:1] - ERROR [main:QuorumPeer@1148] - Unable to load database on disk java.io.IOException: Found od in /cloud/data/zookeeper/data/version-2/currentEpoch at org.apache.zookeeper.server.quorum.QuorumPeer.readLongFromFile(QuorumPeer.java:2126) at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1100) at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1079) at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:227) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:136) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90) 2024-07-30 16:32:48,953 [myid:1] - ERROR [main:QuorumPeerMain@113] - Unexpected exception, exiting abnormally java.lang.RuntimeException: Unable to run quorum server at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1149) at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1079) at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:227) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:136) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90) Caused by: java.io.IOException: Found od in /cloud/data/zookeeper/data/version-2/currentEpoch at org.apache.zookeeper.server.quorum.QuorumPeer.readLongFromFile(QuorumPeer.java:2126) at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1100) ... 4 more 2024-07-30 16:32:48,954 [myid:1] - INFO [main:ZKAuditProvider@42] - ZooKeeper audit is disabled. 2024-07-30 16:32:48,955 [myid:1] - ERROR [main:ServiceUtils@48] - Exiting JVM with code 1 {code} I accidentally encountered this error and found that the current Epoch file had been written with letters. Then, the zk process detected the contents of this file during restart and threw an exception before exiting the process. However, zk was unable to recover it.\nComments: - The code written into the contents of this file is as follows: {code:java} /** * Write a long value to disk atomically. Either succeeds or an exception * is thrown. * @param name file name to write the long to * @param value the long value to write to the named file * @throws IOException if the file cannot be written atomically */ // visibleForTest void writeLongToFile(String name, final long value) throws IOException { File file = new File(logFactory.getSnapDir(), name); new AtomicFileWritingIdiom(file, new WriterStatement() { @Override public void write(Writer bw) throws IOException { bw.write(Long.toString(value)); } }); } {code} At the moment when the file was written with letters, there may have been a node power outage, disk failure, etc., but I feel that these should not have caused the writing of long numbers to become writing letters \u2018od\u2019. Who has experience and insights in this area, please. - The severity of this problem lies in the fact that once the \u2018currentEpoch\u2018 content becomes letters, if zk restarts due to other factors, it will never be able to start successfully. I have proposed a solution to avoid and fix the issue, which allows the zk process to automatically recover when restarted. This involves a file deletion operation when incorrect file content is found during the startup process. This way, when zk is pulled up again, a new valid file can be created by default. {code:java} private long readLongFromFile(String name) throws IOException { File file = new File(logFactory.getSnapDir(), name); BufferedReader br = new BufferedReader(new FileReader(file)); String line = \"\"; try { line = br.readLine(); return Long.parseLong(line); } catch (NumberFormatException e) { // Delete the file and print a log when deletion fails if (!file.delete()) { LOG.error(\"Unable to delete wrong file {}\", file); } throw new IOException(\"Found \" + line + \" in \" + file); } finally { br.close(); } } {code} If the file does not exist at startup, it will be created by default, which is an existing logic\uff1a {code:java} private void loadDataBase() { try { zkDb.loadDataBase(); // load the epochs long lastProcessedZxid = zkDb.getDataTree().lastProcessedZxid; long epochOfZxid = ZxidUtils.getEpochFromZxid(lastProcessedZxid); try { currentEpoch = readLongFromFile(CURRENT_EPOCH_FILENAME); } catch (FileNotFoundException e) { // pick a reasonable epoch number // this should only happen once when moving to a // new code version currentEpoch = epochOfZxid; LOG.info( \"{} not found! Creating with a reasonable default of {}. \" + \"This should only happen when you are upgrading your installation\", CURRENT_EPOCH_FILENAME, currentEpoch); writeLongToFile(CURRENT_EPOCH_FILENAME, currentEpoch); } {code}", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4846", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4846"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Failure to reload database due to missing ACL\nDescription: ZooKeeper snapshots are {_}fuzzy{_}, as the server does not stop processing requests while ACLs and nodes are being streamed to disk. ACLs, notably, are streamed {_}first{_}, as a mapping between the full serialized ACL and an \"ACL ID\" referenced by the node. Consequently, a snapshot can very well contain ACL IDs which do not exist in the mapping. Prior to ZOOKEEPER-4799, such situations would produce harmless (if annoying) \"Ignoring acl XYZ as it does not exist in the cache\" INFO entries in the server logs. With ZOOKEEPER-4799, we started \"eagerly\" fetching the referenced ACLs in {{DataTree}} operations such as {{{}createNode{}}}, {{{}deleteNode{}}}, etc.\u2014as opposed to just fetching them from request processors. This can result in fatal errors during the {{fastForwardFromEdits}} phase of restoring a database, when transactions are processed on top of an inconsistent data tree\u2014preventing the server from starting. The errors are thrown in this code path: {code:java} // ReferenceCountedACLCache.java:90 List<ACL> acls = longKeyMap.get(longVal); if (acls == null) { LOG.error(\"ERROR: ACL not available for long {}\", longVal); throw new RuntimeException(\"Failed to fetch acls for \" + longVal); } {code} Here is a scenario leading to such a failure: * An existing node {{{}/foo{}}}, sporting an unique ACL, is deleted. This is recorded in transaction log {{{}$SNAP-1{}}}; said ACL is also deallocated; * Snapshot {{$SNAP}} is started; * The ACL map is serialized to {{{}$SNAP{}}}; * A new node {{/foo}} sporting the same unique ACL is created in a portion of the data tree which still has to be serialized; * Node {{/foo}} is serialized to {{{}$SNAP{}}}\u2014but its ACL isn't; * The server is restarted; * The {{DataTree}} is initialized from {{{}$SNAP{}}}, including node {{/foo}} with a dangling ACL reference; * Transaction log {{$SNAP-1}} is being replayed, leading to a {{{}deleteNode(\"/foo\"){}}}; * {{getACL(node)}} panics, preventing a successful restart.\nComments: - Example stack trace. {noformat} 2025-02-05 16:07:20,312 ERROR org.apache.zookeeper.server.ZooKeeperServerMain: Unexpected exception, exiting abnormally java.lang.RuntimeException: Failed to fetch acls for 7382 at org.apache.zookeeper.server.ReferenceCountedACLCache.convertLong(ReferenceCountedACLCache.java:93) at org.apache.zookeeper.server.DataTree.getACL(DataTree.java:801) at org.apache.zookeeper.server.DataTree.createNode(DataTree.java:455) at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:881) at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:864) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:442) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:350) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.lambda$restore$0(FileTxnSnapLog.java:267) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:312) at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:285) at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:531) at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:704) at org.apache.zookeeper.server.NettyServerCnxnFactory.startup(NettyServerCnxnFactory.java:745) at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:130) at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:161) at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:113) at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:68) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:141) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:91){noformat} It indicates that problem happens when trying to execute createTxn a znode whose *parent* doesn't have a valid ACL. - Found a solution. It's super simple. ZooKeeper actually doesn't lose that txn gap, it's in the previous logfile and it iterates over it. Look at this log snippet: {noformat} 2025-02-06 17:12:40,818 [myid:] - INFO [main:FileSnap@85] - Reading snapshot /home/andor/tmp/zookeeper/version-2/snapshot.674c0 2025-02-06 17:12:40,845 [myid:] - INFO [main:ReferenceCountedACLCache@174] - Ignoring acl 7382 as it does not exist in the cache 2025-02-06 17:12:40,847 [myid:] - INFO [main:DataTree@1719] - The digest in the snapshot has digest version of 2, with zxid as 0x674c1, and digest value as 8282470946294 2025-02-06 17:12:40,858 [myid:] - INFO [main:FileTxnLog$FileTxnIterator@693] - Going to next log file /home/andor/tmp/zookeeper/version-2/log.5a1df 2025-02-06 17:12:40,924 [myid:] - INFO [main:FileTxnSnapLog@351] - Processing zxid 0x674c1: 0x674c1 2025-02-06 17:12:40,924 [myid:] - INFO [main:DataTree@469] - Creating node /cloudera_manager_zookeeper_canary with acl 6 2025-02-06 17:12:40,928 [myid:] - INFO [main:FileTxnLog$FileTxnIterator@693] - Going to next log file /home/andor/tmp/zookeeper/version-2/log.674c2 2025-02-06 17:12:40,928 [myid:] - INFO [main:FileTxnSnapLog@351] - Processing zxid 0x674c2: 0x674c2 2025-02-06 17:12:40,928 [myid:] - INFO [main:FileTxnSnapLog@351] - Processing zxid 0x674c3: 0x674c3 2025-02-06 17:12:40,929 [myid:] - ERROR [main:ReferenceCountedACLCache@92] - ERROR: ACL not available for long 7382 2025-02-06 17:12:40,929 [myid:] - ERROR [main:ZooKeeperServerMain@91] - Unexpected exception, exiting abnormally java.lang.RuntimeException: Failed to fetch acls for 7382 at org.apache.zookeeper.server.ReferenceCountedACLCache.convertLong(ReferenceCountedACLCache.java:93) {noformat} Snapshot zxid is {*}0x674c0{*}, so ZK goes to txn logfile *0x5a1df* first and fast forward to txn zxid {*}0x674c1{*}, so actually processing the txn where the parent znode was created. Clearly the ACL id is different in the cache and the data tree: 6 <> 7382 Look at the code where we process the txn: {code:java} synchronized (parent) { parentAcl = getACL(parent); // Add the ACL to ACL cache first, to avoid the ACL not being // created race condition during fuzzy snapshot sync. // // This is the simplest fix, which may add ACL reference count // again if it's already counted in in the ACL map of fuzzy // snapshot, which might also happen for deleteNode txn, but // at least it won't cause the ACL not exist issue. // // Later we can audit and delete all non-referenced ACLs from // ACL map when loading the snapshot/txns from disk, like what // we did for the global sessions. Long longval = aclCache.convertAcls(acl); <---- we re-create the ACL in the ACL cache, but potentially with different id Set<String> children = parent.getChildren(); if (children.contains(childName)) { throw new KeeperException.NodeExistsException(); <---- znode already exists in DataTree, so we skip } ... {code} The only thing that we need to do here is to get a reference to the existing znode and fix the ACL reference. {code:java} if (children.contains(childName)) { DataNode child = nodes.get(path); child.acl = longval; throw new KeeperException.NodeExistsException(); } {code} or maybe something more elegant. We don't lose the ACL information. - [~andor] , nice, I think that makes sense! - Issue resolved by pull request 2222 [https://github.com/apache/zookeeper/pull/2222]", "output": "Status: Closed\nPriority: Blocker"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4845", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4845"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix the digest mismatch log\nDescription: Currently we log the TxnDigest object as the expected digest instead of the tree digest number. This makes it quite confusing when comparing the expected digest with the actual digest, as the actual digest is the number. Change digest to logDigest in the following code. {code:java} if (firstMismatchTxn) { LOG.error(\"First digest mismatch on txn: {}, {}, \" + \"expected digest is {}, actual digest is {}, \", header, txn, digest, actualDigest); firstMismatchTxn = false; } {code}\nComments: - will submit a PR for the fix.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4844", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4844"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fail-slow disk while executing writeLongToFile can cause the follower to hang\nDescription: {*}Symptom:{*}If a thread is doing a file write and stuck in writeLongToFile, this thread will hang. This blocking shoud be handled by the zookeeper via PING. However, if the QuorumPeer executes the writeLongToFile and encounters a fail-slow disk, the entire follower can be stuck. The leader will abandon this follower, but the follower believes that it is a follower. Callstack is as following: {code:java} at org.apache.zookeeper.common.AtomicFileOutputStream.write(AtomicFileOutputStream.java:72) at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221) at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291) at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295) at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141) at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229) at java.io.BufferedWriter.flush(BufferedWriter.java:254) at org.apache.zookeeper.common.AtomicFileWritingIdiom.<init>(AtomicFileWritingIdiom.java:72) at org.apache.zookeeper.common.AtomicFileWritingIdiom.<init>(AtomicFileWritingIdiom.java:54) at org.apache.zookeeper.server.quorum.QuorumPeer.writeLongToFile(QuorumPeer.java:2233) at org.apache.zookeeper.server.quorum.QuorumPeer.setAcceptedEpoch(QuorumPeer.java:2262) at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:510) at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:91) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1556) {code} *Root cause:* The Quorum is blocked in writeLongToFile and can not execute readPacket, so no timeout exception is arised to trigger the error handler. Moreover, this problem cannot be handle by add \"-Dlearner.asyncSending=true\"(https://issues.apache.org/jira/browse/ZOOKEEPER-4074)\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4843", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4843"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Encountering an 'Unreasonable Length' error when configuring jute.maxbuffer to 1GB or more\nDescription: *Problem:* Encountering an 'Unreasonable Length' error when configuring jute.maxbuffer to 1GB or more {code:java} 2024-07-04 11:55:41,806 [myid:localhost:2181] - WARN [main-SendThread(localhost:2181):o.a.z.ClientCnxn$SendThread@1300] - Session 0x0 for server localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException. java.io.IOException: Unreasonable length = 16 at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:166) at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:127) at org.apache.zookeeper.proto.ConnectResponse.deserialize(ConnectResponse.java:80) at org.apache.zookeeper.ClientCnxnSocket.readConnectResult(ClientCnxnSocket.java:141) at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:86) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1289) 2024-07-04 11:55:42,567 [myid:] - INFO [main:o.a.z.u.ServiceUtils@45] - Exiting JVM with code 0 {code} *Analysis:* If jute.maxbuffer is set to 1GB (equivalent to 1073741824 bytes), this means that both maxBufferSize and extraMaxBufferSize are also set to 1073741824. Even when the response size (denoted as 'len') is minimal, the following condition check still fails: {code:java} if (len < 0 || len > maxBufferSize + extraMaxBufferSize) { throw new IOException(UNREASONBLE_LENGTH + len); } {code} It is because (maxBufferSize + extraMaxBufferSize) overflow int MAX_VALUE and the result is a negative number. *Solution:* Configure jute.maxbuffer.extrasize explicitly and give a reasonable value, so that the sum of maxBufferSize and extraMaxBufferSize is less than Integer.MAX_VALUE. *Improvement:* While setting jute.maxbuffer to 1GB may seem unreasonable, it would be better to validate the maxBuffer and extraMaxBufferSize configuration. If the sum of these two values is greater than Integer.MAX_VALUE, it should be handled gracefully by setting the sum to Integer.MAX_VALUE\nComments: - [~arshad.mohammad] {quote}If jute.maxbuffer is set to 1GB (equivalent to 1073741824 bytes), this means that both maxBufferSize and extraMaxBufferSize are also set to 1073741824. {quote} Why do you say that? These're 2 different and independent settings. If I set jute.maxbuffer to 1GB only, extraMaxBufferSize will be the default value 1,024. Is that correct? - [~arshad.mohammad] Ignore my previous comment, I see it now: {noformat} final Integer configuredExtraMaxBuffer = Integer.getInteger(\"zookeeper.jute.maxbuffer.extrasize\", maxBuffer); {noformat} Let me commit your fix, thank you.", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4842", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4842"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Zookeeper quorum is not formed intermittently with trailing dot in the cluster domain name\nDescription: On kubernetes, we've set up the cluster domain with a trailing dot. Doing so, we are seeing very often that the zookeeper quorum itself is not being established. {code:java} bash-4.4$ env -u KAFKA_OPTS zookeeper-shell localhost:2181 config Connecting to localhost:2181 [2024-06-25 10:36:39,178] WARN Client session timed out, have not heard from server in 30031ms for session id 0x0 (org.apache.zookeeper.ClientCnxn) [2024-06-25 10:36:39,182] WARN Session 0x0 for server localhost/[0:0:0:0:0:0:0:1]:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException. (org.apache.zookeeper.ClientCnxn) org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 30031ms for session id 0x0 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1257) KeeperErrorCode = ConnectionLoss for /zookeeper/config {code} In the zookeeper logs, we see a lot of IOExceptions, UnknownHost and Interrupted exceptions. {code:java} java.io.IOException: ZooKeeperServer not running at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:565) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:350) at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:508) at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:153) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source) {\"type\":\"log\", \"host\":\"zk-swkf-2.default\", \"level\":\"WARN\", \"systemid\":\"zookeeper-2b13339237454984887b4908dc3a6df0\", \"system\":\"zookeeper\", \"time\":\"2024-06-25T10:23:16.325Z\", \"timezone\":\"UTC\", \"log\":{\"message\":\"NIOWorkerThread-1 - org.apache.zookeeper.server.NIOServerCnxn - Close of session 0x0\"}} java.lang.InterruptedException at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source) at org.apache.zookeeper.util.CircularBlockingQueue.poll(CircularBlockingQueue.java:105) at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1453) at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$900(QuorumCnxManager.java:99) at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1277) {code} this is the content of the /etc/resolve.conf {code:java} bash-4.4$ cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local bcmt nameserver 10.254.0.10 options ndots:5{code} {code:java} [root@vm-10-76-72-33 ckaf-kafka]# nslookup zk-swkf.default.svc.cluster.local. Server: 10.76.72.33 Address: 10.76.72.33#53 Name: zk-swkf.default.svc.cluster.local Address: 10.254.94.24 [root@vm-10-76-72-33 ckaf-kafka]# nslookup zk-swkf.default.svc.cluster.local Server: 10.76.72.33 Address: 10.76.72.33#53 Name: zk-swkf.default.svc.cluster.local Address: 10.254.94.24 {code}\nComments: - A point to be noted is that, without the trailing dot in the cluster domain, this issue is never reproducible. - [~phunt] , could we get some help with this issue.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4841", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4841"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add op count metrics to get observability on traffic load and pattern\nDescription: Add op count metrics to get observability on traffic load and pattern.\nComments: - Will submit a PR for it. - Does {{RequestPathMetricsCollector}} help ? - Also, we have {{AuditLogger}} which could be used to log(or whatever you want through {{zookeeper.audit.impl.class}}) almost every operation.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4840", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4840"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Repeated SessionExpiredException after Zookeeper daemon restart\nDescription: h2. TL;DR After upgrade Apache Zookeeper & Apache Curator, our application is experiencing a new unexpected behaviour after events such as: * Zookeeper server(s) being restarted * Zookeeper server failing & being replaced h2. Background Application is using Zookeeper for leader election & metadata storage. The application runs on 3 hosts which each also have 1 Zookeeper server/daemon running. Previously the application was running on Zookeeper version 3.5.10 & Curator version 4.3.0 After upgrading to Zookeeper version 3.9.1 & Curator version 5.2.0 a new edge case was observed where after Zookeeper daemons are restarted/failed the application (i.e. Zookeeper client) enters a 15+ minute loop of repeatedly logging \"{{{}SessionExpiredException\"{}}} These repeated \"{{{}SessionExpiredException\"{}}} are not indicative of a full Zookeeper client/communication outage because DEBUG logs show that other Zookeeper sessions are communicating just fine. The \"{{{}SessionExpiredException\"{}}} logs unfortunately do not show the associated Session ID which is encountering the \"{{{}SessionExpiredException\"{}}} h2. Symptoms When using Zookeeper version 3.9.1 & Curator version 5.2.0, after restarting/failing some of the Zookeeper daemons: # All the 3 zookeeper clients experience some connections failures lasting a few seconds after the Zookeeper daemons were failed/restarted. # These connection failure issues are resolved shortly without any action needed. # Around 1 minute after the Zookeeper daemons were failed/restarted, all the 3 zookeeper clients start repeatedly logging \"{{{}SessionExpiredException\"{}}} # The \"{{{}SessionExpiredException\"{}}} is repeatedly logged for 15+ minutes. During this time there are no connectivity issues. We can see from the Zookeeper server logs that all 3 Zookeeper servers are receiving regular traffic from the clients. # Interestingly, each Zookeeper server is not receiving any requests from the local Zookeeper client for the duration of the period where \"{{{}SessionExpiredException\"{}}} is repeatedly logged by the clients. However, each Zookeeper server is receiving regular traffic from the 2 remote Zookeeper clients. The evidence suggests that this is a client-side issue & the \"{{{}SessionExpiredException\"{}}} is being thrown before the request is even sent to the Zookeeper server. h2. Root Cause Theory Based on my analysis of the available DEBUG & TRACE level logs, my current working theory is that the Zookeeper client is repeatedly trying to use a connection/session in CLOSED state which is causing the client to repeatedly throw \"{{{}SessionExpiredException\"{}}} for each retry. The first \"{{{}SessionExpiredException\"{}}} is thrown after a log pattern like the following: {quote}2024-06-18 01:14:45,694 WARN Curator-ConnectionStateManager-0: Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 60000. Adjusted session timeout ms: 60000 2024-06-18 01:14:45,694 INFO Curator-ConnectionStateManager-0: ZK getBindInformation: ip-172-31-200-85.ec2.internal:22181,ip-172-31-200-230.ec2.internal:22181,ip-172-31-200-39.ec2.internal:22181 2024-06-18 01:14:45,695 INFO Curator-ConnectionStateManager-0: injectSessionExpiration() called 2024-06-18 01:14:45,695 INFO Curator-ConnectionStateManager-0: channel is told closing 2024-06-18 01:14:45,698 INFO epollEventLoopGroup-3-1: channel is disconnected: [id: 0x86a592c8, L:/172.31.200.230:35836 ! R:ip-172-31-200-230.ec2.internal/172.31.200.230:22181] 2024-06-18 01:14:45,698 INFO epollEventLoopGroup-3-1: channel is told closing 2024-06-18 01:14:45,698 TRACE application-SendThread(ip-172-31-200-230.ec2.internal:22181): SendThread exited loop for session: 0x1000000fc490005 2024-06-18 01:14:45,703 DEBUG application-EventThread: Retry-able exception received org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /... at org.apache.zookeeper.KeeperException.create(KeeperException.java:133) at org.apache.zookeeper.KeeperException.create(KeeperException.java:53) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1972) at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue.readValue(ConnectionStateAwareSharedValue.java:253) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue.access$200(ConnectionStateAwareSharedValue.java:26) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue$1.process(ConnectionStateAwareSharedValue.java:45) at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:83) at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:563) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:538) {quote} It seems the initial Zookeeper daemon restart/failure is causing the Curator client to \"{_}injectSessionExpiration(){_}\" which is then causing a session \"{_}0x1000000fc490005{_}\" to have its SendThread exited. [The injectSessionExpiration method is then putting the ClientCnxn object for the session into CLOSED state.|https://github.com/apache/zookeeper/blob/39973dc5fddf778733e4f0370980629c07a16d67/zookeeper-server/src/main/java/org/apache/zookeeper/ZooKeeperTestable.java#L41] This is causing [the following code path to throw \"{{{}SessionExpiredException\"{}}} before the request is even sent to the Zookeeper server|https://github.com/apache/zookeeper/blob/39973dc5fddf778733e4f0370980629c07a16d67/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L777] Then 25 minutes later, the EventThread for the Zookeeper session is finally also stopped: {quote}2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: GetDataBuilderImpl-Background - 1163211 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: GetChildrenBuilderImpl-Background - 1163211 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: CreateBuilderImpl-Background - 1163211 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: GetChildrenBuilderImpl-Background - 1003198 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: GetDataBuilderImpl-Background - 383164 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: GetChildrenBuilderImpl-Background - 383164 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: CreateBuilderImpl-Background - 383164 ms 2024-06-18 01:40:43,942 TRACE application-EventThread: Trace: GetChildrenBuilderImpl-Background - 266157 ms 2024-06-18 01:40:43,942 INFO application-EventThread: EventThread shut down for session: 0x1000000fc490005 {quote} Only after the EventThread is stopped do the \"{{{}SessionExpiredException\"{}}} stop occurring. This leads me to believe that the lingering EventThread for the closed session is leading to the closed session being used which is leading to the repeated \"{{{}SessionExpiredException\"{}}} which occur until the EventThread is stopped. In between \"2024-06-18 01:14:45\" & \"2024-06-18 01:40:43\" we see 0 requests from the local client \"{_}172.31.200.230{_}\" reaching the Zookeeper server also on \"{_}172.31.200.230{_}\". However, other sessions to other Zookeeper servers are unaffected & operate without connection or session expiry issues. h2. Additional Notes * We can see the stack trace from \"Zookeeper.getData\" down to \"conPacketLoss\" here: ** [Zookeeper.getData|https://github.com/apache/zookeeper/blob/39973dc5fddf778733e4f0370980629c07a16d67/zookeeper-server/src/main/java/org/apache/zookeeper/ZooKeeper.java#L2013] ** [ClientCnxn.submitRequest|https://github.com/apache/zookeeper/blob/39973dc5fddf778733e4f0370980629c07a16d67/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L1516] ** [ClientCnxn.queuePacket|https://github.com/apache/zookeeper/blob/39973dc5fddf778733e4f0370980629c07a16d67/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L1621] ** [ClientCnxn.conPacketLoss|https://github.com/apache/zookeeper/blob/39973dc5fddf778733e4f0370980629c07a16d67/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L777] * I have validated the Curator RetryPolicy configuration is not the problem here. Each \"{{{}SessionExpiredException\"{}}} is retried 5 times within under 1 minute before being surfaced as a \"Watcher exception\": {quote}2024-06-18 01:17:42,200 INFO application-EventThread: ZK getBindInformation: ip-172-31-200-85.ec2.internal:22181,ip-172-31-200-230.ec2.internal:22181,ip-172-31-200-39.ec2.internal:22181 2024-06-18 01:17:42,200 DEBUG application-EventThread: Retry-able exception received org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /... at org.apache.zookeeper.KeeperException.create(KeeperException.java:133) at org.apache.zookeeper.KeeperException.create(KeeperException.java:53) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1972) at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue.readValue(ConnectionStateAwareSharedValue.java:253) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue.access$200(ConnectionStateAwareSharedValue.java:26) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue$1.process(ConnectionStateAwareSharedValue.java:45) at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:83) at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:563) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:538) ... 2024-06-18 01:18:27,202 TRACE application-EventThread: Counter retries-allowed: 1 2024-06-18 01:18:27,202 DEBUG application-EventThread: Retrying operation 2024-06-18 01:18:27,202 INFO application-EventThread: ZK getBindInformation: ip-172-31-200-85.ec2.internal:22181,ip-172-31-200-230.ec2.internal:22181,ip-172-31-200-39.ec2.internal:22181 2024-06-18 01:18:27,202 DEBUG application-EventThread: Retry-able exception received org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /... ... 2024-06-18 01:18:27,202 TRACE application-EventThread: Counter retries-disallowed: 1 2024-06-18 01:18:27,202 DEBUG application-EventThread: Retry policy not allowing retry 2024-06-18 01:18:27,202 ERROR application-EventThread: Watcher exception org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /... at org.apache.zookeeper.KeeperException.create(KeeperException.java:133) at org.apache.zookeeper.KeeperException.create(KeeperException.java:53) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1972) at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue.readValue(ConnectionStateAwareSharedValue.java:253) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue.access$200(ConnectionStateAwareSharedValue.java:26) at org.apache.curator.framework.recipes.shared.ConnectionStateAwareSharedValue$1.process(ConnectionStateAwareSharedValue.java:45) at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:83) at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:563) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:538) {quote}\nComments: - Maybe this is related to: https://issues.apache.org/jira/browse/CURATOR-561 - The issue reproduces with Curator 5.2.1, as such I do not think its fixed by: https://issues.apache.org/jira/browse/CURATOR-561 - This issue does not reproduce when using Curator 4.3.0 with Zookeeper 3.9.1 In this case the \"injectSessionExpiration\" logic does not occur (because it was not introduced until Curator 5.x) & the repeated \"Session expired\" failure logs also do not occur I am thinking this is an issue related to the Curator \"injectSessionExpiration\" logic - Could be related to: https://issues.apache.org/jira/browse/CURATOR-437 Symptom is different but this statement seems related: {quote}Curator inject will set zookeeper state to CLOSED when session expires without close zk associated threads {quote} - Application code base is using a custom Curator recipe \"ConnectionStateAwareSharedValue\" which was previously based on: [https://github.com/apache/curator/blob/master/curator-recipes/src/main/java/org/apache/curator/framework/recipes/shared/SharedValue.java] It seems the root cause here is that this bug fix [https://issues.apache.org/jira/browse/CURATOR-311] was not ported to \"ConnectionStateAwareSharedValue\" - Confirmed that this was due to https://issues.apache.org/jira/browse/CURATOR-311 The custom \"ConnectionStateAwareSharedValue\" recipe was missing upstream bug fixes from the \"SharedValue\" recipe - Duplicate of https://issues.apache.org/jira/browse/CURATOR-311", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4839", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4839"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: When DigestMD5 is used to enable mandatory client authentication,Users that do not exist can log in\nDescription: When DigestMD5 is used to enable mandatory client authentication. Consider the following scenario: After successfully logging in with the correct user and password for the first time, change the user to keep the correct password for the last time, and you can still log in normally. I looked at both versions 3.5.10 and 3.9.2. See the class SaslServerCallbackHandler server-side code. A global private variable called userName is defined, but in the handleNameCallback method, if the given user name is not configured, it simply returns without updating userName. This results in the handlePasswordCallback method still using the userName of the last successful login to retrieve, and naturally can find the last password, and the comparison is correct.\nComments: - !image-2024-06-19-11-04-14-140.png! - There is an uncovered condition when processing users that are not on the credential list. When a user is not contained in the credentials, we should invoke the appropriate method to handle this scenario. we should call the method {code:java} pc.clearPassword(); {code} when user not in credentials. Let me try to fix it! - I have created a pr to resolve it. [Clear user credentials when userName non-exist in sasl login module by luoxiner \u00b7 Pull Request #2176 \u00b7 apache/zookeeper (github.com)|https://github.com/apache/zookeeper/pull/2176] - Issue resolved by pull request 2176 [https://github.com/apache/zookeeper/pull/2176]", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4838", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4838"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Use NettyServerCnxnFactory as the default ServerCnxnFactory implementation\nDescription: NettyServerCnxnFactory has been released for a long time, supports more features, and has higher performance. Maybe it can become the default implementation for ServerCnxnFactory.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4837", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4837"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Network issue causes ephemeral node unremoved after the session expiration\nDescription: In our testing cluster with the latest ZooKeeper version (66202cb), we observed that sometimes an ephemeral node never gets deleted if there is a network issue during the PROPOSAL request, even after the session expires. This bug is essentially related to ZOOKEEPER-2355, but the issue was not entirely fixed in the previous patch. We also tested on some related open PRs (e.g., [https://github.com/apache/zookeeper/pull/2152] and [https://github.com/apache/zookeeper/pull/1925] ), and confirmed the issue exists after the proposed fix. Steps to reproduce this bug: # Start a cluster with 3 servers, follower A, leader B, follower C # Open a zk client in server A # Create an ephemeral node in the client # Inject network issue in all server that causes SocketTimeoutException during readPacket if the packet is a PROPOSAL # Close the client # Wait until the cluster is stable (the leader will change between B and C several times) # Remove the network issue from all server # Check every server for ephemeral node existence. The ephemeral node will exist in server A. However, server B and C will not have the ephemeral node anymore. Essentially the bug is caused by loadDatabase loading a snapshot with a higher Zxid than the truncated log, causing fastForwardFromEdits to fail when trying to replay the transactions. For example, if one of the follower has a lastProcessedZxid of 0x200000001 and last snapshot snapshot.200000001, and the leader sends a TRUNC with a zxid of 100000002, truncateLog will truncate the follower's log to 100000002. However, loadDatabase will load snapshot.200000001. So when fastForwardFromEdits happens, it will set the data tree to 200000001 instead of 100000002. We also attached a test case to reproduce this issue. Note that this test case is still pretty flaky at this moment. We proposed to fix this case by loading the database from the last snapshot that happens before the last truncated-log entry during truncateLog. See our PR attached. Of course, this may not be the ideal solution and we welcome suggestions. Some other potential solutions include: (1) Disable fastForwardDatabase in shutdown (2) Run setLastProcessedZxid at the end of Learner's syncWithLeader function if the packet is Leader.DIFF Your insights are very much appreciated. We will continue following up this issue until it is resolved.\nComments: - I can confirm that the failure is also reproducible on a different cluster. Hello [~lvfangmin] and [~andor] , you were both incredibly helpful a few years ago when we reported a different bug (ZOOKEEPER-3531). If you have the time, could you please take a look at this issue as well? Thank you! I have also included [~jonmv] for his insights, given his involvement in a related bug case ([#1925|https://github.com/apache/zookeeper/pull/1925]). - If the root cause is as reported, is that mean the sequence of all TRUNC is incorrect? Data of each servers after TRUNC are always inconsistent, such as some transcations of session expired truncated but still visible. We have reproduced a issue like this in our cluster. The affected(network issus) follower has log 'Digests are not matching' after trunc. After a while, we reboot all server at same time. The affected server become leader after election, I guess because it has the highest zxid. We can see some ephemeral nodes which has been deleted some time before on the leader, and they are invisible on followers. I can't upload attachment, part of log is like this. {code:java} WARN (CommitProcessor:5,98) RateLogger,87 - Message:Digests are not matching. Value is Zxid. Value:51539607553 DEBUG (CommitProcessor:5,98) DataTree,1809 - Digest in log: 364897877017, actual tree: 352915824196 DEBUG (nioEventLoopGroup-4-3,59) NettyServerCnxnFactory$CnxnChannelHandler,318 - Received ReadEvent.ENABLE ERROR (CommitProcessor:5,98) DataTree,1811 - First digest mismatch on txn: 360288272277504000,0,51539607553,1744568253693,-10 , 9000 , expected digest is 2,364897877017 , actual digest is 352915824196,{code}", "output": "Status: Open\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4836", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4836"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Inconsist aclIndex leads to MarshallingError\nDescription: There is a three-node cluster. A client create /1 with a new acl1 to leader. When leader sends the snapshot to the follower, the nic becomes fail-slow. Hence, the follower1 get aclcache(aclIndex 2) without acl1. But the follower1 get a correct datatree. When deserializing the datatree, aclIndex is reset to 1. At this time, the aclIndex is inconsistent (leader:3, follower1:1,follower2:3). The follower1 execute Txn that logs the create operation. In follower1, the aclIndex is set to 2 and node/1 points to 3. When executing getAcl /1, MarshallingError arises. System logs are attached. Are there any comments to figure out this issues\uff1f I will very appreciate them.\nComments: - Is this a duplicate of ZOOKEEPER-4689 ? - Thanks for your reply. This seems like a same bug in version 3.10. - [~kezhuw] hi kezhu, if needed, i can provide workloads and methods to reproduce this issue in deployed environment. Hope it can move this issue forward. - [~gendong1] If possible, you can post/upload new logs/envs in ZOOKEEPER-4689. Thank you for reporting this! - I have added links to system logs in ZOOKEEPER-4689.", "output": "Status: Resolved\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4835", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4835"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Netty should be an optional dependency\nDescription: ZK should not mandate the inclusion of Netty if Netty features aren't being used. There are very few usages of Netty from ZK files that are not named Netty, so this looks pretty easy.\nComments: - I attempted to conclusively confirm how viable excluding Netty might be. I came upon ArchUnit[1] and wrote a little test[2] to look for ZK classes that don't have Netty in the name yet call such a class or that which call Netty directly. The results show X509 utilities are intertwined with Netty. And furthermore the methods ZooKeeper.getClientCnxnSocket and ZooKeeperServer.getOutstandingHandshakeNum and UnifiedServerSocket$UnifiedSocket.detectMode() refer to Netty. Thus it seems not safe to exclude Netty today but if the ZK project wanted to invest in better separation, it seems close at hand. [1] ArchUnit: https://www.archunit.org/ [2] my test: https://gist.github.com/dsmiley/8a34cf16dd5827e5396e6da24e19afd2 - I think that this is doable. That said when you use TLS you need Netty and I am pretty sure that many users are using TLS. I think that the main value is to have something that ensures that the zookeeper client really works without any Netty class in the classpath. It is possible that in the future someone uses some utility from Netty. - Note that TLS can be achieved completely independently via Mesh/Istio. {quote} that the zookeeper client really works without any Netty {quote} It's not clear how to make a \"client\" vs \"server\" distinction within the codebase to enforce that it's okay for the \"server\" to talk to Netty but not the \"client\". Plenty of packages are in common (utils). Anyway, as the ArchUnit test shows, it's really pretty close to making a class name based distinction of which classes can talk to Netty.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4834", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4834"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Introduce Typo CI Check to Apache ZooKeeper\nDescription: Hi, I want to introduce new lint tools: typos[1]. This tool is designed to automatically detect and correct typographical errors in our codebase, documentation, and comments. It\u2019s a valuable addition to ensure high-quality contributions and maintain the professional standard of our project. It has also been used in the Apache OpenDAL and Kvrocks projects.\nComments: - master via 9a2dc253caf5e94ec64bd65a72a35c6bc5e2042c - still need add github action", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4833", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4833"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bring back ARM64 to Github Actions using macos-latest\nDescription: ARM64 ci was added in ZOOKEEPER-3919 but lost in ZOOKEEPER-4642 as a consequence to migration to github actions. Now github actions has runner [{{macos-latest}} on {{M1}}|https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners/about-github-hosted-runners#standard-github-hosted-runners-for-public-repositories]. We can bring it back. I think it is valuable for ZooKeeper as we have {{zookeeper-client-c}}.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4832", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4832"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Better guidance on how to configure zookeeper for FIPS\nDescription: Hi there. We're attempting to work out how to produce a zookeeper package and image which is FIPS compliant. We've found multiple references in the code base to `zookeeper.fips-mode`, however on closer inspection this is very misleading, as it is not enabling any FIPS specific settings, neither does it enable zookeeper for FIPS mode. Instead, it just looks to disable 'ZKTrustManager'. It would be great to get some guidance here, and possibly an article / docs update with configuration details. For example, when working with Java applications, there are usually multiple layers to building a FIPS image, including: * Configuring OpenSSL for FIPS mode * Configuring a FIPS compliant JDK/JRE on the host, such as bcfips (FIPS BouncyCastle) * Creating a suitable java.security file to restrict usage to non-approved FIPS providers and crypto algorithms * Updating the CLASSPATH to reference the bcfips jars * Refactoring the code base - removing any references to non-FIPS crypto usage, such as non-FIPS bouncycastle, and potentially any other crypto libs * Remove any usage of unapproved crypto algorithms (i.e des, md5 etc) Some questions: # Do you have any more info you can share on how to properly configure zookeeper for FIPS? # Zookeeper seems to reference bouncycastle in some tests - can these be ignored safely? Any other usage of non-FIPS bouncycastle elsewhere? # Are there any other crypto libraries used which may be a concern? # Are there any dependencies used which themselves use non-FIPS crypto? # Are the references to non-approved crypto algorithms in critical path? {*}Expanding on question 2 above{*}, this is the only references i could seem to find for bouncycastle: ``` zookeeper-server/src/test/java/org/apache/zookeeper/common/BaseX509ParameterizedTestCase.java zookeeper-server/src/test/java/org/apache/zookeeper/common/X509TestContext.java zookeeper-server/src/test/java/org/apache/zookeeper/common/X509TestHelpers.java ``` *Expanding on question 5:* md5 usage: ```zookeeper-server/src/main/java/org/apache/zookeeper/server/ZooKeeperServer.java zookeeper-server/src/main/java/org/apache/zookeeper/server/auth/DigestLoginModule.java zookeeper-server/src/main/java/org/apache/zookeeper/server/auth/SaslServerCallbackHandler.java zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/auth/SaslQuorumServerCallbackHandler.java zookeeper-server/src/main/java/org/apache/zookeeper/util/SecurityUtils.java ``` des usage: ``` zookeeper-server/src/test/java/org/apache/zookeeper/common/X509TestHelpers.java ```\nComments: - Hi [~electricthunder] , Thanks for opening this Jira for discussion. Let me try to answer your questions here: I agree that currently *zookeeper.fips-mode* setting only toggles the usage of {_}ZKTrustManager{_}, because that was the first biggest obstacle against running ZK in FIPS-mode, so I had to quickly disable it. We could certainly expand the coverage of that switch for other meaningful changes. It's important to note that packaging of ZooKeeper or releasing ZooKeeper packages is not in the scope of the work of Apache ZooKeeper Community. As an ASF project team, we only release the source code of the product as it is with the documentation and some examples. We do not and will never provide guidance on how to configure your system for running in FIPS-mode. As a consequence, let me skip your questions about configurations outside of ZooKeeper source code. {quote} * Refactoring the code base - removing any references to non-FIPS crypto usage, such as non-FIPS bouncycastle, and potentially any other crypto libs * Remove any usage of unapproved crypto algorithms (i.e des, md5 etc){quote} Great idea. I highly encourage you to provide pull requests covering these changes for the community to review if you have the bandwidth. {quote}{*}Expanding on question 2 above{*}, this is the only references i could seem to find for bouncycastle: {quote} THis should not be a blocker for FIPS, right? Test classes are not built into the runtime. {quote}*Expanding on question 5:* {quote} Could you please provide a patch to optionally remove the usage of MD5? - For instance, a good subtask would be to set default digest method to SHA3-512 in DigestAuthenticationProvider. - Hi [~electricthunder] I'm not sure if you're still monitoring this effort, but I recently created a pull request to avoid usage of DIGEST-MD5 sasl mech in FIPS mode: ZOOKEEPER-4889 It might be interesting for you. Anything you think outstanding for Fips support would be useful.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4831", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4831"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: update slf4j from 1.x to 2.0.13, logback to 1.3.14\nDescription: \nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4830", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4830"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: zk_learners is incorrectly referenced as zk_followers\nDescription: https://issues.apache.org/jira/browse/ZOOKEEPER-3117 renamed the `zk_followers` metric to `zk_learners`, but some references to `zk_followers` remained in the repo, including in the documentation. These should be corrected.\nComments: - master via https://github.com/apache/zookeeper/commit/ee994fbca51f826e4b26d6a105866975d0007f6e.", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4829", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4829"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Support DatadirCleanup in minutes\nDescription: On the cloud, space can be limited. Currently, the DatadirCleanup only supports hours; we should also support cleanup intervals in minutes. {code:java} 2024-02-20 20:55:28,862 - WARN [QuorumPeer[myid=5](plain=disabled)(secure=[0:0:0:0:0:0:0:0]:50512):o.a.z.s.q.Follower@131] - Exception when following the leader java.io.IOException: No space left on device at java.base/java.io.FileOutputStream.writeBytes(Native Method) at java.base/java.io.FileOutputStream.write(FileOutputStream.java:354) at org.apache.zookeeper.common.AtomicFileOutputStream.write(AtomicFileOutputStream.java:72) at java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233) at java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312) at java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316) at java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153) at java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251) at java.base/java.io.BufferedWriter.flush(BufferedWriter.java:257) at org.apache.zookeeper.common.AtomicFileWritingIdiom.<init>(AtomicFileWritingIdiom.java:72) at org.apache.zookeeper.common.AtomicFileWritingIdiom.<init>(AtomicFileWritingIdiom.java:54) at org.apache.zookeeper.server.quorum.QuorumPeer.writeLongToFile(QuorumPeer.java:2229) at org.apache.zookeeper.server.quorum.QuorumPeer.setAcceptedEpoch(QuorumPeer.java:2258) at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:511) at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:91) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1551) 2024-02-20 20:55:28,863 - INFO [QuorumPeer[myid=5](plain=disabled)(secure=[0:0:0:0:0:0:0:0]:50512):o.a.z.s.q.Follower@145]{code}\nComments: - Would you like to contribute a patch? Side comment: this is a pretty unusual request, I have seeing many installations of ZK in the cloud on k8s and I have heard about such problem. Did you consider using a bigger disk? Running maintenance top often may impact latency. - I'll be contributing a patch. Indeed, the problem was resolved by switching to a larger disk. The system generated many snapshots, and with the deletion interval set at one hour, the disk filled up quickly. However, we only needed a few snapshots. In this case, the larger disk was not necessary. - Issue resolved by pull request 2178 [https://github.com/apache/zookeeper/pull/2178]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4828", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4828"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Minor 3.9 broke custom TLS setup with ssl.context.supplier.class\nDescription: We run embedded ZooKeeper in Vespa, and use a custom TLS stack, where we, e.g., do additional validation and authorisation in our TLS trust manager, for client certificates. The changes in https://github.com/apache/zookeeper/commit/4a794276d3d371071c31f86c14da824fdd2e53c0, done for ZOOKEEPER-4622, broke the `ssl.context.supplier.class configuration parameter`, documented in the ZK admin guide (https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_configuration). Consequently, current code (3.9.2) now enforces a file-based key and trust store for _any TLS_, which is not an option for us. I looked at two ways to fix this: 1. Add new configuration parameters for _key_ and _trust_ store _suppliers_, as an alternative to the key and trust store _files_ required in the new (with 3.9.0) ClientX509Util code\u2014this adds another pair of config options, of which there are already plenty, and the user is stuck with the default JDK `Provider` (optional argument to SSLContext.getInstance(protocols, provider); it also lets users with a custom key and trust store use the native SSL support of Netty. Oh, and, Netty provides the option to specify a JDK `Provider` in the SslContextBuilder, too, so that _could_ be made configurable as well. 2. Restore the option of specifying a custom SSL context, and prefer this over using the Netty SslContextBuilder in the new ClientX509Util code, when present\u2014this lets users specify a JDK `Provider`, but file based key and trust stores will be required for the native SSL added in 3.9.0. I don't have a strong opinion on which option is better. I can also contribute a code change with either.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4827", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4827"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Bump bouncycastl version from 1.75 to 1.78\nDescription: Upgrade Bouncy Castle to 1.78 to address CVEs https://bouncycastle.org/releasenotes.html#r1rv78 - https://www.cve.org/CVERecord?id=CVE-2024-29857 (reserved) - https://security.snyk.io/vuln/SNYK-JAVA-ORGBOUNCYCASTLE-6613079 - https://www.cve.org/CVERecord?id=CVE-2024-30171 (reserved) - https://security.snyk.io/vuln/SNYK-JAVA-ORGBOUNCYCASTLE-6613076 - https://www.cve.org/CVERecord?id=CVE-2024-30172 (reserved) - https://security.snyk.io/vuln/SNYK-JAVA-ORGBOUNCYCASTLE-6612984\nComments: - master via https://github.com/apache/zookeeper/commit/6ac8edaabbb9b04529b7438ff226d19ec0e40ec9.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4826", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4826"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Reduce unnecessary executable permissions on files\nDescription: ***Summary:*** This patch aims to modify the permissions of various files within the ZooKeeper repository that currently have executable permissions set (755) but do not require such permissions for their operation. Changing these permissions to 644 enhances security and maintains the consistency of file permissions throughout the project. ***Details:*** Several non-executable files (not including scripts or executable binaries) are currently set with executable permissions. This is generally unnecessary and can lead to potential security concerns. This patch will adjust these permissions to a more appropriate setting (644), which is sufficient for reading and writing operations but does not allow execution.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4825", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4825"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: CVE-2023-6378 is present in the current logback version (1.2.13) and hence we need to upgrade to 1.4.12\nDescription: \nComments: - can this be assigned to me? - master via https://github.com/apache/zookeeper/pull/2162", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4824", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4824"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: fix CVE-2024-29025 in netty package\nDescription: [CVE-2024-29025|https://github.com/advisories/GHSA-5jpm-x58v-624v] is the CVE for all netty-codec-http < 4.1.108.Final\nComments: - Current version is <netty.version>4.1.105.Final</netty.version>. Please assign this Jira to me.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4823", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4823"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Proposal: Update the wiki / design of Zab 1.0 (Phase 2) to make it more precise and practical for the implementation\nDescription: As ZooKeeper evolves these years, its code implementation deviates the design of [Zab 1.0 (wiki)|https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab1.0] in several aspects. One critical deviation lies in the _atomic actions_ upon a follower receives NEWLEADER (see 2.{*}f{*} in Phase 2). The protocol requires that the follower \" _*atomically*_ applies the new state and sets {*}f{*}.currentEpoch = {_}e{_}\". However, the atomicity is not guaranteed with the current code implementation. Asynchronous logging and committing by multi-threads with node crash can interrupt this process and lead to possible data loss (see {-}ZOOKEEPER-3911{-}, ZOOKEEPER-4643, ZOOKEEPER-4646, {-}ZOOKEEPER-4785{-}). On the other hand, to implement atomicity is expensive and affecting performance. It is reasonable to adopt an implementation without requiring atomic updates in this step. It is highly recommended to update the design of Zab without requiring atomicity in Step 2.{*}f{*} and make it more precise and practical to guide the code implementation. h3. Update Step 2.{*}f{*} by removing the requirement of atomicity Here provides a possible design of Step 2.{*}f{*} in Phase 2 with the removal of atomicity requirement. h4. Phase 2: Sync with followers # *l* ... # *f* The follower syncs with the leader, but doesn't modify its state until it receives the NEWLEADER({_}e{_}) packet. Once it receives NEWLEADER({_}e{_}), -_it atomically applies the new state, and then sets f.currentEpoch = e. It then sends ACK(e << 32)._- *it executes the following actions sequentially:* * *2.1. applies the new state;* * *2.2. sets f.currentEpoch = e;* * *2.3. sends ACK(e << 32).* 3. *l* ... Note: * To ensure the correctness without requiring atomicity, the follower must persist and sync the data before it updates its currentEpoch and replies NEWLEADER ack (See the analysis in ZOOKEEPER-4643 & ZOOKEEPER-4785) * This new design conforms to the code implementation in current latest code version (ZooKeeper v3.9.2). This code version has fixed the known data loss issues that stay unresolved for a long time due to non-atomic executions in Step 2.{*}f{*} , including {-}ZOOKEEPER-3911{-}, ZOOKEEPER-4643, ZOOKEEPER-4646 & {-}ZOOKEEPER-4785{-}. (see the code fixes in [PR-2111|https://github.com/apache/zookeeper/pull/2111] & [PR-2152|https://github.com/apache/zookeeper/pull/2152]). * The correctness of this new design has been verified with the TLA+ specifications of Zab at different abstraction levels, including the [high-level protocol specification|https://github.com/AlphaCanisMajoris/zookeeper-tla-spec/blob/main/Zab_new.tla] (developed based on the original [protocol spec|https://github.com/apache/zookeeper/blob/master/zookeeper-specifications/protocol-spec/Zab.tla]) & the [multi-threading-level specification|https://github.com/AlphaCanisMajoris/zookeeper-tla-spec/blob/main/zk_pr_2152.tla] (developed based on the original [system spec.|https://github.com/apache/zookeeper/blob/master/zookeeper-specifications/system-spec/zk-3.7/ZkV3_7_0.tla] This spec is implemented by [PR-2152|https://github.com/apache/zookeeper/pull/2152], an effort to fix more known issues in Phase 2). In the verification, the TLC model checker checks whether the new design satisfies the properties given by the Zab paper. No violation is found during the checking with various configurations. We sincerely hope that the above update of the protocol design can be presented at the wiki page, and make it guide the future code implementation better! About us: We are a research team using TLA+ to verify the correctness of distributed systems. Looking forward to receiving feedback from the ZooKeeper community!\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4822", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4822"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Quorum TLS - Enable member authorization based on certificate CN\nDescription: Quorum TLS enables mutual authentication of quorum members. Member authorization, however, cannot be configured on the basis of the presented principal CN; a round of SASL authentication has to be performed on top of the secured connection. This ticket is about enabling authorization based on trusted client certificates.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4821", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4821"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: ConnectRequest got NOTREADONLY ReplyHeader\nDescription: I would expect {{ConnectRequest}} has two kinds of response in normal conditions: {{ConnectResponse}} and socket close. But if sever was configured with {{readonlymode.enabled}} but not {{localSessionsEnabled}}, then client could get {{NOTREADONLY}} in reply to {{ConnectRequest}}. I saw, at least, no handling in java client. And, I encountered this in writing tests for rust client. It guess it is not by design. And we probably could close the socket in early phase. But also, it could be solved in client sides as {{sizeof(ConnectResponse)}} is larger than {{sizeof(ReplyHeader)}}. Then, we gain ability to carry error for {{ConnectRequest}} while {{ConnectResponse}} does not.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4820", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4820"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: zookeeper pom leaks logback dependency\nDescription: Since v3.8.0 https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper/3.8.0 It's fine that Zookeeper uses Logback on the server side - but users who want to access Zookeeper using client side code also add this zookeeper jar to their classpaths. When zookeeper is used as client side lib, it should ideally not expose a logback dependency - just an slf4j-api jar dependency. Would it be possible to rework the zookeper pom so that client side users don't have to explicitly exclude logback jars? Many users will have their own preferred logging framework. Is there another zookeeper client side jar that could be instead of zookeeper.jar?\nComments: - In case anybody wants to work on this, the right way to do this in Maven is to mark the dependency optional in addition to making it runtime: {code:xml} <optional>true</optional> <scope>runtime</scope> {code} However, you have to be careful if you use any build automation that changing the dependency in this way doesn't cause it to be omitted from a distribution assembly, like a .tar.gz, or .zip file. This might happen if you use a maven-assembly-plugin assembly descriptor that bundles dependencies, but omits optional ones, for example. I'm not sure if the default behavior of [includeDependencies|https://maven.apache.org/plugins/maven-assembly-plugin/assembly.html] will include optional ones or not. - or `<scope>provided</scope>` - Provided scope may achieve the same result for dependency resolution for dependent projects. However, they mean very different things. Provided means it's required, but not expected to be bundled because it is expected to be provided by the destination environment. In this case, we want the opposite: it's not required, but is expected to be bundled. However, there may be times where you need to use provided instead of using optional, because you need it to be handled in a specific way by a Maven plugin during the build, because different plugins do different things with these. For this, the correct way of doing it that communicates the relevant intent is to mark it optional and keep the scope as runtime. However, if provided is needed instead, to make a particular plugin work, you should understand that this may cause other problems downstream, and at the very least, you should add a comment in the POM explaining why provided is used instead, so the intention is clear. For what it's worth, I think provided dependencies will be excluded by the maven-assembly-plugin, which is not what you want (but may not matter if the plugin is declared explicitly in the assembly descriptor). I'd stick with optional if it works. - In this case I find both the {{optional}} attribute and the {{provided}} scope inappropriate for the {{zookeeper}} artifact. If Zookeeper were to have an {{optional}} dependency on Logback, it might suggest that some part of Zookeeper uses Logback *directly*, e.g. to manage and change log levels. AFAIK it is not the case. If Zookeeper were to have a dependency on Logback in the {{provided}} scope, I would interpret it as a strict requirement to have Logback on the runtime classpath. AFAIK any other logging backend will work as well. - Issue resolved by pull request 2155 [https://github.com/apache/zookeeper/pull/2155]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4819", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4819"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Can't seek for writable tls server if connected to readonly server\nDescription: {{[ClientCnxn::pingRwServer|https://github.com/apache/zookeeper/blob/d12aba599233b0fcba0b9b945ed3d2f45d4016f0/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L1280]}} uses raw socket to issue \"isro\" 4lw command. This results in unsuccessful handshake to tls server.\nComments: - [~kezhuw] I can take a look and resolve it. Can you assign this issue to me ? It seems we should make a socket from SSLSocketFactory with specified trustStore info when zookeeper.client.secure=true - Hi [~kezhuw] I submit a pr to resolve it, can you take a look? - master: 75f9781ecc3cde9fa5a032fa8cfa8c79b69ef879 branch-3.9: 644dcdbc9ac2b5e4a00dcaca7de4db26e923b59f", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4818", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4818"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Export JVM heap metrics in ServerMetrics\nDescription: A metric for JVM heap occupancy is not included in ServerMetrics. According to [https://zookeeper.apache.org/doc/current/zookeeperMonitor.html] the recommended practice is for someone to enable the PrometheusMetricsProvider and the Prometheus base class upon which that provider is based does export that information. See [https://zookeeper.apache.org/doc/current/zookeeperMonitor.html] . The example provided for alerting on heap utilization there is: {noformat} - alert: JvmMemoryFillingUp expr: jvm_memory_bytes_used / jvm_memory_bytes_max{area=\"heap\"} > 0.8 for: 5m labels: severity: warning annotations: summary: \"JVM memory filling up (instance {{ $labels.instance }})\" description: \"JVM memory is filling up (> 80%)\\n labels: {{ $labels }} value = {{ $value }}\\n\" {noformat} where {{jvm_memory_bytes_used}} and {{jvm_memory_bytes_max}} are provided by a Prometheus base class. Where PrometheusMetricsProvider is the right choice that's good enough but where the ServerMetrics information is consumed in an alternate way, by 4-letter-word scraping, or by JMX, ServerMetrics should provide the same information. {{jvm_memory_bytes_used}} and {{jvm_memory_bytes_max}} (presuming heap) are reasonable names. An alternative could be to calculate the heap occupancy and provide that as a percentage, either an integer in the range 0 - 100 or floating point value in the range 0.0 - 1.0. There is some precedent for exporting JVM metrics in ServerMetrics from ZOOKEEPER-3845 .\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4817", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4817"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Client disconnection warning is missed in system log sometimes.\nDescription: Recently, we encounter an confused issue. The client disconnection warning disappears in system log. However, sometimes, this message appears in system log. There is a cluster consisting of three node. A client sends many creation requests and then read the node created by the first request. The client read operation failed. We watch the system log. Sometimes, there is a client disconnection warning. Sometimes, there is not. This incomplete system log mislead client judgement on the problem. After investigating, when NIOServerCnxn.doIO is stuck in any IO point in this function and the stuck time exceeds 20s, the client disconnection warning will disappear. If the stuck time is less than 20s, the client disconnection warning will appear in system log. We find that the root cause is that selectorThread is set as the daemon thread. When one node encounter the fail-slow nic, the client disconnects with the node. If the NIOServerCnxn.doIO is stuck and the stuck time exceeds 20s, the corresponding selectorThread will be killed by JVM. Hence, the client disconnection warning is missed. Attached logs(20s) contain CancelledKeyException, but logs(25) do not contain. Are there any comments to figure out this issues and improve the diagnosability of ZooKeeper\uff1f I will very appreciate them.\nComments: - [~kezhuw] hi, cloud you help me figure out this phenomenon? - {quote} We find that the root cause is that selectorThread is set as the daemon thread. When one node encounter the fail-slow nic, the client disconnects with the node. If the NIOServerCnxn.doIO is stuck and the stuck time exceeds 20s, the corresponding selectorThread will be killed by JVM. Hence, the client disconnection warning is missed. {quote} What did you mean by \"be killed by JVM\" ? And I am not sure, if the whole server is going to shut, how does \"CancelledKeyException\" matter ? I may mislead something. - I try to trace the thread of NIOServerCnxn. I found that the thread ID of NIOServerCnxn in the node2 changes when the stuck time exceeds 20s. Hence, i guess that the thread of NIOServerCnxn may be killed by JVM due to some reasons. Additionally, the system logs show that the server is still alive. Do you have any suggestion to figure out this problem? Thanks.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4816", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4816"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: A follower can not join the cluster for 30s seconds\nDescription: We encounter a strange scenario. When we set up the cluster of zookeeper(3 nodes totally), the third node is stuck in ({*}sealStream{*}) serializing the snapshot to the local disk. However, the leader election is executed normally. After the election, the third node is elected as the leader. The other two nodes fail to connect with the leader. Hence, the first and second nodes restart the leader election, finally the second node is elected as the leader. At this time, the third node still act as the leader. There are two leaders in the cluster. The first node can not join the cluster for 30s. The logs of the first node are as following. {code:java} 2024-07-01 02:35:28,223 [myid:] - INFO [WorkerReceiver[myid=1]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x2, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:35:28,253 [myid:] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.FastLeaderElection@996] - Notification time out: 12800 ms2024-07-01 02:35:28,254 [myid:] - INFO [WorkerReceiver[myid=1]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:2, n.round:0x2, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:35:28,287 [myid:] - INFO [WorkerReceiver[myid=1]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:35:28,288 [myid:] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.FastLeaderElection@1205] - Oracle indicates not to follow2024-07-01 02:35:29,493 [myid:] - WARN [NIOWorkerThread-20:o.a.z.s.NIOServerCnxn@397] - Close of session 0x0{code} During this procedure, the client can not connect with any nodes of the cluster. Runtime logs are attached. The root cause is the serializing the snapshot blocks the status modification of the third node? Sometimes, this phenomenon will disappears. The configuration is the same as above. The second node is stuck in sealStream due to fail-slow disk. {code:java} // node1 2024-07-01 02:19:21,942 [myid:] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@920] - Peer state changed: looking2024-07-01 02:19:21,943 [myid:] - WARN [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@1602] - PeerState set to LOOKING2024-07-01 02:19:21,943 [myid:] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@1471] - LOOKING2024-07-01 02:19:21,943 [myid:] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.FastLeaderElection@946] - New election. My id = 1, proposed zxid=0x02024-07-01 02:19:21,980 [myid:] - INFO [WorkerReceiver[myid=1]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x2, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:19:22,012 [myid:] - INFO [WorkerReceiver[myid=1]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:3, n.state:FOLLOWING, n.leader:2, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:19:22,026 [myid:] - INFO [WorkerReceiver[myid=1]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:2, n.state:LEADING, n.leader:2, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:19:22,026 [myid:] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@906] - Peer state changed: following {code} {code:java} // node3 2024-07-01 02:19:22,780 [myid:] - INFO [QuorumPeer[myid=3](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@920] - Peer state changed: looking2024-07-01 02:19:22,780 [myid:] - WARN [QuorumPeer[myid=3](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@1602] - PeerState set to LOOKING2024-07-01 02:19:22,780 [myid:] - INFO [QuorumPeer[myid=3](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@1471] - LOOKING2024-07-01 02:19:22,781 [myid:] - INFO [QuorumPeer[myid=3](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.FastLeaderElection@946] - New election. My id = 3, proposed zxid=0x02024-07-01 02:19:22,819 [myid:] - INFO [WorkerReceiver[myid=3]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:3, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:19:22,838 [myid:] - INFO [WorkerReceiver[myid=3]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:1, n.state:FOLLOWING, n.leader:2, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:19:22,849 [myid:] - INFO [WorkerReceiver[myid=3]:o.a.z.s.q.FastLeaderElection$Messenger$WorkerReceiver@391] - Notification: my state:LOOKING; n.sid:2, n.state:LEADING, n.leader:2, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x02024-07-01 02:19:22,850 [myid:] - INFO [QuorumPeer[myid=3](plain=0.0.0.0:2181)(secure=disabled):o.a.z.s.q.QuorumPeer@906] - Peer state changed: following {code} But the leader election notification is strange. Node1 follows node2 since node3 follows node2. However, Node3 follows node2 since node1 follows node2. This condition cannot stand. Moreover, this two scenarios are both stuck in sealStream. But there are different results. Is there a data race? Are there any comments to figure out this issues\uff1f I will very appreciate them.\nComments: No comments.", "output": "Status: Open\nPriority: Critical"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4815", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4815"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: custom the data format of /zookeeper/config\nDescription: When using QuorumMaj, I hope to support custom /zookeeper/config node data formats, such as server. x=xx. xx. xx. xx: 2888:3888: observer; 0.0.0.0:2181; Group1 server. y=xx. xx. xx. xx: 2888:3888: observer; 0.0.0.0:2181; Group2\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4814", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4814"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Protocol desynchronization after Connect for (some) old clients\nDescription: Some old clients experience a protocol synchronization after receiving a {{ConnectResponse}} from the server. This started happening with ZOOKEEPER-4492, \"Merge readOnly field into ConnectRequest and Response,\" which writes overlong responses to clients which do not know about the {{readOnly}} flag. (One example of such a client is ZooKeeper's own C client library prior to version 3.5!)\nComments: - Also related: [ClickHouse ticket, \"Incompatibility with Zookeeper 3.9\"|https://github.com/ClickHouse/ClickHouse/issues/53749] whose custom (?) client was updated by [patch \"Add support for read-only mode in ZooKeeper\"|https://github.com/ClickHouse/ClickHouse/pull/57479/files]. - master: 90f8d835e065ea12dddd8ed9ca20872a4412c78a branch-3.9: 4d68e3e56fadbee46907213449a0fe6da302e161", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4813", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4813"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Make zookeeper start successfully when the last log file is dirty during the restore progress\nDescription: When the zookeeper restarts, it will restore the data from the last valid snapshot file, and replay txn log to append data. But if the last log file is empty due to some reason, the restore will fail, not make the zookeeper can not restart. The logs as followings: {noformat} 14:12:16.023 [main] INFO org.apache.zookeeper.server.persistence.SnapStream - Invalid snapshot snapshot.188700025d87. len = 761554294, byte = 45 14:12:16.024 [main] INFO org.apache.zookeeper.server.persistence.FileSnap - Reading snapshot /pulsar/data/zookeeper/version-2/snapshot.188700025a05 14:12:17.350 [main] INFO org.apache.zookeeper.server.DataTree - The digest in the snapshot has digest version of 2, with zxid as 0x188700025b07, and digest value as 510776662607117 14:12:17.492 [main] ERROR org.apache.zookeeper.server.quorum.QuorumPeer - Unable to load database on disk java.io.EOFException: null at java.io.DataInputStream.readInt(DataInputStream.java:386) ~[?:?] at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96) ~[org.apache.zookeeper-zookeeper-jute-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:67) ~[org.apache.zookeeper-zookeeper-jute-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:725) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:743) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:711) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:792) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:361) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnSnapLog.lambda$restore$0(FileTxnSnapLog.java:267) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:312) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:288) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1149) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1135) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:229) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:137) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:91) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] 14:12:17.502 [main] INFO org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider - Shutdown executor service with timeout 1000 14:12:17.508 [main] INFO org.eclipse.jetty.server.AbstractConnector - Stopped ServerConnector@2484f433{HTTP/1.1, (http/1.1)}{0.0.0.0:8000} 14:12:17.510 [main] INFO org.eclipse.jetty.server.handler.ContextHandler - Stopped o.e.j.s.ServletContextHandler@59a67c3a{/,null,STOPPED} 14:12:17.515 [main] ERROR org.apache.zookeeper.server.quorum.QuorumPeerMain - Unexpected exception, exiting abnormally java.lang.RuntimeException: Unable to run quorum server at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1204) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1135) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:229) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:137) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:91) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] Caused by: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:386) ~[?:?] at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96) ~[org.apache.zookeeper-zookeeper-jute-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:67) ~[org.apache.zookeeper-zookeeper-jute-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:725) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:743) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:711) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:792) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:361) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnSnapLog.lambda$restore$0(FileTxnSnapLog.java:267) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:312) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:288) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1149) ~[org.apache.zookeeper-zookeeper-3.9.1.jar:3.9.1] ... 4 more {noformat} In fact, if the last log file open failed, we can ignore the log file.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4812", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4812"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Another reconfiguration is in progress -- concurrent reconfigs not supported (yet)\nDescription: \u5728\u4f7f\u7528reconfig\u547d\u4ee4\u589e\u52a0\u6216\u5220\u9664\u6210\u5458\u65f6\u63d0\u793a Another reconfiguration is in progress -- concurrent reconfigs not supported (yet) \uff0c\u4e00\u76f4\u65e0\u6cd5\u64cd\u4f5c\u96c6\u7fa4\u7684\u589e\u51cf\u8282\u70b9\u3002\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4811", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4811"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: When configuring the IP address of the zk server on the client side to connect to zk, the connection establishment time is high\nDescription: When configuring the IP address of the zk server on the client side to connect to zk, the connection establishment time is high\u3002Mainly because Obtaining the hostname of the address takes approximately 5 seconds. 3.4.6 has mechanism to safely avoid reverse DNS lookup\uff0cbut 3.7 don't do that\u3002 1.What's the reasone\uff1f 2.Can we modify the method StaticHostProvider/resolve() to avoid reverse DNS lookup?\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4810", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4810"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix data race in format_endpoint_info()\nDescription: \nComments: - Issue resolved by pull request 2140 [https://github.com/apache/zookeeper/pull/2140]", "output": "Status: Closed\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4809", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4809"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: do_completion() use-after-free when log level is debug\nDescription: {code:c} void *do_completion(void *v) { zhandle_t *zh = v; // ... api_epilog(zh, 0); // L1 LOG_DEBUG(LOGCALLBACK(zh), \"completion thread terminated\"); // L2 return 0; } {code} When the log level is debug, L2 gets the log calback after zookeeper_close(), causes uaf.\nComments: - master: 4e999c8708656ab18ea466607ee9859b517b680a", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4808", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4808"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Fix the log statement in FastLeaderElection\nDescription: The proposedZxid and proposedEpoch is out of order in the following debug statement. {code:java} LOG.debug( \"Sending Notification: {} (n.leader), 0x{} (n.peerEpoch), 0x{} (n.zxid), 0x{} (n.round), {} (recipient),\"+underlined text+ + \" {} (myid) \", proposedLeader, Long.toHexString(proposedZxid), Long.toHexString(proposedEpoch), Long.toHexString(logicalclock.get()), sid, self.getMyId()); {code}\nComments: No comments.", "output": "Status: Resolved\nPriority: Trivial"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4807", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4807"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add sid for the leader goodbyte log\nDescription: When a follower disconnects with the leader, the leader will print the remote address. But if the zookeeper is along with istio, the remote address is not right. 2024-02-05T03:23:54,967+0000 [LearnerHandler-/127.0.0.6:56085] WARN org.apache.zookeeper.server.quorum.LearnerHandler - ******* GOODBYE /127.0.0.6:56085 ******** We would better print the sid in the goodbye log.\nComments: No comments.", "output": "Status: Resolved\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4806", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4806"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Commits have to be refreshed after merging\nDescription: The following error occurs if somebody wants to cherry-pick immediately after merge: {noformat} All checks have passed on the github. Pull request #2115 merged. Sha: #18c78cd10bc02d764a46ac1659b263cf69f2671d Would you like to pick 18c78cd10bc02d764a46ac1659b263cf69f2671d into another branch? (y/n): y Enter a branch name [branch-3.9]: git fetch apache From https://gitbox.apache.org/repos/asf/zookeeper 72e3d9ce9..e571dd814 master -> apache/master git checkout -b PR_TOOL_PICK_PR_2115_BRANCH-3.9 apache/branch-3.9 Switched to a new branch 'PR_TOOL_PICK_PR_2115_BRANCH-3.9' git cherry-pick -sx 18c78cd10bc02d764a46ac1659b263cf69f2671d fatal: bad object 18c78cd10bc02d764a46ac1659b263cf69f2671d Error cherry-picking: Command '['git', 'cherry-pick', '-sx', '18c78cd10bc02d764a46ac1659b263cf69f2671d']' returned non-zero exit status 128.{noformat} The reason for this is, because the local git repo doesn't know about the new commit yet. We should do a {{git fetch}} after successfully merged via GitHub.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4805", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4805"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Update cwiki page with latest changes\nDescription: Update the following wiki page with latest changes and instructions how to use the script: [https://cwiki.apache.org/confluence/display/ZOOKEEPER/Merging+Github+Pull+Requests]\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4804", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4804"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Use daemon threads for Netty client\nDescription: When the Netty client is used, the Java process hangs on System.exit if there is an open Zookeeper connection. This is caused by the non-daemon threads created by Netty. Exiting without closing the connection is not a good practice, but this hang does not happen with the NIO client, and I think ZK should behave the same regardless of the client implementation used. The Netty ThreadFactory implementation is configurable, it shouldn't be too hard make sure that daemon threads are created.\nComments: - FYI [~andor]. - would you like to send a PR ? - I don't have the time right now. Maybe in a few weeks, but if anyone feels like doing this, don't wait for me. - I have uploaded a PR with the fix [~eolivelli]. - master via 803c485db99134d8f41c2ea90ca9305e6b806d52 - I think this is important enopugh to be backported to all active branches. FYI [~andor]", "output": "Status: Closed\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4803", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4803"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky test: QuorumPeerMainTest.testLeaderOutOfView\nDescription: {code:java} [2024-02-08T02:01:19.039Z] [INFO] [2024-02-08T02:01:19.039Z] [INFO] Results: [2024-02-08T02:01:19.039Z] [INFO] [2024-02-08T02:01:19.039Z] [ERROR] Failures: [2024-02-08T02:01:19.039Z] [ERROR] QuorumPeerMainTest.testLeaderOutOfView:881 expected: <LOOKING> but was: <FOLLOWING> [2024-02-08T02:01:19.039Z] [INFO] [2024-02-08T02:01:19.039Z] [ERROR] Tests run: 3116, Failures: 1, Errors: 0, Skipped: 4 [2024-02-08T02:01:19.039Z] [INFO] [2024-02-08T02:01:19.039Z] [INFO] ------------------------------------------------------------------------ [2024-02-08T02:01:19.039Z] [INFO] Reactor Summary for Apache ZooKeeper 3.10.0-SNAPSHOT: {code} Link: https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-2043/4/pipeline\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4802", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4802"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky test:RestoreQuorumTest.testRestoreAfterQuorumLost\nDescription: {code:java} [INFO] Running org.apache.zookeeper.server.admin.RestoreQuorumTest 836[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.489 s <<< FAILURE! - in org.apache.zookeeper.server.admin.RestoreQuorumTest 837[ERROR] testRestoreAfterQuorumLost Time elapsed: 3.344 s <<< ERROR! 838java.net.ConnectException: Connection refused (Connection refused) 839 at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) 840 at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412) 841 at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255) 842 at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237) 843 at java.base/java.net.Socket.connect(Socket.java:609) 844 at java.base/java.net.Socket.connect(Socket.java:558) 845 at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) 846 at org.apache.zookeeper.server.admin.SnapshotAndRestoreCommandTest.takeSnapshotAndValidate(SnapshotAndRestoreCommandTest.java:413) 847 at org.apache.zookeeper.server.admin.RestoreQuorumTest.testRestoreAfterQuorumLost(RestoreQuorumTest.java:56) 848 849[INFO] Running org.apache.zookeeper.server.admin.CommandResponseTest {code} Link: https://github.com/apache/zookeeper/actions/runs/7812662872/job/21310154983\nComments: - Another case. https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-2105/5/pipeline/#step-35-log-911 {noformat} [2024-08-12T02:47:02.580Z] java.net.ConnectException: Connection refused (Connection refused) [2024-08-12T02:47:02.580Z] at java.net.PlainSocketImpl.socketConnect(Native Method) [2024-08-12T02:47:02.580Z] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) [2024-08-12T02:47:02.580Z] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) [2024-08-12T02:47:02.580Z] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) [2024-08-12T02:47:02.580Z] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [2024-08-12T02:47:02.580Z] at java.net.Socket.connect(Socket.java:607) [2024-08-12T02:47:02.580Z] at java.net.Socket.connect(Socket.java:556) [2024-08-12T02:47:02.580Z] at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) [2024-08-12T02:47:02.580Z] at org.apache.zookeeper.server.admin.SnapshotAndRestoreCommandTest.takeSnapshotAndValidate(SnapshotAndRestoreCommandTest.java:413) [2024-08-12T02:47:02.580Z] at org.apache.zookeeper.server.admin.RestoreQuorumTest.testRestoreAfterQuorumLost(RestoreQuorumTest.java:56) {noformat}", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4801", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4801"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Add memory size limitation policy for ZkDataBase#committedLog\nDescription: The ZkDataBase support commit log count to limit the memory, which is not precise, some request payloads may be huge, it will cost lots of heap memory. So support payload size limitation will be better.\nComments: No comments.", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4800", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4800"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Flaky test:ReconfigRollingRestartCompatibilityTest.testRollingRestartWithExtendedMembershipConfig\nDescription: Link: https://github.com/apache/zookeeper/actions/runs/7781886238/job/21217202024?pr=1932 {code:java} [ERROR] Failures: 1023[ERROR] ReconfigRollingRestartCompatibilityTest.testRollingRestartWithExtendedMembershipConfig:263 waiting for server 2 being up ==> expected: <true> but was: <false> 1024[INFO] 1025[ERROR] Tests run: 3114, Failures: 1, Errors: 0, Skipped: 4 1026[INFO] 1027[INFO] ------------------------------------------------------------------------ {code}\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4799", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4799"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Refactor ACL check in addWatch command\nDescription: \nComments: - This is the fix for [CVE-2024-23944|https://www.cve.org/CVERecord?id=CVE-2024-23944]", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4798", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4798"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Secure prometheus support\nDescription: The current implementation uses HTTP. However, many companies, especially those on the cloud, require HTTPS.\nComments: No comments.", "output": "Status: Resolved\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4797", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4797"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Allow for -XX:MaxRAMPercentage JVM setting\nDescription: When running Zk in a containerized environment, it's sometimes desirable to express your heap size in terms of percentage of available memory allocated to a container. As it stands, zkEnv.sh forces your to have -Xmx, which defaults to 1GB. Some environments wanted to set it to more, mostly related to the amount of Ram. This is a request to implement the option of using -XX:MaxRamPercentage option when starting zookeeper. Suggested implementation is to also make a variable ZK_SERVER_MAXRAMPERCENTAGE available to be added to SERVER_JVMFLAGS. If the variable is set, ZK_HEAP_SERVER is ignored, if no ZK_SERVER_MAXRAMPERCENTAGE, ZK_SERVER_HEAP is set as usual.\nComments: - I don't see this property as particularly special over other JVM flags. The only thing that makes Xmx special is how common it is. The percentage option is not nearly as common. It doesn't make sense to add an option for every little thing users might set. Instead, it'd be better to make it simple and easy for users to set anything they want. Also, there's value in keeping this config file simple, so it can be easily edited by anybody who needs to do something unique, rather than bake in complexity to handle a variety of scenarios. See my suggestion in https://github.com/apache/zookeeper/pull/2120#pullrequestreview-1851494424", "output": "Status: Open\nPriority: Major"}
{"meta": {"source": "Apache Jira", "id": "ZOOKEEPER-4796", "url": "https://issues.apache.org/jira/browse/ZOOKEEPER-4796"}, "instruction": "Analyze the issue description and discussion to determine the current status and priority.", "input": "Title: Requests submitted first may carry a larger xid resulting in ZRUNTIMEINCONSISTENCY\nDescription: When multiple threads attempt to submit requests, it's possible for a request from a thread that acquired its xid earlier to be inserted after a request from a thread that acquired its xid later in the submission queue, which causes a ZRUNTIMEINCONSISTENCY error. To fix it, acquires the lock before get_xid() and releases it after request submission.\nComments: No comments.", "output": "Status: Open\nPriority: Minor"}
